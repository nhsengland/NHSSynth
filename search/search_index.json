{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NHS Synth","text":"<p>Under construction, see the Code Reference or Model Card.</p>"},{"location":"model_card/","title":"Model Card: Variational AutoEncoder with Differential Privacy","text":"<p>Following Model Cards for Model Reporting (Mitchell et al.) and Lessons from Archives (Jo &amp; Gebru), we're providing some information about about the Variational AutoEncoder (VAE) with Differential Privacy within this repository.</p>"},{"location":"model_card/#model-details","title":"Model Details","text":"<p>The implementation of the Variational AutoEncoder (VAE) with Differential Privacy within this repository was created as part of an NHSX Analytics Unit PhD internship project undertaken by Dominic Danks (last commit to the repository: commit 88a4bdf). This model card describes the updated version of the model, released in March 2022.  Further information about the previous version created by Dom and its model implementation can be found in Section 5.4 of the associated report.</p>"},{"location":"model_card/#model-use","title":"Model Use","text":""},{"location":"model_card/#intended-use","title":"Intended Use","text":"<p>This model is intended for use in experimenting with differential privacy and VAEs.</p>"},{"location":"model_card/#training-data","title":"Training Data","text":"<p>Experiments in this repository are run against the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) dataset accessed via the pycox python library. We also performed further analysis on a single table that we extracted from MIMIC-III.</p>"},{"location":"model_card/#performance-and-limitations","title":"Performance and Limitations","text":"<p>A from-scratch VAE implementation was compared against various models available within the SDV framework using a variety of quality and privacy metrics on the SUPPORT dataset. The VAE was found to be competitive with all of these models across the various metrics. Differential Privacy (DP) was introduced via DP-SGD and the performance of the VAE for different levels of privacy was evaluated. It was found that as the level of Differential Privacy introduced by DP-SGD was increased, it became easier to distinguish between synthetic and real data.</p> <p>Proper evaluation of quality and privacy of synthetic data is challenging. In this work, we utilised metrics from the SDV library due to their natural integration with the rest of the codebase. A valuable extension of this work would be to apply a variety of external metrics, including more advanced adversarial attacks to more thoroughly evaluate the privacy of the considered methods, including as the level of DP is varied. It would also be of interest to apply DP-SGD and/or PATE to all of the considered methods and evaluate whether the performance drop as a function of implemented privacy is similar or different across the models.</p> <p>Currently the SynthVAE model only works for data which is 'clean'. I.e data that has no missingness or NaNs within its input. It can handle continuous, categorical and datetime variables. Special types such as nominal data cannot be handled properly however the model may still run. Column names have to be specified in the code for the variable group they belong to.</p> <p>Hyperparameter tuning of the model can result in errors if certain parameter values are selected. Most commonly, changing learning rate in our example results in errors during training. An extensive test to evaluate plausible ranges has not been performed as of yet. If you get errors during tuning then consider your hyperparameter values and adjust accordingly.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli<ul> <li>config</li> <li>module_arguments</li> <li>module_setup</li> <li>run</li> </ul> </li> <li>modules<ul> <li>dataloader<ul> <li>io</li> <li>metadata</li> <li>run</li> <li>transformers</li> </ul> </li> <li>evaluation<ul> <li>metrics</li> <li>run</li> </ul> </li> <li>model<ul> <li>DPVAE</li> <li>run</li> </ul> </li> <li>plotting<ul> <li>plot</li> <li>run</li> </ul> </li> <li>structure<ul> <li>run</li> </ul> </li> </ul> </li> <li>utils<ul> <li>constants</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/cli/","title":"cli","text":""},{"location":"reference/cli/#nhssynth.cli.config","title":"<code>config</code>","text":""},{"location":"reference/cli/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>cli/config.py</code> <pre><code>def assemble_config(\n    args: argparse.Namespace,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.\n\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\n    args_dict = vars(args)\n    modules_to_run = args_dict.pop(\"modules_to_run\", None)\n    if not modules_to_run:\n        run_type = get_key_by_value({mn: mc.func for mn, mc in ms.MODULE_MAP.items()}, args_dict[\"func\"])\n        modules_to_run = ms.PIPELINE if run_type == \"pipeline\" else run_type\n    elif len(modules_to_run) == 1:\n        run_type = modules_to_run[0]\n    elif modules_to_run == ms.PIPELINE:\n        run_type = \"pipeline\"\n    else:\n        raise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n\n    # Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\n    module_args = {\n        module_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\n        for module_name in modules_to_run\n    }\n\n    # Use the flat namespace to populate a nested (by module) dictionary of config args and values\n    out_dict = {}\n    for module_name in modules_to_run:\n        for k in args_dict.copy().keys():\n            if k in module_args[module_name]:\n                if out_dict.get(module_name):\n                    out_dict[module_name].update({k: args_dict.pop(k)})\n                else:\n                    out_dict[module_name] = {k: args_dict.pop(k)}\n\n    # Assemble the final dictionary in YAML-compliant form\n    return {\n        **({\"run_type\": run_type} if run_type else {}),\n        **{k: v for k, v in args_dict.items() if k not in {\"func\", \"run_name\", \"save_config\", \"save_config_path\"}},\n        **out_dict,\n    }\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>ArgumentParser</code> <p>The top-level parser.</p> required <code>module_parsers</code> <code>list</code> <p>The list of module-level parsers.</p> required <p>Returns:</p> Type Description <code>A tuple containing two elements</code> <ul> <li>A dictionary containing all arguments and their default values.<ul> <li>A list of the names of the required arguments.</li> </ul> </li> </ul> Source code in <code>cli/config.py</code> <pre><code>def get_default_and_required_args(\n    top_parser: argparse.ArgumentParser,\n    module_parsers: list[argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n\n    Args:\n        top_parser: The top-level parser.\n        module_parsers: The list of module-level parsers.\n\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of the names of the required arguments.\n    \"\"\"\n    all_actions = top_parser._actions + [action for sub_parser in module_parsers for action in sub_parser._actions]\n    defaults = {}\n    required_args = []\n    for action in all_actions:\n        if action.dest not in [\"help\", \"==SUPPRESS==\"]:\n            defaults[action.dest] = action.default\n            if action.required:\n                required_args.append(action.dest)\n    return defaults, required_args\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config Namespace object for the inferred modules to run and executes.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default config from the parser and <code>all_subparsers</code> for the <code>modules_to_run</code></li> <li>Overwrite this config with the specified subset (or full set) of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting config</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>ArgumentParser</code> <p>top-level ArgumentParser object</p> required <code>all_subparsers</code> <code>dict</code> <p>dictionary of ArgumentParser objects, one for each module</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>Namespace object containing the assembled configuration settings</p> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file</p> Source code in <code>cli/config.py</code> <pre><code>def read_config(\n    args: argparse.Namespace,\n    parser: argparse.ArgumentParser,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config Namespace object for the inferred modules to run and executes.\n\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default config from the parser and `all_subparsers` for the `modules_to_run`\n    4. Overwrite this config with the specified subset (or full set) of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting config\n\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level ArgumentParser object\n        all_subparsers: dictionary of ArgumentParser objects, one for each module\n\n    Returns:\n        Namespace object containing the assembled configuration settings\n\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file\n    \"\"\"\n    # Open the passed yaml file and load into a dictionary\n    with open(f\"config/{args.input_config}.yaml\") as stream:\n        config_dict = yaml.safe_load(stream)\n\n    valid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\n\n    run_type = config_dict.pop(\"run_type\", None)\n    # TODO Check this covers all bases\n    if run_type == \"pipeline\":\n        modules_to_run = ms.PIPELINE\n    else:\n        modules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\n        if not args.custom_pipeline:\n            modules_to_run = sorted(modules_to_run, key=lambda x: ms.PIPELINE.index(x))\n\n    if not modules_to_run:\n        warnings.warn(\n            \"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n        )\n        modules_to_run = ms.PIPELINE\n\n    # Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\n    args_dict, required_args = get_default_and_required_args(\n        parser, [all_subparsers[module_name] for module_name in modules_to_run]\n    )\n\n    # Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\n    non_default_passed_args_dict = {\n        k: v\n        for k, v in vars(args).items()\n        if k in [\"input_config\", \"custom_pipeline\"] or (k != \"func\" and v != args_dict[k])\n    }\n\n    # Overwrite the default arguments with the ones from the yaml file\n    args_dict.update(flatten_dict(config_dict))\n\n    # Overwrite the result of the above with any non-default CLI args\n    args_dict.update(non_default_passed_args_dict)\n\n    # Create a new Namespace using the assembled dictionary\n    new_args = argparse.Namespace(**args_dict)\n    assert all(\n        getattr(new_args, req_arg) for req_arg in required_args\n    ), \"Required arguments are missing from the passed config file\"\n\n    # Run the appropriate execution function(s)\n    for module in modules_to_run:\n        ms.MODULE_MAP[module].func(new_args)\n\n    new_args.modules_to_run = modules_to_run\n    return new_args\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary containing all subparsers for the config args.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>cli/config.py</code> <pre><code>def write_config(\n    args: argparse.Namespace,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO Maybe build a function that does this kind of filtering\n    if args.sdv_workflow:\n        del args.synthesizer\n    args_dict = assemble_config(args, all_subparsers)\n    with open(f\"{args.save_config_path}\", \"w\") as yaml_file:\n        yaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments","title":"<code>module_arguments</code>","text":""},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\n    parser.add_argument(\n        \"-i\",\n        \"--input-file\",\n        required=(not override),\n        help=\"the name of the `.csv` file to prepare\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output-file\",\n        default=\"_prepared\",\n        help=\"where to write the prepared data, defaults to `experiments/&lt;args.run_name&gt;/&lt;args.input_file&gt;_prepared.csv`\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--dir\",\n        default=\"./data\",\n        help=\"the directory to read and write data from and to\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--metadata-file\",\n        default=\"_metadata\",\n        help=\"metadata for the input data, defaults to `&lt;args.dir&gt;/&lt;args.input_file&gt;_metadata.yaml`\",\n    )\n    parser.add_argument(\n        \"-ic\",\n        \"--index-col\",\n        default=None,\n        choices=[None, 0],\n        help=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n    )\n    parser.add_argument(\n        \"-sdv\",\n        \"--sdv-workflow\",\n        action=\"store_true\",\n        help=\"utilise the SDV synthesizer workflow for transformation and metadata, rather than a `HyperTransformer` from RDT\",\n    )\n    parser.add_argument(\n        \"-ant\",\n        \"--allow-null-transformers\",\n        action=\"store_true\",\n        help=\"allow null / None transformers, i.e. leave some columns as they are\",\n    )\n    parser.add_argument(\n        \"-cy\",\n        \"--collapse-yaml\",\n        action=\"store_true\",\n        help=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n    )\n    parser.add_argument(\n        \"-is\",\n        \"--imputation-strategy\",\n        default=\"mean\",\n        choices=[\"mean\", \"median\", \"cull\"],\n        help=\"imputation strategy for missing values\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--synthesizer\",\n        default=\"TVAE\",\n        choices=list(SDV_SYNTHESIZER_CHOICES.keys()),\n        help=\"pick a synthesizer to use (note this can also be specified in the model module, these must match)\",\n    )\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_top_level_args","title":"<code>add_top_level_args(parser)</code>","text":"<p>Adds top-level arguments to an existing ArgumentParser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_top_level_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds top-level arguments to an existing ArgumentParser instance.\"\"\"\n    parser.add_argument(\n        \"-rn\",\n        \"--run-name\",\n        default=TIME,\n        help=f\"name the run to affect logging, config and outputs, defaults to current time, i.e. `{TIME}`\",\n    )\n    parser.add_argument(\"-sc\", \"--save-config\", action=\"store_true\", help=\"save the config provided via the cli\")\n    parser.add_argument(\n        \"-scp\",\n        \"--save-config-path\",\n        help=\"where to save the config when `-sc` is provided, defaults to `experiments/&lt;RUN_NAME&gt;/config_&lt;RUN_NAME&gt;.yaml`\",\n    )\n    parser.add_argument(\"-s\", \"--seed\", help=\"specify a seed for reproducibility\")\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup","title":"<code>module_setup</code>","text":""},{"location":"reference/cli/#nhssynth.cli.module_setup.ModuleConfig","title":"<code> ModuleConfig        </code>","text":"Source code in <code>cli/module_setup.py</code> <pre><code>class ModuleConfig:\n    def __init__(\n        self,\n        func: Callable[..., Any],\n        add_args_func: Callable[..., Any],\n        description: str,\n        help: str,\n    ) -&gt; None:\n\"\"\"\n        Represents a module's configuration, containing the following attributes:\n\n        Args:\n            func: A callable that executes the module's functionality.\n            add_args_func: A callable that populates the module's sub-parser arguments.\n            description: A description of the module's functionality.\n            help: A help message for the module's command-line interface.\n\n        Returns:\n            None\n        \"\"\"\n        self.func = func\n        self.add_args_func = add_args_func\n        self.description = description\n        self.help = help\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.ModuleConfig.__init__","title":"<code>__init__(self, func, add_args_func, description, help)</code>  <code>special</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>A callable that executes the module's functionality.</p> required <code>add_args_func</code> <code>Callable[..., Any]</code> <p>A callable that populates the module's sub-parser arguments.</p> required <code>description</code> <code>str</code> <p>A description of the module's functionality.</p> required <code>help</code> <code>str</code> <p>A help message for the module's command-line interface.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>cli/module_setup.py</code> <pre><code>def __init__(\n    self,\n    func: Callable[..., Any],\n    add_args_func: Callable[..., Any],\n    description: str,\n    help: str,\n) -&gt; None:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n\n    Args:\n        func: A callable that executes the module's functionality.\n        add_args_func: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n\n    Returns:\n        None\n    \"\"\"\n    self.func = func\n    self.add_args_func = add_args_func\n    self.description = description\n    self.help = help\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\n    parser.add_argument(\n        \"-c\",\n        \"--input-config\",\n        required=True,\n        help=\"specify the config file name\",\n    )\n    parser.add_argument(\n        \"-cp\",\n        \"--custom-pipeline\",\n        action=\"store_true\",\n        help=\"infer a custom pipeline running order of modules from the config\",\n    )\n    for module_name in VALID_MODULES:\n        group = parser.add_argument_group(title=f\"{module_name} overrides\")\n        MODULE_MAP[module_name].add_args_func(group, override=True)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> for each module in the pipeline.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` for each module in the pipeline.\"\"\"\n    for module_name in PIPELINE:\n        group = parser.add_argument_group(title=module_name)\n        MODULE_MAP[module_name].add_args_func(group)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>config</code> <code>ModuleConfig</code> <p>A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_subparser(\n    subparsers: argparse._SubParsersAction,\n    name: str,\n    config: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        config: A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.\n\n    Returns:\n        The newly created subparser.\n    \"\"\"\n    parser = subparsers.add_parser(\n        name=name,\n        description=config.description,\n        help=config.help,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    config.add_args_func(parser)\n    parser.set_defaults(func=config.func)\n    return parser\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\n    print(\"Running full pipeline...\")\n    for module_name in PIPELINE:\n        MODULE_MAP[module_name].func(args)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.run","title":"<code>run</code>","text":""},{"location":"reference/cli/#nhssynth.cli.run.run","title":"<code>run()</code>","text":"<p>CLI for preparing, training and evaluating a synthetic data generator.</p> Source code in <code>cli/run.py</code> <pre><code>def run() -&gt; None:\n\"\"\"CLI for preparing, training and evaluating a synthetic data generator.\"\"\"\n\n    parser = argparse.ArgumentParser(\n        prog=\"nhssynth\", description=\"CLI for preparing, training and evaluating a synthetic data generator.\"\n    )\n    add_top_level_args(parser)\n\n    # Below we instantiate one subparser for each module + one for running with a config file and one for\n    # doing a full pipeline run with CLI-specified config\n    subparsers = parser.add_subparsers()\n\n    # TODO can probably do this better as we dont actually need the `pipeline` or `config` subparsers in this dict\n    all_subparsers = {\n        name: add_subparser(subparsers, name, option_config) for name, option_config in MODULE_MAP.items()\n    }\n\n    args = parser.parse_args()\n\n    # Use get to return None when no function has been set, i.e. user made no running choice\n    executor = vars(args).get(\"func\")\n\n    # If `config` is the specified running choice, we mutate `args` in `read_config`\n    # else we execute according to the user's choice\n    # else we return `--help` if no choice has been passed, i.e. executor is None\n    if not executor:\n        args = read_config(args, parser, all_subparsers)\n    elif executor:\n        executor(args)\n    else:\n        parser.parse_args([\"--help\"])\n\n    # Whenever either are specified, we want to dump the configuration to allow for this run to be replicated\n    if args.save_config or args.save_config_path:\n        if not args.save_config_path:\n            args.save_config_path = f\"experiments/{args.run_name}/config_{args.run_name}.yaml\"\n        write_config(args, all_subparsers)\n\n    print(\"Complete!\")\n</code></pre>"},{"location":"reference/cli/config/","title":"config","text":""},{"location":"reference/cli/config/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>cli/config.py</code> <pre><code>def assemble_config(\n    args: argparse.Namespace,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.\n\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\n    args_dict = vars(args)\n    modules_to_run = args_dict.pop(\"modules_to_run\", None)\n    if not modules_to_run:\n        run_type = get_key_by_value({mn: mc.func for mn, mc in ms.MODULE_MAP.items()}, args_dict[\"func\"])\n        modules_to_run = ms.PIPELINE if run_type == \"pipeline\" else run_type\n    elif len(modules_to_run) == 1:\n        run_type = modules_to_run[0]\n    elif modules_to_run == ms.PIPELINE:\n        run_type = \"pipeline\"\n    else:\n        raise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n\n    # Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\n    module_args = {\n        module_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\n        for module_name in modules_to_run\n    }\n\n    # Use the flat namespace to populate a nested (by module) dictionary of config args and values\n    out_dict = {}\n    for module_name in modules_to_run:\n        for k in args_dict.copy().keys():\n            if k in module_args[module_name]:\n                if out_dict.get(module_name):\n                    out_dict[module_name].update({k: args_dict.pop(k)})\n                else:\n                    out_dict[module_name] = {k: args_dict.pop(k)}\n\n    # Assemble the final dictionary in YAML-compliant form\n    return {\n        **({\"run_type\": run_type} if run_type else {}),\n        **{k: v for k, v in args_dict.items() if k not in {\"func\", \"run_name\", \"save_config\", \"save_config_path\"}},\n        **out_dict,\n    }\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>ArgumentParser</code> <p>The top-level parser.</p> required <code>module_parsers</code> <code>list</code> <p>The list of module-level parsers.</p> required <p>Returns:</p> Type Description <code>A tuple containing two elements</code> <ul> <li>A dictionary containing all arguments and their default values.<ul> <li>A list of the names of the required arguments.</li> </ul> </li> </ul> Source code in <code>cli/config.py</code> <pre><code>def get_default_and_required_args(\n    top_parser: argparse.ArgumentParser,\n    module_parsers: list[argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n\n    Args:\n        top_parser: The top-level parser.\n        module_parsers: The list of module-level parsers.\n\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of the names of the required arguments.\n    \"\"\"\n    all_actions = top_parser._actions + [action for sub_parser in module_parsers for action in sub_parser._actions]\n    defaults = {}\n    required_args = []\n    for action in all_actions:\n        if action.dest not in [\"help\", \"==SUPPRESS==\"]:\n            defaults[action.dest] = action.default\n            if action.required:\n                required_args.append(action.dest)\n    return defaults, required_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config Namespace object for the inferred modules to run and executes.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default config from the parser and <code>all_subparsers</code> for the <code>modules_to_run</code></li> <li>Overwrite this config with the specified subset (or full set) of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting config</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>ArgumentParser</code> <p>top-level ArgumentParser object</p> required <code>all_subparsers</code> <code>dict</code> <p>dictionary of ArgumentParser objects, one for each module</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>Namespace object containing the assembled configuration settings</p> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file</p> Source code in <code>cli/config.py</code> <pre><code>def read_config(\n    args: argparse.Namespace,\n    parser: argparse.ArgumentParser,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config Namespace object for the inferred modules to run and executes.\n\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default config from the parser and `all_subparsers` for the `modules_to_run`\n    4. Overwrite this config with the specified subset (or full set) of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting config\n\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level ArgumentParser object\n        all_subparsers: dictionary of ArgumentParser objects, one for each module\n\n    Returns:\n        Namespace object containing the assembled configuration settings\n\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file\n    \"\"\"\n    # Open the passed yaml file and load into a dictionary\n    with open(f\"config/{args.input_config}.yaml\") as stream:\n        config_dict = yaml.safe_load(stream)\n\n    valid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\n\n    run_type = config_dict.pop(\"run_type\", None)\n    # TODO Check this covers all bases\n    if run_type == \"pipeline\":\n        modules_to_run = ms.PIPELINE\n    else:\n        modules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\n        if not args.custom_pipeline:\n            modules_to_run = sorted(modules_to_run, key=lambda x: ms.PIPELINE.index(x))\n\n    if not modules_to_run:\n        warnings.warn(\n            \"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n        )\n        modules_to_run = ms.PIPELINE\n\n    # Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\n    args_dict, required_args = get_default_and_required_args(\n        parser, [all_subparsers[module_name] for module_name in modules_to_run]\n    )\n\n    # Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\n    non_default_passed_args_dict = {\n        k: v\n        for k, v in vars(args).items()\n        if k in [\"input_config\", \"custom_pipeline\"] or (k != \"func\" and v != args_dict[k])\n    }\n\n    # Overwrite the default arguments with the ones from the yaml file\n    args_dict.update(flatten_dict(config_dict))\n\n    # Overwrite the result of the above with any non-default CLI args\n    args_dict.update(non_default_passed_args_dict)\n\n    # Create a new Namespace using the assembled dictionary\n    new_args = argparse.Namespace(**args_dict)\n    assert all(\n        getattr(new_args, req_arg) for req_arg in required_args\n    ), \"Required arguments are missing from the passed config file\"\n\n    # Run the appropriate execution function(s)\n    for module in modules_to_run:\n        ms.MODULE_MAP[module].func(new_args)\n\n    new_args.modules_to_run = modules_to_run\n    return new_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary containing all subparsers for the config args.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>cli/config.py</code> <pre><code>def write_config(\n    args: argparse.Namespace,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO Maybe build a function that does this kind of filtering\n    if args.sdv_workflow:\n        del args.synthesizer\n    args_dict = assemble_config(args, all_subparsers)\n    with open(f\"{args.save_config_path}\", \"w\") as yaml_file:\n        yaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/module_arguments/","title":"module_arguments","text":""},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\n    parser.add_argument(\n        \"-i\",\n        \"--input-file\",\n        required=(not override),\n        help=\"the name of the `.csv` file to prepare\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output-file\",\n        default=\"_prepared\",\n        help=\"where to write the prepared data, defaults to `experiments/&lt;args.run_name&gt;/&lt;args.input_file&gt;_prepared.csv`\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--dir\",\n        default=\"./data\",\n        help=\"the directory to read and write data from and to\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--metadata-file\",\n        default=\"_metadata\",\n        help=\"metadata for the input data, defaults to `&lt;args.dir&gt;/&lt;args.input_file&gt;_metadata.yaml`\",\n    )\n    parser.add_argument(\n        \"-ic\",\n        \"--index-col\",\n        default=None,\n        choices=[None, 0],\n        help=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n    )\n    parser.add_argument(\n        \"-sdv\",\n        \"--sdv-workflow\",\n        action=\"store_true\",\n        help=\"utilise the SDV synthesizer workflow for transformation and metadata, rather than a `HyperTransformer` from RDT\",\n    )\n    parser.add_argument(\n        \"-ant\",\n        \"--allow-null-transformers\",\n        action=\"store_true\",\n        help=\"allow null / None transformers, i.e. leave some columns as they are\",\n    )\n    parser.add_argument(\n        \"-cy\",\n        \"--collapse-yaml\",\n        action=\"store_true\",\n        help=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n    )\n    parser.add_argument(\n        \"-is\",\n        \"--imputation-strategy\",\n        default=\"mean\",\n        choices=[\"mean\", \"median\", \"cull\"],\n        help=\"imputation strategy for missing values\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--synthesizer\",\n        default=\"TVAE\",\n        choices=list(SDV_SYNTHESIZER_CHOICES.keys()),\n        help=\"pick a synthesizer to use (note this can also be specified in the model module, these must match)\",\n    )\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_top_level_args","title":"<code>add_top_level_args(parser)</code>","text":"<p>Adds top-level arguments to an existing ArgumentParser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_top_level_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds top-level arguments to an existing ArgumentParser instance.\"\"\"\n    parser.add_argument(\n        \"-rn\",\n        \"--run-name\",\n        default=TIME,\n        help=f\"name the run to affect logging, config and outputs, defaults to current time, i.e. `{TIME}`\",\n    )\n    parser.add_argument(\"-sc\", \"--save-config\", action=\"store_true\", help=\"save the config provided via the cli\")\n    parser.add_argument(\n        \"-scp\",\n        \"--save-config-path\",\n        help=\"where to save the config when `-sc` is provided, defaults to `experiments/&lt;RUN_NAME&gt;/config_&lt;RUN_NAME&gt;.yaml`\",\n    )\n    parser.add_argument(\"-s\", \"--seed\", help=\"specify a seed for reproducibility\")\n</code></pre>"},{"location":"reference/cli/module_setup/","title":"module_setup","text":""},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig","title":"<code> ModuleConfig        </code>","text":"Source code in <code>cli/module_setup.py</code> <pre><code>class ModuleConfig:\n    def __init__(\n        self,\n        func: Callable[..., Any],\n        add_args_func: Callable[..., Any],\n        description: str,\n        help: str,\n    ) -&gt; None:\n\"\"\"\n        Represents a module's configuration, containing the following attributes:\n\n        Args:\n            func: A callable that executes the module's functionality.\n            add_args_func: A callable that populates the module's sub-parser arguments.\n            description: A description of the module's functionality.\n            help: A help message for the module's command-line interface.\n\n        Returns:\n            None\n        \"\"\"\n        self.func = func\n        self.add_args_func = add_args_func\n        self.description = description\n        self.help = help\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig.__init__","title":"<code>__init__(self, func, add_args_func, description, help)</code>  <code>special</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>A callable that executes the module's functionality.</p> required <code>add_args_func</code> <code>Callable[..., Any]</code> <p>A callable that populates the module's sub-parser arguments.</p> required <code>description</code> <code>str</code> <p>A description of the module's functionality.</p> required <code>help</code> <code>str</code> <p>A help message for the module's command-line interface.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>cli/module_setup.py</code> <pre><code>def __init__(\n    self,\n    func: Callable[..., Any],\n    add_args_func: Callable[..., Any],\n    description: str,\n    help: str,\n) -&gt; None:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n\n    Args:\n        func: A callable that executes the module's functionality.\n        add_args_func: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n\n    Returns:\n        None\n    \"\"\"\n    self.func = func\n    self.add_args_func = add_args_func\n    self.description = description\n    self.help = help\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\n    parser.add_argument(\n        \"-c\",\n        \"--input-config\",\n        required=True,\n        help=\"specify the config file name\",\n    )\n    parser.add_argument(\n        \"-cp\",\n        \"--custom-pipeline\",\n        action=\"store_true\",\n        help=\"infer a custom pipeline running order of modules from the config\",\n    )\n    for module_name in VALID_MODULES:\n        group = parser.add_argument_group(title=f\"{module_name} overrides\")\n        MODULE_MAP[module_name].add_args_func(group, override=True)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> for each module in the pipeline.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` for each module in the pipeline.\"\"\"\n    for module_name in PIPELINE:\n        group = parser.add_argument_group(title=module_name)\n        MODULE_MAP[module_name].add_args_func(group)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>config</code> <code>ModuleConfig</code> <p>A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_subparser(\n    subparsers: argparse._SubParsersAction,\n    name: str,\n    config: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        config: A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.\n\n    Returns:\n        The newly created subparser.\n    \"\"\"\n    parser = subparsers.add_parser(\n        name=name,\n        description=config.description,\n        help=config.help,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    config.add_args_func(parser)\n    parser.set_defaults(func=config.func)\n    return parser\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\n    print(\"Running full pipeline...\")\n    for module_name in PIPELINE:\n        MODULE_MAP[module_name].func(args)\n</code></pre>"},{"location":"reference/cli/run/","title":"run","text":""},{"location":"reference/cli/run/#nhssynth.cli.run.run","title":"<code>run()</code>","text":"<p>CLI for preparing, training and evaluating a synthetic data generator.</p> Source code in <code>cli/run.py</code> <pre><code>def run() -&gt; None:\n\"\"\"CLI for preparing, training and evaluating a synthetic data generator.\"\"\"\n\n    parser = argparse.ArgumentParser(\n        prog=\"nhssynth\", description=\"CLI for preparing, training and evaluating a synthetic data generator.\"\n    )\n    add_top_level_args(parser)\n\n    # Below we instantiate one subparser for each module + one for running with a config file and one for\n    # doing a full pipeline run with CLI-specified config\n    subparsers = parser.add_subparsers()\n\n    # TODO can probably do this better as we dont actually need the `pipeline` or `config` subparsers in this dict\n    all_subparsers = {\n        name: add_subparser(subparsers, name, option_config) for name, option_config in MODULE_MAP.items()\n    }\n\n    args = parser.parse_args()\n\n    # Use get to return None when no function has been set, i.e. user made no running choice\n    executor = vars(args).get(\"func\")\n\n    # If `config` is the specified running choice, we mutate `args` in `read_config`\n    # else we execute according to the user's choice\n    # else we return `--help` if no choice has been passed, i.e. executor is None\n    if not executor:\n        args = read_config(args, parser, all_subparsers)\n    elif executor:\n        executor(args)\n    else:\n        parser.parse_args([\"--help\"])\n\n    # Whenever either are specified, we want to dump the configuration to allow for this run to be replicated\n    if args.save_config or args.save_config_path:\n        if not args.save_config_path:\n            args.save_config_path = f\"experiments/{args.run_name}/config_{args.run_name}.yaml\"\n        write_config(args, all_subparsers)\n\n    print(\"Complete!\")\n</code></pre>"},{"location":"reference/modules/","title":"modules","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader","title":"<code>dataloader</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.io","title":"<code>io</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.io.check_ending","title":"<code>check_ending(fn, ending='.csv')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".csv\".</p> <code>'.csv'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_ending(fn: str, ending: str = \".csv\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, appends `ending`.\n\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".csv\".\n\n    Returns:\n        str: The filename with the correct ending.\n    \"\"\"\n    return fn if fn.endswith(ending) else fn + ending\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.io.setup_io","title":"<code>setup_io(fn_in, fn_out, fn_metadata, dir_data, run_name, dir_exp='experiments')</code>","text":"<p>Formats the input and output filenames and directories for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_in</code> <code>str</code> <p>The input data filename.</p> required <code>fn_out</code> <code>str</code> <p>The output data filename / suffix to append to <code>fn_in</code>.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_in</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory containing the data.</p> required <code>run_name</code> <code>str</code> <p>The name of the experiment run.</p> required <code>dir_exp</code> <code>str</code> <p>The parent directory for the experiment folders. Default is \"experiments\".</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>tuple[Path, Path, Path, Path]</code> <p>A tuple containing the formatted input, output, and metadata (in and out) paths.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_in</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_out</code> includes directory separators, as only the name of the output file will be used.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def setup_io(\n    fn_in: str,\n    fn_out: str,\n    fn_metadata: str,\n    dir_data: str,\n    run_name: str,\n    dir_exp: str = \"experiments\",\n) -&gt; tuple[Path, Path, Path, Path]:\n\"\"\"\n    Formats the input and output filenames and directories for an experiment.\n\n    Args:\n        fn_in: The input data filename.\n        fn_out: The output data filename / suffix to append to `fn_in`.\n        fn_metadata: The metadata filename / suffix to append to `fn_in`.\n        dir_data: The directory containing the data.\n        run_name: The name of the experiment run.\n        dir_exp: The parent directory for the experiment folders. Default is \"experiments\".\n\n    Returns:\n        tuple[Path, Path, Path, Path]: A tuple containing the formatted input, output, and metadata (in and out) paths.\n\n    Warnings:\n        Raises a UserWarning when the path to `fn_in` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_out` includes directory separators, as only the name of the output file will be used.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\n    # ensure .csv ending consistency\n    fn_in, fn_out, fn_metadata = check_ending(fn_in), check_ending(fn_out), check_ending(fn_metadata, ending=\".yaml\")\n\n    dir_data = Path(dir_data)\n\n    # check if `fn_out` and `fn_metadata` are given as suffixes (start with an underscore) to append to `fn_in`\n    # if not assume it is a name in its own right\n    if fn_out[0] == \"_\":\n        fn_out = check_ending(fn_in[:-4] + fn_out)\n    if fn_metadata[0] == \"_\":\n        fn_metadata = check_ending(fn_in[:-4] + fn_metadata, ending=\".yaml\")\n\n    if \"/\" in fn_in:\n        fn_in = Path(fn_in)\n        warnings.warn(\n            f\"Using the path supplied to `--input-file` appended to `--dir`, i.e. attempting to read data from {dir_data / fn_in},\\nto avoid this warning, specify the path using `--dir` and only the name using `--input-file`\\ne.g. `... --dir {(dir_data / fn_in).parent} --input-file {fn_in.name} ...`\",\n            UserWarning,\n        )\n\n    # generate timestamped experiment folder\n    dir_exp = Path(dir_exp) / run_name\n\n    if \"/\" in fn_out:\n        fn_out = Path(fn_out).name\n        warnings.warn(\n            f\"Paths are not supported via `--output-file`, using the name part instead, i.e. attempting to write data to {dir_exp / fn_out}\",\n            UserWarning,\n        )\n\n    if \"/\" in fn_metadata:\n        fn_metadata = Path(fn_metadata)\n        warnings.warn(\n            f\"Using the path supplied to `--metadata` appended to `--dir`, i.e. attempting to read data from {dir_data / fn_metadata},\\nto avoid this warning, specify the path using `--dir` and only the name using `--metadata`\\ne.g. `... --dir {(dir_data / fn_metadata).parent} --metadata {fn_metadata.name} ...`\",\n            UserWarning,\n        )\n\n    # Make the experiment directory\n    dir_exp.mkdir(parents=True, exist_ok=True)\n\n    return dir_data / fn_in, dir_exp / fn_out, dir_data / fn_metadata, dir_exp / fn_metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata","title":"<code>metadata</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.assemble_metadata","title":"<code>assemble_metadata(dtypes, metatransformer, sdv_workflow)</code>","text":"<p>Constructs a metadata dictionary from a list of data types and a metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>dtypes</code> <code>dict</code> <p>A dictionary mapping column names of the input data to their assigned data types.</p> required <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A meta-transformer used to create synthetic data. - If <code>sdv_workflow</code> is True, <code>metatransformer</code> should be an SDV single-table synthesizer. - If <code>sdv_workflow</code> is False, <code>metatransformer</code> should be an RDT HyperTransformer object   wrapping a dictionary containing transformers and sdtypes for each column.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys:     - dtype: The name of the data type for the column.     - sdtype: The data type for the column (only present if <code>sdv_workflow</code> is False).     - transformer: A dictionary containing information about the transformer       used for the column (if any). The dictionary has the following keys:       - name: The name of the transformer.       - Any other properties of the transformer that are not private or set by a random seed.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def assemble_metadata(\n    dtypes: dict[str, type],\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    sdv_workflow: bool,\n) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Constructs a metadata dictionary from a list of data types and a metatransformer.\n\n    Args:\n        dtypes: A dictionary mapping column names of the input data to their assigned data types.\n        metatransformer: A meta-transformer used to create synthetic data.\n            - If `sdv_workflow` is True, `metatransformer` should be an SDV single-table synthesizer.\n            - If `sdv_workflow` is False, `metatransformer` should be an RDT HyperTransformer object\n              wrapping a dictionary containing transformers and sdtypes for each column.\n        sdv_workflow: A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.\n\n    Returns:\n        dict[str, dict[str, Any]]: A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The name of the data type for the column.\n            - sdtype: The data type for the column (only present if `sdv_workflow` is False).\n            - transformer: A dictionary containing information about the transformer\n              used for the column (if any). The dictionary has the following keys:\n              - name: The name of the transformer.\n              - Any other properties of the transformer that are not private or set by a random seed.\n    \"\"\"\n    if sdv_workflow:\n        sdmetadata = metatransformer.metadata\n        transformers = metatransformer.get_transformers()\n        metadata = {\n            cn: {\n                **cd,\n                **{\n                    \"transformer\": {\n                        \"name\": type(transformers[cn]).__name__,\n                        **filter_dict(\n                            transformers[cn].__dict__,\n                            {\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n                        ),\n                    }\n                    if transformers[cn]\n                    else None,\n                    \"dtype\": dtypes[cn].name if not isinstance(dtypes[cn], str) else dtypes[cn],\n                },\n            }\n            for cn, cd in sdmetadata.columns.items()\n        }\n    else:\n        config = metatransformer.get_config()\n        metadata = {\n            cn: {\n                \"sdtype\": cd,\n                \"transformer\": {\n                    \"name\": type(config[\"transformers\"][cn]).__name__,\n                    **filter_dict(\n                        config[\"transformers\"][cn].__dict__,\n                        {\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n                    ),\n                }\n                if config[\"transformers\"][cn]\n                else None,\n                \"dtype\": dtypes[cn].name if not isinstance(dtypes[cn], str) else dtypes[cn],\n            }\n            for cn, cd in config[\"sdtypes\"].items()\n        }\n    return metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the metadata correspond to valid columns in the DataFrame. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the DataFrame.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns in metadata are not present in the DataFrame.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the metadata correspond to valid columns in the DataFrame.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n\n    Args:\n        metadata: A dictionary containing metadata for the columns in the DataFrame.\n        data: The DataFrame to check against the metadata.\n\n    Raises:\n        AssertionError: If any columns in metadata are not present in the DataFrame.\n    \"\"\"\n    assert all([k in data.columns for k in metadata.keys()])\n    metadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # columns that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.\n\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # columns that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\n    c_index = 1\n    column_types = {}\n    t_index = 1\n    transformers = {}\n    for cn, cd in metadata.items():\n\n        if cd not in column_types.values():\n            column_types[c_index] = cd.copy()\n            metadata[cn] = column_types[c_index]\n            c_index += 1\n        else:\n            cix = get_key_by_value(column_types, cd)\n            metadata[cn] = column_types[cix]\n\n        if cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\n            transformers[t_index] = cd[\"transformer\"].copy()\n            metadata[cn][\"transformer\"] = transformers[t_index]\n            t_index += 1\n        elif cd[\"transformer\"]:\n            tix = get_key_by_value(transformers, cd[\"transformer\"])\n            metadata[cn][\"transformer\"] = transformers[tix]\n\n    return {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame for which an empty metadata dictionary is created.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n\n    Args:\n        data: The DataFrame for which an empty metadata dictionary is created.\n\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\n    return {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.instantiate_dtypes","title":"<code>instantiate_dtypes(metadata, data)</code>","text":"<p>Instantiate the data types for each column based on the given metadata.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata information for each column, including the data type.</p> required <code>data</code> <code>DataFrame</code> <p>A pandas DataFrame containing the data to be instantiated.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the instantiated data types for each column.</p> <p>Exceptions:</p> Type Description <code>UserWarning</code> <p>If incomplete metadata is detected, i.e., if there are columns with missing 'dtype' information.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def instantiate_dtypes(metadata: dict[str, dict], data: pd.DataFrame) -&gt; dict[str, np.dtype]:\n\"\"\"\n    Instantiate the data types for each column based on the given metadata.\n\n    Args:\n        metadata: A dictionary containing metadata information for each column, including the data type.\n        data: A pandas DataFrame containing the data to be instantiated.\n\n    Returns:\n        A dictionary containing the instantiated data types for each column.\n\n    Raises:\n        UserWarning: If incomplete metadata is detected, i.e., if there are columns with missing 'dtype' information.\n    \"\"\"\n    dtypes = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\n    if not all(dtypes.values()):\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in dtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        dtypes.update({cn: data[cn].dtype for cn, cv in dtypes.items() if not cv})\n    return dtypes\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code> DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code> DataFrame.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data` DataFrame.\n\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n\n    Returns:\n        A metadata dictionary containing information about the columns in the `data` DataFrame.\n    \"\"\"\n    if in_path.exists():\n        with open(in_path) as stream:\n            metadata = yaml.safe_load(stream)\n        metadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\n    else:\n        metadata = create_empty_metadata(data)\n    check_metadata_columns(metadata, data)\n    return metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, dtypes, metatransformer, sdv_workflow=True, collapse_yaml=True)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>dtypes</code> <code>dict</code> <p>A dictionary mapping column names of the input data to their assigned data types.</p> required <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>The synthesizer or hypertransformer that was used to transform the data.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.</p> <code>True</code> <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\n    out_path: pathlib.Path,\n    dtypes: dict[str, Any],\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    sdv_workflow: bool = True,\n    collapse_yaml: bool = True,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        dtypes: A dictionary mapping column names of the input data to their assigned data types.\n        metatransformer: The synthesizer or hypertransformer that was used to transform the data.\n        sdv_workflow: A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n\n    Returns:\n        None\n    \"\"\"\n    metadata = assemble_metadata(dtypes, metatransformer, sdv_workflow)\n\n    if collapse_yaml:\n        metadata = collapse(metadata)\n\n    with open(out_path, \"w\") as yaml_file:\n        yaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.run","title":"<code>run</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; None:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.\n\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n\n    Returns:\n        None\n    \"\"\"\n\n    print(\"Running dataloader...\")\n\n    if args.seed:\n        np.random.seed(args.seed)\n\n    input_path, output_path, metadata_input_path, metadata_output_path = setup_io(\n        args.input_file, args.output_file, args.metadata_file, args.dir, args.run_name\n    )\n\n    # Load the dataset and accompanying metadata\n    input = pd.read_csv(input_path, index_col=args.index_col)\n    metadata = load_metadata(metadata_input_path, input)\n\n    # Setup the input data dtypes and apply them\n    dtypes = instantiate_dtypes(metadata, input)\n    typed_input = input.astype(dtypes)\n\n    # Setup the metatransformer\n    metatransformer = instantiate_metatransformer(\n        metadata,\n        typed_input,\n        args.sdv_workflow,\n        args.allow_null_transformers,\n        SDV_SYNTHESIZER_CHOICES[args.synthesizer],\n    )\n\n    # Output the metadata corresponding to `transformed_input`, for reproducibility\n    output_metadata(metadata_output_path, dtypes, metatransformer, args.sdv_workflow, args.collapse_yaml)\n\n    print(\"Transforming input...\")\n    transformed_input = apply_transformer(metatransformer, typed_input, args.sdv_workflow)\n\n    # Write the transformed input to the appropriate file\n    print(\"Writing output\")\n    transformed_input.to_csv(output_path, index=False)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.transformers","title":"<code>transformers</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.transformers.apply_transformer","title":"<code>apply_transformer(metatransformer, typed_input, sdv_workflow)</code>","text":"<p>Applies a metatransformer to the typed input data.</p> <p>Parameters:</p> Name Type Description Default <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A metatransformer object that can be either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV.</p> required <code>typed_input</code> <code>DataFrame</code> <p>The typed input data.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean flag indicating whether to use the <code>preprocess()</code> method of the <code>metatransformer</code> if it's an <code>SDV</code> synthesizer, or the <code>fit_transform()</code> method if it's an <code>RDT</code> transformer.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def apply_transformer(\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    typed_input: pd.DataFrame,\n    sdv_workflow: bool,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Applies a metatransformer to the typed input data.\n\n    Args:\n        metatransformer: A metatransformer object that can be either a `HyperTransformer` from RDT or a `BaseSingleTableSynthesizer` from SDV.\n        typed_input: The typed input data.\n        sdv_workflow: A boolean flag indicating whether to use the `preprocess()` method of the `metatransformer` if it's an `SDV` synthesizer, or the `fit_transform()` method if it's an `RDT` transformer.\n\n    Returns:\n        The transformed data.\n    \"\"\"\n    if sdv_workflow:\n        transformed_input = metatransformer.preprocess(typed_input)\n    else:\n        transformed_input = metatransformer.fit_transform(typed_input)\n    return transformed_input\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.transformers.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object extracted from the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>Optional[sdv.single_table.base.BaseSingleTableSynthesizer]</code> <p>A callable object (transformer) if the dictionary contains transformer data, else None.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def get_transformer(d: dict) -&gt; Optional[BaseSingleTableSynthesizer]:\n\"\"\"\n    Return a callable transformer object extracted from the given dictionary.\n\n    Args:\n        d: A dictionary containing the transformer data.\n\n    Returns:\n        A callable object (transformer) if the dictionary contains transformer data, else None.\n    \"\"\"\n    transformer_data = d.get(\"transformer\", None)\n    if isinstance(transformer_data, dict):\n        # Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\n        transformer_data = transformer_data.copy()\n        transformer_name = transformer_data.pop(\"name\", None)\n        transformer = eval(transformer_name)(**transformer_data) if transformer_name else None\n    else:\n        transformer = eval(transformer_data)() if transformer_data else None\n    return transformer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.transformers.instantiate_hypertransformer","title":"<code>instantiate_hypertransformer(sdtypes, transformers, data, allow_null_transformers)</code>","text":"<p>Instantiates a HyperTransformer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <code>dict</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <code>dict</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>HyperTransformer</code> <p>A HyperTransformer object instantiated from the given metadata and data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_hypertransformer(\n    sdtypes: dict[str, dict],\n    transformers: dict[str, Optional[BaseTransformer]],\n    data: pd.DataFrame,\n    allow_null_transformers: bool,\n) -&gt; HyperTransformer:\n\"\"\"\n    Instantiates a HyperTransformer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A HyperTransformer object instantiated from the given metadata and data.\n    \"\"\"\n    ht = HyperTransformer()\n    if all(sdtypes.values()) and (all(transformers.values()) or allow_null_transformers):\n        ht.set_config(\n            config={\n                \"sdtypes\": {k: v[\"sdtype\"] for k, v in sdtypes.items()},\n                \"transformers\": transformers,\n            }\n        )\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in sdtypes.items() if not v])) if not all(sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in transformers.items() if not v])) if not all(transformers.values()) and not allow_null_transformers else ''} automatically...\",\n            UserWarning,\n        )\n        ht.detect_initial_config(data)\n        ht.update_sdtypes({k: v[\"sdtype\"] for k, v in sdtypes.items() if v})\n        ht.update_transformers(\n            transformers if allow_null_transformers else {k: v for k, v in transformers.items() if v}\n        )\n    return ht\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.transformers.instantiate_metatransformer","title":"<code>instantiate_metatransformer(metadata, data, sdv_workflow, allow_null_transformers, Synthesizer)</code>","text":"<p>Instantiates a metatransformer based on the given metadata and input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing the metadata for the input data. Each key corresponds to a column name in the input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the name of the transformer to use for the column and its configuration (to be instantiated below).</p> required <code>data</code> <code>DataFrame</code> <p>The input data as a pandas DataFrame.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from RDT will be used instead.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A boolean flag indicating whether to allow transformers to be None. If True, a None value for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated for that column.</p> required <p>Returns:</p> Type Description <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A metatransformer object that can be either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_metatransformer(\n    metadata: dict[str, dict],\n    data: pd.DataFrame,\n    sdv_workflow: bool,\n    allow_null_transformers: bool,\n    Synthesizer: type[BaseSingleTableSynthesizer],\n) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n    Instantiates a metatransformer based on the given metadata and input data.\n\n    Args:\n        metadata: A dictionary containing the metadata for the input data. Each key corresponds to a column name in the\n            input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata\n            should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the\n            name of the transformer to use for the column and its configuration (to be instantiated below).\n        data: The input data as a pandas DataFrame.\n        sdv_workflow: A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the\n            TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from\n            RDT will be used instead.\n        allow_null_transformers: A boolean flag indicating whether to allow transformers to be None. If True, a None value\n            for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated\n            for that column.\n\n    Returns:\n        A metatransformer object that can be either a `HyperTransformer` from RDT or a `BaseSingleTableSynthesizer` from SDV.\n    \"\"\"\n    sdtypes = {cn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()}\n    transformers = {cn: get_transformer(cd) for cn, cd in metadata.items()}\n    if sdv_workflow:\n        metatransformer = instantiate_synthesizer(sdtypes, transformers, data, allow_null_transformers, Synthesizer)\n    else:\n        metatransformer = instantiate_hypertransformer(sdtypes, transformers, data, allow_null_transformers)\n    return metatransformer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.transformers.instantiate_synthesizer","title":"<code>instantiate_synthesizer(sdtypes, transformers, data, allow_null_transformers, Synthesizer)</code>","text":"<p>Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <code>dict</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <code>dict</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>BaseSingleTableSynthesizer</code> <p>A BaseSingleTableSynthesizer object instantiated from the given metadata and data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_synthesizer(\n    sdtypes: dict[str, dict],\n    transformers: dict[str, Optional[BaseTransformer]],\n    data: pd.DataFrame,\n    allow_null_transformers: bool,\n    Synthesizer: type[BaseSingleTableSynthesizer],\n) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n    Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A BaseSingleTableSynthesizer object instantiated from the given metadata and data.\n    \"\"\"\n    if all(sdtypes.values()):\n        metadata = SingleTableMetadata.load_from_dict({\"columns\": sdtypes})\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in sdtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        metadata = SingleTableMetadata()\n        metadata.detect_from_dataframe(data)\n        for column_name, values in sdtypes.items():\n            if values:\n                metadata.update_column(column_name=column_name, **values)\n    if not all(transformers.values()) and not allow_null_transformers:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in transformers.items() if not v]} automatically...\",\n            UserWarning,\n        )\n    synthesizer = Synthesizer(metadata)\n    synthesizer.auto_assign_transformers(data)\n    synthesizer.update_transformers(\n        transformers if allow_null_transformers else {k: v for k, v in transformers.items() if v}\n    )\n    return synthesizer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model","title":"<code>model</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.DPVAE","title":"<code>DPVAE</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\n\n    def __init__(\n        self,\n        latent_dim,\n        num_continuous,\n        num_categories=[0],\n        hidden_dim=32,\n        activation=nn.Tanh,\n        device=\"gpu\",\n    ):\n        super().__init__()\n\n        output_dim = num_continuous + sum(num_categories)\n        self.num_continuous = num_continuous\n        self.num_categories = num_categories\n\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, z):\n        return self.net(z)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\n    return self.net(z)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\n\n    def __init__(\n        self, input_dim, latent_dim, hidden_dim=32, activation=nn.Tanh, device=\"gpu\",\n    ):\n        super().__init__()\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        output_dim = 2 * latent_dim\n        self.latent_dim = latent_dim\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, x):\n        outs = self.net(x)\n        mu_z = outs[:, : self.latent_dim]\n        logsigma_z = outs[:, self.latent_dim :]\n        return mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\n    outs = self.net(x)\n    mu_z = outs[:, : self.latent_dim]\n    logsigma_z = outs[:, self.latent_dim :]\n    return mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\n    def __init__(self, num_continuous):\n        super().__init__()\n        self.output_logsigma_fn = nn.Linear(num_continuous, num_continuous, bias=True)\n        torch.nn.init.zeros_(self.output_logsigma_fn.weight)\n        torch.nn.init.zeros_(self.output_logsigma_fn.bias)\n        self.output_logsigma_fn.weight.requires_grad = False\n\n    def forward(self, X):\n        return self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\n    return self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\n\n    def __init__(self, encoder, decoder, lr=1e-3):\n        super().__init__()\n        self.encoder = encoder.to(encoder.device)\n        self.decoder = decoder.to(decoder.device)\n        self.device = encoder.device\n        self.num_categories = self.decoder.num_categories\n        self.num_continuous = self.decoder.num_continuous\n        self.noiser = Noiser(self.num_continuous).to(decoder.device)\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n        self.lr = lr\n\n    def reconstruct(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        x_recon = self.decoder(mu_z)\n        return x_recon\n\n    def generate(self, N):\n        z_samples = torch.randn_like(\n            torch.ones((N, self.encoder.latent_dim)), device=self.device\n        )\n        x_gen = self.decoder(z_samples)\n        x_gen_ = torch.ones_like(x_gen, device=self.device)\n        i = 0\n\n        for v in range(len(self.num_categories)):\n            x_gen_[\n                :, i : (i + self.num_categories[v])\n            ] = torch.distributions.one_hot_categorical.OneHotCategorical(\n                logits=x_gen[:, i : (i + self.num_categories[v])]\n            ).sample()\n            i = i + self.num_categories[v]\n\n        x_gen_[:, -self.num_continuous :] = x_gen[\n            :, -self.num_continuous :\n        ] + torch.exp(self.noiser(x_gen[:, -self.num_continuous :])) * torch.randn_like(\n            x_gen[:, -self.num_continuous :]\n        )\n        return x_gen_\n\n    def loss(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        p = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\n        q = Normal(mu_z, torch.exp(logsigma_z))\n\n        divergence_loss = torch.sum(torch.distributions.kl_divergence(q, p))\n\n        s = torch.randn_like(mu_z)\n        z_samples = mu_z + s * torch.exp(logsigma_z)\n\n        x_recon = self.decoder(z_samples)\n\n        categoric_loglik = 0\n        if sum(self.num_categories) != 0:\n            i = 0\n\n            for v in range(len(self.num_categories)):\n\n                categoric_loglik += -torch.nn.functional.cross_entropy(\n                    x_recon[:, i : (i + self.num_categories[v])],\n                    torch.max(X[:, i : (i + self.num_categories[v])], 1)[1],\n                ).sum()\n                i = i + self.decoder.num_categories[v]\n\n        gauss_loglik = 0\n        if self.decoder.num_continuous != 0:\n            gauss_loglik = (\n                Normal(\n                    loc=x_recon[:, -self.num_continuous :],\n                    scale=torch.exp(self.noiser(x_recon[:, -self.num_continuous :])),\n                )\n                .log_prob(X[:, -self.num_continuous :])\n                .sum()\n            )\n\n        reconstruct_loss = -(categoric_loglik + gauss_loglik)\n\n        elbo = divergence_loss + reconstruct_loss\n\n        return (elbo, reconstruct_loss, divergence_loss, categoric_loglik, gauss_loglik)\n\n    def train(\n        self,\n        x_dataloader,\n        n_epochs,\n        logging_freq=1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        # mean_norm = 0\n        # counter = 0\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(n_epochs):\n\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruc,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruc.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # counter += 1\n                # l2_norm = 0\n                # for p in self.parameters():\n                #     if p.requires_grad:\n                #         p_norm = p.grad.detach().data.norm(2)\n                #         l2_norm += p_norm.item() ** 2\n                # l2_norm = l2_norm ** 0.5  # / Y_subset.shape[0]\n                # mean_norm = (mean_norm * (counter - 1) + l2_norm) / counter\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n            # self.mean_norm = mean_norm\n\n            if stop_counter == patience:\n\n                n_epochs = epoch + 1\n\n                break\n\n        return (\n            n_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def diff_priv_train(\n        self,\n        x_dataloader,\n        n_epochs,\n        C=1e16,\n        noise_scale=None,\n        target_eps=1,\n        target_delta=1e-5,\n        logging_freq=1,\n        sample_rate=0.1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        if noise_scale is not None:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                noise_multiplier=noise_scale,\n                max_grad_norm=C,\n            )\n        else:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                target_epsilon=target_eps,\n                target_delta=target_delta,\n                epochs=n_epochs,\n                max_grad_norm=C,\n            )\n        self.privacy_engine.attach(self.optimizer)\n\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(n_epochs):\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n            # print(self.get_privacy_spent(target_delta))\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruct,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruct.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # print(self.get_privacy_spent(target_delta))\n                # print(loss.item())\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n\n            if stop_counter == patience:\n\n                n_epochs = epoch + 1\n                break\n\n        return (\n            n_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def get_privacy_spent(self, delta):\n        if hasattr(self, \"privacy_engine\"):\n            return self.privacy_engine.get_privacy_spent(delta)\n        else:\n            print(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n            )\n\n    def save(self, filename):\n        torch.save(self.state_dict(), filename)\n\n    def load(self, filename):\n        self.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, n_epochs, logging_freq=1, patience=5, delta=10, filepath=None)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\n    self,\n    x_dataloader,\n    n_epochs,\n    logging_freq=1,\n    patience=5,\n    delta=10,\n    filepath=None,\n):\n    # mean_norm = 0\n    # counter = 0\n    log_elbo = []\n    log_reconstruct = []\n    log_divergence = []\n    log_cat_loss = []\n    log_num_loss = []\n\n    # EARLY STOPPING #\n    min_elbo = 0.0  # For early stopping workflow\n    patience = patience  # How many epochs patience we give for early stopping\n    stop_counter = 0  # Counter for stops\n    delta = delta  # Difference in elbo value\n\n    for epoch in range(n_epochs):\n\n        train_loss = 0.0\n        divergence_epoch_loss = 0.0\n        reconstruction_epoch_loss = 0.0\n        categorical_epoch_reconstruct = 0.0\n        numerical_epoch_reconstruct = 0.0\n\n        for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n            self.optimizer.zero_grad()\n            (\n                elbo,\n                reconstruct_loss,\n                divergence_loss,\n                categorical_reconstruc,\n                numerical_reconstruct,\n            ) = self.loss(Y_subset.to(self.encoder.device))\n            elbo.backward()\n            self.optimizer.step()\n\n            train_loss += elbo.item()\n            divergence_epoch_loss += divergence_loss.item()\n            reconstruction_epoch_loss += reconstruct_loss.item()\n            categorical_epoch_reconstruct += categorical_reconstruc.item()\n            numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n            # counter += 1\n            # l2_norm = 0\n            # for p in self.parameters():\n            #     if p.requires_grad:\n            #         p_norm = p.grad.detach().data.norm(2)\n            #         l2_norm += p_norm.item() ** 2\n            # l2_norm = l2_norm ** 0.5  # / Y_subset.shape[0]\n            # mean_norm = (mean_norm * (counter - 1) + l2_norm) / counter\n\n        log_elbo.append(train_loss)\n        log_reconstruct.append(reconstruction_epoch_loss)\n        log_divergence.append(divergence_epoch_loss)\n        log_cat_loss.append(categorical_epoch_reconstruct)\n        log_num_loss.append(numerical_epoch_reconstruct)\n\n        if epoch == 0:\n\n            min_elbo = train_loss\n\n        if train_loss &lt; (min_elbo - delta):\n\n            min_elbo = train_loss\n            stop_counter = 0  # Set counter to zero\n            if filepath != None:\n                self.save(filepath)  # Save best model if we want to\n\n        else:  # elbo has not improved\n\n            stop_counter += 1\n\n        if epoch % logging_freq == 0:\n            print(\n                f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n            )\n            # print(f\"\\tMean norm: {mean_norm}\")\n        # self.mean_norm = mean_norm\n\n        if stop_counter == patience:\n\n            n_epochs = epoch + 1\n\n            break\n\n    return (\n        n_epochs,\n        log_elbo,\n        log_reconstruct,\n        log_divergence,\n        log_cat_loss,\n        log_num_loss,\n    )\n</code></pre>"},{"location":"reference/modules/dataloader/","title":"dataloader","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io","title":"<code>io</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.check_ending","title":"<code>check_ending(fn, ending='.csv')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".csv\".</p> <code>'.csv'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_ending(fn: str, ending: str = \".csv\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, appends `ending`.\n\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".csv\".\n\n    Returns:\n        str: The filename with the correct ending.\n    \"\"\"\n    return fn if fn.endswith(ending) else fn + ending\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.setup_io","title":"<code>setup_io(fn_in, fn_out, fn_metadata, dir_data, run_name, dir_exp='experiments')</code>","text":"<p>Formats the input and output filenames and directories for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_in</code> <code>str</code> <p>The input data filename.</p> required <code>fn_out</code> <code>str</code> <p>The output data filename / suffix to append to <code>fn_in</code>.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_in</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory containing the data.</p> required <code>run_name</code> <code>str</code> <p>The name of the experiment run.</p> required <code>dir_exp</code> <code>str</code> <p>The parent directory for the experiment folders. Default is \"experiments\".</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>tuple[Path, Path, Path, Path]</code> <p>A tuple containing the formatted input, output, and metadata (in and out) paths.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_in</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_out</code> includes directory separators, as only the name of the output file will be used.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def setup_io(\n    fn_in: str,\n    fn_out: str,\n    fn_metadata: str,\n    dir_data: str,\n    run_name: str,\n    dir_exp: str = \"experiments\",\n) -&gt; tuple[Path, Path, Path, Path]:\n\"\"\"\n    Formats the input and output filenames and directories for an experiment.\n\n    Args:\n        fn_in: The input data filename.\n        fn_out: The output data filename / suffix to append to `fn_in`.\n        fn_metadata: The metadata filename / suffix to append to `fn_in`.\n        dir_data: The directory containing the data.\n        run_name: The name of the experiment run.\n        dir_exp: The parent directory for the experiment folders. Default is \"experiments\".\n\n    Returns:\n        tuple[Path, Path, Path, Path]: A tuple containing the formatted input, output, and metadata (in and out) paths.\n\n    Warnings:\n        Raises a UserWarning when the path to `fn_in` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_out` includes directory separators, as only the name of the output file will be used.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\n    # ensure .csv ending consistency\n    fn_in, fn_out, fn_metadata = check_ending(fn_in), check_ending(fn_out), check_ending(fn_metadata, ending=\".yaml\")\n\n    dir_data = Path(dir_data)\n\n    # check if `fn_out` and `fn_metadata` are given as suffixes (start with an underscore) to append to `fn_in`\n    # if not assume it is a name in its own right\n    if fn_out[0] == \"_\":\n        fn_out = check_ending(fn_in[:-4] + fn_out)\n    if fn_metadata[0] == \"_\":\n        fn_metadata = check_ending(fn_in[:-4] + fn_metadata, ending=\".yaml\")\n\n    if \"/\" in fn_in:\n        fn_in = Path(fn_in)\n        warnings.warn(\n            f\"Using the path supplied to `--input-file` appended to `--dir`, i.e. attempting to read data from {dir_data / fn_in},\\nto avoid this warning, specify the path using `--dir` and only the name using `--input-file`\\ne.g. `... --dir {(dir_data / fn_in).parent} --input-file {fn_in.name} ...`\",\n            UserWarning,\n        )\n\n    # generate timestamped experiment folder\n    dir_exp = Path(dir_exp) / run_name\n\n    if \"/\" in fn_out:\n        fn_out = Path(fn_out).name\n        warnings.warn(\n            f\"Paths are not supported via `--output-file`, using the name part instead, i.e. attempting to write data to {dir_exp / fn_out}\",\n            UserWarning,\n        )\n\n    if \"/\" in fn_metadata:\n        fn_metadata = Path(fn_metadata)\n        warnings.warn(\n            f\"Using the path supplied to `--metadata` appended to `--dir`, i.e. attempting to read data from {dir_data / fn_metadata},\\nto avoid this warning, specify the path using `--dir` and only the name using `--metadata`\\ne.g. `... --dir {(dir_data / fn_metadata).parent} --metadata {fn_metadata.name} ...`\",\n            UserWarning,\n        )\n\n    # Make the experiment directory\n    dir_exp.mkdir(parents=True, exist_ok=True)\n\n    return dir_data / fn_in, dir_exp / fn_out, dir_data / fn_metadata, dir_exp / fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata","title":"<code>metadata</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.assemble_metadata","title":"<code>assemble_metadata(dtypes, metatransformer, sdv_workflow)</code>","text":"<p>Constructs a metadata dictionary from a list of data types and a metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>dtypes</code> <code>dict</code> <p>A dictionary mapping column names of the input data to their assigned data types.</p> required <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A meta-transformer used to create synthetic data. - If <code>sdv_workflow</code> is True, <code>metatransformer</code> should be an SDV single-table synthesizer. - If <code>sdv_workflow</code> is False, <code>metatransformer</code> should be an RDT HyperTransformer object   wrapping a dictionary containing transformers and sdtypes for each column.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys:     - dtype: The name of the data type for the column.     - sdtype: The data type for the column (only present if <code>sdv_workflow</code> is False).     - transformer: A dictionary containing information about the transformer       used for the column (if any). The dictionary has the following keys:       - name: The name of the transformer.       - Any other properties of the transformer that are not private or set by a random seed.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def assemble_metadata(\n    dtypes: dict[str, type],\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    sdv_workflow: bool,\n) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Constructs a metadata dictionary from a list of data types and a metatransformer.\n\n    Args:\n        dtypes: A dictionary mapping column names of the input data to their assigned data types.\n        metatransformer: A meta-transformer used to create synthetic data.\n            - If `sdv_workflow` is True, `metatransformer` should be an SDV single-table synthesizer.\n            - If `sdv_workflow` is False, `metatransformer` should be an RDT HyperTransformer object\n              wrapping a dictionary containing transformers and sdtypes for each column.\n        sdv_workflow: A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.\n\n    Returns:\n        dict[str, dict[str, Any]]: A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The name of the data type for the column.\n            - sdtype: The data type for the column (only present if `sdv_workflow` is False).\n            - transformer: A dictionary containing information about the transformer\n              used for the column (if any). The dictionary has the following keys:\n              - name: The name of the transformer.\n              - Any other properties of the transformer that are not private or set by a random seed.\n    \"\"\"\n    if sdv_workflow:\n        sdmetadata = metatransformer.metadata\n        transformers = metatransformer.get_transformers()\n        metadata = {\n            cn: {\n                **cd,\n                **{\n                    \"transformer\": {\n                        \"name\": type(transformers[cn]).__name__,\n                        **filter_dict(\n                            transformers[cn].__dict__,\n                            {\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n                        ),\n                    }\n                    if transformers[cn]\n                    else None,\n                    \"dtype\": dtypes[cn].name if not isinstance(dtypes[cn], str) else dtypes[cn],\n                },\n            }\n            for cn, cd in sdmetadata.columns.items()\n        }\n    else:\n        config = metatransformer.get_config()\n        metadata = {\n            cn: {\n                \"sdtype\": cd,\n                \"transformer\": {\n                    \"name\": type(config[\"transformers\"][cn]).__name__,\n                    **filter_dict(\n                        config[\"transformers\"][cn].__dict__,\n                        {\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n                    ),\n                }\n                if config[\"transformers\"][cn]\n                else None,\n                \"dtype\": dtypes[cn].name if not isinstance(dtypes[cn], str) else dtypes[cn],\n            }\n            for cn, cd in config[\"sdtypes\"].items()\n        }\n    return metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the metadata correspond to valid columns in the DataFrame. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the DataFrame.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns in metadata are not present in the DataFrame.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the metadata correspond to valid columns in the DataFrame.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n\n    Args:\n        metadata: A dictionary containing metadata for the columns in the DataFrame.\n        data: The DataFrame to check against the metadata.\n\n    Raises:\n        AssertionError: If any columns in metadata are not present in the DataFrame.\n    \"\"\"\n    assert all([k in data.columns for k in metadata.keys()])\n    metadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # columns that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.\n\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # columns that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\n    c_index = 1\n    column_types = {}\n    t_index = 1\n    transformers = {}\n    for cn, cd in metadata.items():\n\n        if cd not in column_types.values():\n            column_types[c_index] = cd.copy()\n            metadata[cn] = column_types[c_index]\n            c_index += 1\n        else:\n            cix = get_key_by_value(column_types, cd)\n            metadata[cn] = column_types[cix]\n\n        if cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\n            transformers[t_index] = cd[\"transformer\"].copy()\n            metadata[cn][\"transformer\"] = transformers[t_index]\n            t_index += 1\n        elif cd[\"transformer\"]:\n            tix = get_key_by_value(transformers, cd[\"transformer\"])\n            metadata[cn][\"transformer\"] = transformers[tix]\n\n    return {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame for which an empty metadata dictionary is created.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n\n    Args:\n        data: The DataFrame for which an empty metadata dictionary is created.\n\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\n    return {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.instantiate_dtypes","title":"<code>instantiate_dtypes(metadata, data)</code>","text":"<p>Instantiate the data types for each column based on the given metadata.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata information for each column, including the data type.</p> required <code>data</code> <code>DataFrame</code> <p>A pandas DataFrame containing the data to be instantiated.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the instantiated data types for each column.</p> <p>Exceptions:</p> Type Description <code>UserWarning</code> <p>If incomplete metadata is detected, i.e., if there are columns with missing 'dtype' information.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def instantiate_dtypes(metadata: dict[str, dict], data: pd.DataFrame) -&gt; dict[str, np.dtype]:\n\"\"\"\n    Instantiate the data types for each column based on the given metadata.\n\n    Args:\n        metadata: A dictionary containing metadata information for each column, including the data type.\n        data: A pandas DataFrame containing the data to be instantiated.\n\n    Returns:\n        A dictionary containing the instantiated data types for each column.\n\n    Raises:\n        UserWarning: If incomplete metadata is detected, i.e., if there are columns with missing 'dtype' information.\n    \"\"\"\n    dtypes = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\n    if not all(dtypes.values()):\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in dtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        dtypes.update({cn: data[cn].dtype for cn, cv in dtypes.items() if not cv})\n    return dtypes\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code> DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code> DataFrame.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data` DataFrame.\n\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n\n    Returns:\n        A metadata dictionary containing information about the columns in the `data` DataFrame.\n    \"\"\"\n    if in_path.exists():\n        with open(in_path) as stream:\n            metadata = yaml.safe_load(stream)\n        metadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\n    else:\n        metadata = create_empty_metadata(data)\n    check_metadata_columns(metadata, data)\n    return metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, dtypes, metatransformer, sdv_workflow=True, collapse_yaml=True)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>dtypes</code> <code>dict</code> <p>A dictionary mapping column names of the input data to their assigned data types.</p> required <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>The synthesizer or hypertransformer that was used to transform the data.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.</p> <code>True</code> <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\n    out_path: pathlib.Path,\n    dtypes: dict[str, Any],\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    sdv_workflow: bool = True,\n    collapse_yaml: bool = True,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        dtypes: A dictionary mapping column names of the input data to their assigned data types.\n        metatransformer: The synthesizer or hypertransformer that was used to transform the data.\n        sdv_workflow: A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n\n    Returns:\n        None\n    \"\"\"\n    metadata = assemble_metadata(dtypes, metatransformer, sdv_workflow)\n\n    if collapse_yaml:\n        metadata = collapse(metadata)\n\n    with open(out_path, \"w\") as yaml_file:\n        yaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.run","title":"<code>run</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; None:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.\n\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n\n    Returns:\n        None\n    \"\"\"\n\n    print(\"Running dataloader...\")\n\n    if args.seed:\n        np.random.seed(args.seed)\n\n    input_path, output_path, metadata_input_path, metadata_output_path = setup_io(\n        args.input_file, args.output_file, args.metadata_file, args.dir, args.run_name\n    )\n\n    # Load the dataset and accompanying metadata\n    input = pd.read_csv(input_path, index_col=args.index_col)\n    metadata = load_metadata(metadata_input_path, input)\n\n    # Setup the input data dtypes and apply them\n    dtypes = instantiate_dtypes(metadata, input)\n    typed_input = input.astype(dtypes)\n\n    # Setup the metatransformer\n    metatransformer = instantiate_metatransformer(\n        metadata,\n        typed_input,\n        args.sdv_workflow,\n        args.allow_null_transformers,\n        SDV_SYNTHESIZER_CHOICES[args.synthesizer],\n    )\n\n    # Output the metadata corresponding to `transformed_input`, for reproducibility\n    output_metadata(metadata_output_path, dtypes, metatransformer, args.sdv_workflow, args.collapse_yaml)\n\n    print(\"Transforming input...\")\n    transformed_input = apply_transformer(metatransformer, typed_input, args.sdv_workflow)\n\n    # Write the transformed input to the appropriate file\n    print(\"Writing output\")\n    transformed_input.to_csv(output_path, index=False)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.transformers","title":"<code>transformers</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.transformers.apply_transformer","title":"<code>apply_transformer(metatransformer, typed_input, sdv_workflow)</code>","text":"<p>Applies a metatransformer to the typed input data.</p> <p>Parameters:</p> Name Type Description Default <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A metatransformer object that can be either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV.</p> required <code>typed_input</code> <code>DataFrame</code> <p>The typed input data.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean flag indicating whether to use the <code>preprocess()</code> method of the <code>metatransformer</code> if it's an <code>SDV</code> synthesizer, or the <code>fit_transform()</code> method if it's an <code>RDT</code> transformer.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def apply_transformer(\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    typed_input: pd.DataFrame,\n    sdv_workflow: bool,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Applies a metatransformer to the typed input data.\n\n    Args:\n        metatransformer: A metatransformer object that can be either a `HyperTransformer` from RDT or a `BaseSingleTableSynthesizer` from SDV.\n        typed_input: The typed input data.\n        sdv_workflow: A boolean flag indicating whether to use the `preprocess()` method of the `metatransformer` if it's an `SDV` synthesizer, or the `fit_transform()` method if it's an `RDT` transformer.\n\n    Returns:\n        The transformed data.\n    \"\"\"\n    if sdv_workflow:\n        transformed_input = metatransformer.preprocess(typed_input)\n    else:\n        transformed_input = metatransformer.fit_transform(typed_input)\n    return transformed_input\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.transformers.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object extracted from the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>Optional[sdv.single_table.base.BaseSingleTableSynthesizer]</code> <p>A callable object (transformer) if the dictionary contains transformer data, else None.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def get_transformer(d: dict) -&gt; Optional[BaseSingleTableSynthesizer]:\n\"\"\"\n    Return a callable transformer object extracted from the given dictionary.\n\n    Args:\n        d: A dictionary containing the transformer data.\n\n    Returns:\n        A callable object (transformer) if the dictionary contains transformer data, else None.\n    \"\"\"\n    transformer_data = d.get(\"transformer\", None)\n    if isinstance(transformer_data, dict):\n        # Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\n        transformer_data = transformer_data.copy()\n        transformer_name = transformer_data.pop(\"name\", None)\n        transformer = eval(transformer_name)(**transformer_data) if transformer_name else None\n    else:\n        transformer = eval(transformer_data)() if transformer_data else None\n    return transformer\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.transformers.instantiate_hypertransformer","title":"<code>instantiate_hypertransformer(sdtypes, transformers, data, allow_null_transformers)</code>","text":"<p>Instantiates a HyperTransformer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <code>dict</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <code>dict</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>HyperTransformer</code> <p>A HyperTransformer object instantiated from the given metadata and data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_hypertransformer(\n    sdtypes: dict[str, dict],\n    transformers: dict[str, Optional[BaseTransformer]],\n    data: pd.DataFrame,\n    allow_null_transformers: bool,\n) -&gt; HyperTransformer:\n\"\"\"\n    Instantiates a HyperTransformer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A HyperTransformer object instantiated from the given metadata and data.\n    \"\"\"\n    ht = HyperTransformer()\n    if all(sdtypes.values()) and (all(transformers.values()) or allow_null_transformers):\n        ht.set_config(\n            config={\n                \"sdtypes\": {k: v[\"sdtype\"] for k, v in sdtypes.items()},\n                \"transformers\": transformers,\n            }\n        )\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in sdtypes.items() if not v])) if not all(sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in transformers.items() if not v])) if not all(transformers.values()) and not allow_null_transformers else ''} automatically...\",\n            UserWarning,\n        )\n        ht.detect_initial_config(data)\n        ht.update_sdtypes({k: v[\"sdtype\"] for k, v in sdtypes.items() if v})\n        ht.update_transformers(\n            transformers if allow_null_transformers else {k: v for k, v in transformers.items() if v}\n        )\n    return ht\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.transformers.instantiate_metatransformer","title":"<code>instantiate_metatransformer(metadata, data, sdv_workflow, allow_null_transformers, Synthesizer)</code>","text":"<p>Instantiates a metatransformer based on the given metadata and input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing the metadata for the input data. Each key corresponds to a column name in the input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the name of the transformer to use for the column and its configuration (to be instantiated below).</p> required <code>data</code> <code>DataFrame</code> <p>The input data as a pandas DataFrame.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from RDT will be used instead.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A boolean flag indicating whether to allow transformers to be None. If True, a None value for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated for that column.</p> required <p>Returns:</p> Type Description <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A metatransformer object that can be either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_metatransformer(\n    metadata: dict[str, dict],\n    data: pd.DataFrame,\n    sdv_workflow: bool,\n    allow_null_transformers: bool,\n    Synthesizer: type[BaseSingleTableSynthesizer],\n) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n    Instantiates a metatransformer based on the given metadata and input data.\n\n    Args:\n        metadata: A dictionary containing the metadata for the input data. Each key corresponds to a column name in the\n            input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata\n            should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the\n            name of the transformer to use for the column and its configuration (to be instantiated below).\n        data: The input data as a pandas DataFrame.\n        sdv_workflow: A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the\n            TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from\n            RDT will be used instead.\n        allow_null_transformers: A boolean flag indicating whether to allow transformers to be None. If True, a None value\n            for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated\n            for that column.\n\n    Returns:\n        A metatransformer object that can be either a `HyperTransformer` from RDT or a `BaseSingleTableSynthesizer` from SDV.\n    \"\"\"\n    sdtypes = {cn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()}\n    transformers = {cn: get_transformer(cd) for cn, cd in metadata.items()}\n    if sdv_workflow:\n        metatransformer = instantiate_synthesizer(sdtypes, transformers, data, allow_null_transformers, Synthesizer)\n    else:\n        metatransformer = instantiate_hypertransformer(sdtypes, transformers, data, allow_null_transformers)\n    return metatransformer\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.transformers.instantiate_synthesizer","title":"<code>instantiate_synthesizer(sdtypes, transformers, data, allow_null_transformers, Synthesizer)</code>","text":"<p>Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <code>dict</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <code>dict</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>BaseSingleTableSynthesizer</code> <p>A BaseSingleTableSynthesizer object instantiated from the given metadata and data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_synthesizer(\n    sdtypes: dict[str, dict],\n    transformers: dict[str, Optional[BaseTransformer]],\n    data: pd.DataFrame,\n    allow_null_transformers: bool,\n    Synthesizer: type[BaseSingleTableSynthesizer],\n) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n    Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A BaseSingleTableSynthesizer object instantiated from the given metadata and data.\n    \"\"\"\n    if all(sdtypes.values()):\n        metadata = SingleTableMetadata.load_from_dict({\"columns\": sdtypes})\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in sdtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        metadata = SingleTableMetadata()\n        metadata.detect_from_dataframe(data)\n        for column_name, values in sdtypes.items():\n            if values:\n                metadata.update_column(column_name=column_name, **values)\n    if not all(transformers.values()) and not allow_null_transformers:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in transformers.items() if not v]} automatically...\",\n            UserWarning,\n        )\n    synthesizer = Synthesizer(metadata)\n    synthesizer.auto_assign_transformers(data)\n    synthesizer.update_transformers(\n        transformers if allow_null_transformers else {k: v for k, v in transformers.items() if v}\n    )\n    return synthesizer\n</code></pre>"},{"location":"reference/modules/dataloader/io/","title":"io","text":""},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_ending","title":"<code>check_ending(fn, ending='.csv')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".csv\".</p> <code>'.csv'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_ending(fn: str, ending: str = \".csv\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, appends `ending`.\n\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".csv\".\n\n    Returns:\n        str: The filename with the correct ending.\n    \"\"\"\n    return fn if fn.endswith(ending) else fn + ending\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.setup_io","title":"<code>setup_io(fn_in, fn_out, fn_metadata, dir_data, run_name, dir_exp='experiments')</code>","text":"<p>Formats the input and output filenames and directories for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_in</code> <code>str</code> <p>The input data filename.</p> required <code>fn_out</code> <code>str</code> <p>The output data filename / suffix to append to <code>fn_in</code>.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_in</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory containing the data.</p> required <code>run_name</code> <code>str</code> <p>The name of the experiment run.</p> required <code>dir_exp</code> <code>str</code> <p>The parent directory for the experiment folders. Default is \"experiments\".</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>tuple[Path, Path, Path, Path]</code> <p>A tuple containing the formatted input, output, and metadata (in and out) paths.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_in</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_out</code> includes directory separators, as only the name of the output file will be used.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def setup_io(\n    fn_in: str,\n    fn_out: str,\n    fn_metadata: str,\n    dir_data: str,\n    run_name: str,\n    dir_exp: str = \"experiments\",\n) -&gt; tuple[Path, Path, Path, Path]:\n\"\"\"\n    Formats the input and output filenames and directories for an experiment.\n\n    Args:\n        fn_in: The input data filename.\n        fn_out: The output data filename / suffix to append to `fn_in`.\n        fn_metadata: The metadata filename / suffix to append to `fn_in`.\n        dir_data: The directory containing the data.\n        run_name: The name of the experiment run.\n        dir_exp: The parent directory for the experiment folders. Default is \"experiments\".\n\n    Returns:\n        tuple[Path, Path, Path, Path]: A tuple containing the formatted input, output, and metadata (in and out) paths.\n\n    Warnings:\n        Raises a UserWarning when the path to `fn_in` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_out` includes directory separators, as only the name of the output file will be used.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\n    # ensure .csv ending consistency\n    fn_in, fn_out, fn_metadata = check_ending(fn_in), check_ending(fn_out), check_ending(fn_metadata, ending=\".yaml\")\n\n    dir_data = Path(dir_data)\n\n    # check if `fn_out` and `fn_metadata` are given as suffixes (start with an underscore) to append to `fn_in`\n    # if not assume it is a name in its own right\n    if fn_out[0] == \"_\":\n        fn_out = check_ending(fn_in[:-4] + fn_out)\n    if fn_metadata[0] == \"_\":\n        fn_metadata = check_ending(fn_in[:-4] + fn_metadata, ending=\".yaml\")\n\n    if \"/\" in fn_in:\n        fn_in = Path(fn_in)\n        warnings.warn(\n            f\"Using the path supplied to `--input-file` appended to `--dir`, i.e. attempting to read data from {dir_data / fn_in},\\nto avoid this warning, specify the path using `--dir` and only the name using `--input-file`\\ne.g. `... --dir {(dir_data / fn_in).parent} --input-file {fn_in.name} ...`\",\n            UserWarning,\n        )\n\n    # generate timestamped experiment folder\n    dir_exp = Path(dir_exp) / run_name\n\n    if \"/\" in fn_out:\n        fn_out = Path(fn_out).name\n        warnings.warn(\n            f\"Paths are not supported via `--output-file`, using the name part instead, i.e. attempting to write data to {dir_exp / fn_out}\",\n            UserWarning,\n        )\n\n    if \"/\" in fn_metadata:\n        fn_metadata = Path(fn_metadata)\n        warnings.warn(\n            f\"Using the path supplied to `--metadata` appended to `--dir`, i.e. attempting to read data from {dir_data / fn_metadata},\\nto avoid this warning, specify the path using `--dir` and only the name using `--metadata`\\ne.g. `... --dir {(dir_data / fn_metadata).parent} --metadata {fn_metadata.name} ...`\",\n            UserWarning,\n        )\n\n    # Make the experiment directory\n    dir_exp.mkdir(parents=True, exist_ok=True)\n\n    return dir_data / fn_in, dir_exp / fn_out, dir_data / fn_metadata, dir_exp / fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/","title":"metadata","text":""},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.assemble_metadata","title":"<code>assemble_metadata(dtypes, metatransformer, sdv_workflow)</code>","text":"<p>Constructs a metadata dictionary from a list of data types and a metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>dtypes</code> <code>dict</code> <p>A dictionary mapping column names of the input data to their assigned data types.</p> required <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A meta-transformer used to create synthetic data. - If <code>sdv_workflow</code> is True, <code>metatransformer</code> should be an SDV single-table synthesizer. - If <code>sdv_workflow</code> is False, <code>metatransformer</code> should be an RDT HyperTransformer object   wrapping a dictionary containing transformers and sdtypes for each column.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys:     - dtype: The name of the data type for the column.     - sdtype: The data type for the column (only present if <code>sdv_workflow</code> is False).     - transformer: A dictionary containing information about the transformer       used for the column (if any). The dictionary has the following keys:       - name: The name of the transformer.       - Any other properties of the transformer that are not private or set by a random seed.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def assemble_metadata(\n    dtypes: dict[str, type],\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    sdv_workflow: bool,\n) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Constructs a metadata dictionary from a list of data types and a metatransformer.\n\n    Args:\n        dtypes: A dictionary mapping column names of the input data to their assigned data types.\n        metatransformer: A meta-transformer used to create synthetic data.\n            - If `sdv_workflow` is True, `metatransformer` should be an SDV single-table synthesizer.\n            - If `sdv_workflow` is False, `metatransformer` should be an RDT HyperTransformer object\n              wrapping a dictionary containing transformers and sdtypes for each column.\n        sdv_workflow: A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.\n\n    Returns:\n        dict[str, dict[str, Any]]: A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The name of the data type for the column.\n            - sdtype: The data type for the column (only present if `sdv_workflow` is False).\n            - transformer: A dictionary containing information about the transformer\n              used for the column (if any). The dictionary has the following keys:\n              - name: The name of the transformer.\n              - Any other properties of the transformer that are not private or set by a random seed.\n    \"\"\"\n    if sdv_workflow:\n        sdmetadata = metatransformer.metadata\n        transformers = metatransformer.get_transformers()\n        metadata = {\n            cn: {\n                **cd,\n                **{\n                    \"transformer\": {\n                        \"name\": type(transformers[cn]).__name__,\n                        **filter_dict(\n                            transformers[cn].__dict__,\n                            {\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n                        ),\n                    }\n                    if transformers[cn]\n                    else None,\n                    \"dtype\": dtypes[cn].name if not isinstance(dtypes[cn], str) else dtypes[cn],\n                },\n            }\n            for cn, cd in sdmetadata.columns.items()\n        }\n    else:\n        config = metatransformer.get_config()\n        metadata = {\n            cn: {\n                \"sdtype\": cd,\n                \"transformer\": {\n                    \"name\": type(config[\"transformers\"][cn]).__name__,\n                    **filter_dict(\n                        config[\"transformers\"][cn].__dict__,\n                        {\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n                    ),\n                }\n                if config[\"transformers\"][cn]\n                else None,\n                \"dtype\": dtypes[cn].name if not isinstance(dtypes[cn], str) else dtypes[cn],\n            }\n            for cn, cd in config[\"sdtypes\"].items()\n        }\n    return metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the metadata correspond to valid columns in the DataFrame. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the DataFrame.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns in metadata are not present in the DataFrame.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the metadata correspond to valid columns in the DataFrame.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n\n    Args:\n        metadata: A dictionary containing metadata for the columns in the DataFrame.\n        data: The DataFrame to check against the metadata.\n\n    Raises:\n        AssertionError: If any columns in metadata are not present in the DataFrame.\n    \"\"\"\n    assert all([k in data.columns for k in metadata.keys()])\n    metadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # columns that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.\n\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # columns that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\n    c_index = 1\n    column_types = {}\n    t_index = 1\n    transformers = {}\n    for cn, cd in metadata.items():\n\n        if cd not in column_types.values():\n            column_types[c_index] = cd.copy()\n            metadata[cn] = column_types[c_index]\n            c_index += 1\n        else:\n            cix = get_key_by_value(column_types, cd)\n            metadata[cn] = column_types[cix]\n\n        if cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\n            transformers[t_index] = cd[\"transformer\"].copy()\n            metadata[cn][\"transformer\"] = transformers[t_index]\n            t_index += 1\n        elif cd[\"transformer\"]:\n            tix = get_key_by_value(transformers, cd[\"transformer\"])\n            metadata[cn][\"transformer\"] = transformers[tix]\n\n    return {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame for which an empty metadata dictionary is created.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n\n    Args:\n        data: The DataFrame for which an empty metadata dictionary is created.\n\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\n    return {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.instantiate_dtypes","title":"<code>instantiate_dtypes(metadata, data)</code>","text":"<p>Instantiate the data types for each column based on the given metadata.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata information for each column, including the data type.</p> required <code>data</code> <code>DataFrame</code> <p>A pandas DataFrame containing the data to be instantiated.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the instantiated data types for each column.</p> <p>Exceptions:</p> Type Description <code>UserWarning</code> <p>If incomplete metadata is detected, i.e., if there are columns with missing 'dtype' information.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def instantiate_dtypes(metadata: dict[str, dict], data: pd.DataFrame) -&gt; dict[str, np.dtype]:\n\"\"\"\n    Instantiate the data types for each column based on the given metadata.\n\n    Args:\n        metadata: A dictionary containing metadata information for each column, including the data type.\n        data: A pandas DataFrame containing the data to be instantiated.\n\n    Returns:\n        A dictionary containing the instantiated data types for each column.\n\n    Raises:\n        UserWarning: If incomplete metadata is detected, i.e., if there are columns with missing 'dtype' information.\n    \"\"\"\n    dtypes = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\n    if not all(dtypes.values()):\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in dtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        dtypes.update({cn: data[cn].dtype for cn, cv in dtypes.items() if not cv})\n    return dtypes\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code> DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code> DataFrame.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data` DataFrame.\n\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n\n    Returns:\n        A metadata dictionary containing information about the columns in the `data` DataFrame.\n    \"\"\"\n    if in_path.exists():\n        with open(in_path) as stream:\n            metadata = yaml.safe_load(stream)\n        metadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\n    else:\n        metadata = create_empty_metadata(data)\n    check_metadata_columns(metadata, data)\n    return metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, dtypes, metatransformer, sdv_workflow=True, collapse_yaml=True)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>dtypes</code> <code>dict</code> <p>A dictionary mapping column names of the input data to their assigned data types.</p> required <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>The synthesizer or hypertransformer that was used to transform the data.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.</p> <code>True</code> <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\n    out_path: pathlib.Path,\n    dtypes: dict[str, Any],\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    sdv_workflow: bool = True,\n    collapse_yaml: bool = True,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        dtypes: A dictionary mapping column names of the input data to their assigned data types.\n        metatransformer: The synthesizer or hypertransformer that was used to transform the data.\n        sdv_workflow: A boolean indicating whether the data was transformed using the SDV / synthesizer workflow.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n\n    Returns:\n        None\n    \"\"\"\n    metadata = assemble_metadata(dtypes, metatransformer, sdv_workflow)\n\n    if collapse_yaml:\n        metadata = collapse(metadata)\n\n    with open(out_path, \"w\") as yaml_file:\n        yaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/dataloader/run/","title":"run","text":""},{"location":"reference/modules/dataloader/run/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; None:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.\n\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n\n    Returns:\n        None\n    \"\"\"\n\n    print(\"Running dataloader...\")\n\n    if args.seed:\n        np.random.seed(args.seed)\n\n    input_path, output_path, metadata_input_path, metadata_output_path = setup_io(\n        args.input_file, args.output_file, args.metadata_file, args.dir, args.run_name\n    )\n\n    # Load the dataset and accompanying metadata\n    input = pd.read_csv(input_path, index_col=args.index_col)\n    metadata = load_metadata(metadata_input_path, input)\n\n    # Setup the input data dtypes and apply them\n    dtypes = instantiate_dtypes(metadata, input)\n    typed_input = input.astype(dtypes)\n\n    # Setup the metatransformer\n    metatransformer = instantiate_metatransformer(\n        metadata,\n        typed_input,\n        args.sdv_workflow,\n        args.allow_null_transformers,\n        SDV_SYNTHESIZER_CHOICES[args.synthesizer],\n    )\n\n    # Output the metadata corresponding to `transformed_input`, for reproducibility\n    output_metadata(metadata_output_path, dtypes, metatransformer, args.sdv_workflow, args.collapse_yaml)\n\n    print(\"Transforming input...\")\n    transformed_input = apply_transformer(metatransformer, typed_input, args.sdv_workflow)\n\n    # Write the transformed input to the appropriate file\n    print(\"Writing output\")\n    transformed_input.to_csv(output_path, index=False)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/","title":"transformers","text":""},{"location":"reference/modules/dataloader/transformers/#nhssynth.modules.dataloader.transformers.apply_transformer","title":"<code>apply_transformer(metatransformer, typed_input, sdv_workflow)</code>","text":"<p>Applies a metatransformer to the typed input data.</p> <p>Parameters:</p> Name Type Description Default <code>metatransformer</code> <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A metatransformer object that can be either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV.</p> required <code>typed_input</code> <code>DataFrame</code> <p>The typed input data.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean flag indicating whether to use the <code>preprocess()</code> method of the <code>metatransformer</code> if it's an <code>SDV</code> synthesizer, or the <code>fit_transform()</code> method if it's an <code>RDT</code> transformer.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def apply_transformer(\n    metatransformer: BaseSingleTableSynthesizer | HyperTransformer,\n    typed_input: pd.DataFrame,\n    sdv_workflow: bool,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Applies a metatransformer to the typed input data.\n\n    Args:\n        metatransformer: A metatransformer object that can be either a `HyperTransformer` from RDT or a `BaseSingleTableSynthesizer` from SDV.\n        typed_input: The typed input data.\n        sdv_workflow: A boolean flag indicating whether to use the `preprocess()` method of the `metatransformer` if it's an `SDV` synthesizer, or the `fit_transform()` method if it's an `RDT` transformer.\n\n    Returns:\n        The transformed data.\n    \"\"\"\n    if sdv_workflow:\n        transformed_input = metatransformer.preprocess(typed_input)\n    else:\n        transformed_input = metatransformer.fit_transform(typed_input)\n    return transformed_input\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/#nhssynth.modules.dataloader.transformers.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object extracted from the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>Optional[sdv.single_table.base.BaseSingleTableSynthesizer]</code> <p>A callable object (transformer) if the dictionary contains transformer data, else None.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def get_transformer(d: dict) -&gt; Optional[BaseSingleTableSynthesizer]:\n\"\"\"\n    Return a callable transformer object extracted from the given dictionary.\n\n    Args:\n        d: A dictionary containing the transformer data.\n\n    Returns:\n        A callable object (transformer) if the dictionary contains transformer data, else None.\n    \"\"\"\n    transformer_data = d.get(\"transformer\", None)\n    if isinstance(transformer_data, dict):\n        # Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\n        transformer_data = transformer_data.copy()\n        transformer_name = transformer_data.pop(\"name\", None)\n        transformer = eval(transformer_name)(**transformer_data) if transformer_name else None\n    else:\n        transformer = eval(transformer_data)() if transformer_data else None\n    return transformer\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/#nhssynth.modules.dataloader.transformers.instantiate_hypertransformer","title":"<code>instantiate_hypertransformer(sdtypes, transformers, data, allow_null_transformers)</code>","text":"<p>Instantiates a HyperTransformer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <code>dict</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <code>dict</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>HyperTransformer</code> <p>A HyperTransformer object instantiated from the given metadata and data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_hypertransformer(\n    sdtypes: dict[str, dict],\n    transformers: dict[str, Optional[BaseTransformer]],\n    data: pd.DataFrame,\n    allow_null_transformers: bool,\n) -&gt; HyperTransformer:\n\"\"\"\n    Instantiates a HyperTransformer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A HyperTransformer object instantiated from the given metadata and data.\n    \"\"\"\n    ht = HyperTransformer()\n    if all(sdtypes.values()) and (all(transformers.values()) or allow_null_transformers):\n        ht.set_config(\n            config={\n                \"sdtypes\": {k: v[\"sdtype\"] for k, v in sdtypes.items()},\n                \"transformers\": transformers,\n            }\n        )\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in sdtypes.items() if not v])) if not all(sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in transformers.items() if not v])) if not all(transformers.values()) and not allow_null_transformers else ''} automatically...\",\n            UserWarning,\n        )\n        ht.detect_initial_config(data)\n        ht.update_sdtypes({k: v[\"sdtype\"] for k, v in sdtypes.items() if v})\n        ht.update_transformers(\n            transformers if allow_null_transformers else {k: v for k, v in transformers.items() if v}\n        )\n    return ht\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/#nhssynth.modules.dataloader.transformers.instantiate_metatransformer","title":"<code>instantiate_metatransformer(metadata, data, sdv_workflow, allow_null_transformers, Synthesizer)</code>","text":"<p>Instantiates a metatransformer based on the given metadata and input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing the metadata for the input data. Each key corresponds to a column name in the input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the name of the transformer to use for the column and its configuration (to be instantiated below).</p> required <code>data</code> <code>DataFrame</code> <p>The input data as a pandas DataFrame.</p> required <code>sdv_workflow</code> <code>bool</code> <p>A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from RDT will be used instead.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A boolean flag indicating whether to allow transformers to be None. If True, a None value for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated for that column.</p> required <p>Returns:</p> Type Description <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A metatransformer object that can be either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_metatransformer(\n    metadata: dict[str, dict],\n    data: pd.DataFrame,\n    sdv_workflow: bool,\n    allow_null_transformers: bool,\n    Synthesizer: type[BaseSingleTableSynthesizer],\n) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n    Instantiates a metatransformer based on the given metadata and input data.\n\n    Args:\n        metadata: A dictionary containing the metadata for the input data. Each key corresponds to a column name in the\n            input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata\n            should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the\n            name of the transformer to use for the column and its configuration (to be instantiated below).\n        data: The input data as a pandas DataFrame.\n        sdv_workflow: A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the\n            TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from\n            RDT will be used instead.\n        allow_null_transformers: A boolean flag indicating whether to allow transformers to be None. If True, a None value\n            for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated\n            for that column.\n\n    Returns:\n        A metatransformer object that can be either a `HyperTransformer` from RDT or a `BaseSingleTableSynthesizer` from SDV.\n    \"\"\"\n    sdtypes = {cn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()}\n    transformers = {cn: get_transformer(cd) for cn, cd in metadata.items()}\n    if sdv_workflow:\n        metatransformer = instantiate_synthesizer(sdtypes, transformers, data, allow_null_transformers, Synthesizer)\n    else:\n        metatransformer = instantiate_hypertransformer(sdtypes, transformers, data, allow_null_transformers)\n    return metatransformer\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/#nhssynth.modules.dataloader.transformers.instantiate_synthesizer","title":"<code>instantiate_synthesizer(sdtypes, transformers, data, allow_null_transformers, Synthesizer)</code>","text":"<p>Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <code>dict</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <code>dict</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>BaseSingleTableSynthesizer</code> <p>A BaseSingleTableSynthesizer object instantiated from the given metadata and data.</p> Source code in <code>modules/dataloader/transformers.py</code> <pre><code>def instantiate_synthesizer(\n    sdtypes: dict[str, dict],\n    transformers: dict[str, Optional[BaseTransformer]],\n    data: pd.DataFrame,\n    allow_null_transformers: bool,\n    Synthesizer: type[BaseSingleTableSynthesizer],\n) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n    Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A BaseSingleTableSynthesizer object instantiated from the given metadata and data.\n    \"\"\"\n    if all(sdtypes.values()):\n        metadata = SingleTableMetadata.load_from_dict({\"columns\": sdtypes})\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in sdtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        metadata = SingleTableMetadata()\n        metadata.detect_from_dataframe(data)\n        for column_name, values in sdtypes.items():\n            if values:\n                metadata.update_column(column_name=column_name, **values)\n    if not all(transformers.values()) and not allow_null_transformers:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in transformers.items() if not v]} automatically...\",\n            UserWarning,\n        )\n    synthesizer = Synthesizer(metadata)\n    synthesizer.auto_assign_transformers(data)\n    synthesizer.update_transformers(\n        transformers if allow_null_transformers else {k: v for k, v in transformers.items() if v}\n    )\n    return synthesizer\n</code></pre>"},{"location":"reference/modules/evaluation/","title":"evaluation","text":""},{"location":"reference/modules/evaluation/metrics/","title":"metrics","text":""},{"location":"reference/modules/evaluation/run/","title":"run","text":""},{"location":"reference/modules/model/","title":"model","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE","title":"<code>DPVAE</code>","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\n\n    def __init__(\n        self,\n        latent_dim,\n        num_continuous,\n        num_categories=[0],\n        hidden_dim=32,\n        activation=nn.Tanh,\n        device=\"gpu\",\n    ):\n        super().__init__()\n\n        output_dim = num_continuous + sum(num_categories)\n        self.num_continuous = num_continuous\n        self.num_categories = num_categories\n\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, z):\n        return self.net(z)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\n    return self.net(z)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\n\n    def __init__(\n        self, input_dim, latent_dim, hidden_dim=32, activation=nn.Tanh, device=\"gpu\",\n    ):\n        super().__init__()\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        output_dim = 2 * latent_dim\n        self.latent_dim = latent_dim\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, x):\n        outs = self.net(x)\n        mu_z = outs[:, : self.latent_dim]\n        logsigma_z = outs[:, self.latent_dim :]\n        return mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\n    outs = self.net(x)\n    mu_z = outs[:, : self.latent_dim]\n    logsigma_z = outs[:, self.latent_dim :]\n    return mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\n    def __init__(self, num_continuous):\n        super().__init__()\n        self.output_logsigma_fn = nn.Linear(num_continuous, num_continuous, bias=True)\n        torch.nn.init.zeros_(self.output_logsigma_fn.weight)\n        torch.nn.init.zeros_(self.output_logsigma_fn.bias)\n        self.output_logsigma_fn.weight.requires_grad = False\n\n    def forward(self, X):\n        return self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\n    return self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\n\n    def __init__(self, encoder, decoder, lr=1e-3):\n        super().__init__()\n        self.encoder = encoder.to(encoder.device)\n        self.decoder = decoder.to(decoder.device)\n        self.device = encoder.device\n        self.num_categories = self.decoder.num_categories\n        self.num_continuous = self.decoder.num_continuous\n        self.noiser = Noiser(self.num_continuous).to(decoder.device)\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n        self.lr = lr\n\n    def reconstruct(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        x_recon = self.decoder(mu_z)\n        return x_recon\n\n    def generate(self, N):\n        z_samples = torch.randn_like(\n            torch.ones((N, self.encoder.latent_dim)), device=self.device\n        )\n        x_gen = self.decoder(z_samples)\n        x_gen_ = torch.ones_like(x_gen, device=self.device)\n        i = 0\n\n        for v in range(len(self.num_categories)):\n            x_gen_[\n                :, i : (i + self.num_categories[v])\n            ] = torch.distributions.one_hot_categorical.OneHotCategorical(\n                logits=x_gen[:, i : (i + self.num_categories[v])]\n            ).sample()\n            i = i + self.num_categories[v]\n\n        x_gen_[:, -self.num_continuous :] = x_gen[\n            :, -self.num_continuous :\n        ] + torch.exp(self.noiser(x_gen[:, -self.num_continuous :])) * torch.randn_like(\n            x_gen[:, -self.num_continuous :]\n        )\n        return x_gen_\n\n    def loss(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        p = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\n        q = Normal(mu_z, torch.exp(logsigma_z))\n\n        divergence_loss = torch.sum(torch.distributions.kl_divergence(q, p))\n\n        s = torch.randn_like(mu_z)\n        z_samples = mu_z + s * torch.exp(logsigma_z)\n\n        x_recon = self.decoder(z_samples)\n\n        categoric_loglik = 0\n        if sum(self.num_categories) != 0:\n            i = 0\n\n            for v in range(len(self.num_categories)):\n\n                categoric_loglik += -torch.nn.functional.cross_entropy(\n                    x_recon[:, i : (i + self.num_categories[v])],\n                    torch.max(X[:, i : (i + self.num_categories[v])], 1)[1],\n                ).sum()\n                i = i + self.decoder.num_categories[v]\n\n        gauss_loglik = 0\n        if self.decoder.num_continuous != 0:\n            gauss_loglik = (\n                Normal(\n                    loc=x_recon[:, -self.num_continuous :],\n                    scale=torch.exp(self.noiser(x_recon[:, -self.num_continuous :])),\n                )\n                .log_prob(X[:, -self.num_continuous :])\n                .sum()\n            )\n\n        reconstruct_loss = -(categoric_loglik + gauss_loglik)\n\n        elbo = divergence_loss + reconstruct_loss\n\n        return (elbo, reconstruct_loss, divergence_loss, categoric_loglik, gauss_loglik)\n\n    def train(\n        self,\n        x_dataloader,\n        n_epochs,\n        logging_freq=1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        # mean_norm = 0\n        # counter = 0\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(n_epochs):\n\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruc,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruc.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # counter += 1\n                # l2_norm = 0\n                # for p in self.parameters():\n                #     if p.requires_grad:\n                #         p_norm = p.grad.detach().data.norm(2)\n                #         l2_norm += p_norm.item() ** 2\n                # l2_norm = l2_norm ** 0.5  # / Y_subset.shape[0]\n                # mean_norm = (mean_norm * (counter - 1) + l2_norm) / counter\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n            # self.mean_norm = mean_norm\n\n            if stop_counter == patience:\n\n                n_epochs = epoch + 1\n\n                break\n\n        return (\n            n_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def diff_priv_train(\n        self,\n        x_dataloader,\n        n_epochs,\n        C=1e16,\n        noise_scale=None,\n        target_eps=1,\n        target_delta=1e-5,\n        logging_freq=1,\n        sample_rate=0.1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        if noise_scale is not None:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                noise_multiplier=noise_scale,\n                max_grad_norm=C,\n            )\n        else:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                target_epsilon=target_eps,\n                target_delta=target_delta,\n                epochs=n_epochs,\n                max_grad_norm=C,\n            )\n        self.privacy_engine.attach(self.optimizer)\n\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(n_epochs):\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n            # print(self.get_privacy_spent(target_delta))\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruct,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruct.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # print(self.get_privacy_spent(target_delta))\n                # print(loss.item())\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n\n            if stop_counter == patience:\n\n                n_epochs = epoch + 1\n                break\n\n        return (\n            n_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def get_privacy_spent(self, delta):\n        if hasattr(self, \"privacy_engine\"):\n            return self.privacy_engine.get_privacy_spent(delta)\n        else:\n            print(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n            )\n\n    def save(self, filename):\n        torch.save(self.state_dict(), filename)\n\n    def load(self, filename):\n        self.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, n_epochs, logging_freq=1, patience=5, delta=10, filepath=None)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\n    self,\n    x_dataloader,\n    n_epochs,\n    logging_freq=1,\n    patience=5,\n    delta=10,\n    filepath=None,\n):\n    # mean_norm = 0\n    # counter = 0\n    log_elbo = []\n    log_reconstruct = []\n    log_divergence = []\n    log_cat_loss = []\n    log_num_loss = []\n\n    # EARLY STOPPING #\n    min_elbo = 0.0  # For early stopping workflow\n    patience = patience  # How many epochs patience we give for early stopping\n    stop_counter = 0  # Counter for stops\n    delta = delta  # Difference in elbo value\n\n    for epoch in range(n_epochs):\n\n        train_loss = 0.0\n        divergence_epoch_loss = 0.0\n        reconstruction_epoch_loss = 0.0\n        categorical_epoch_reconstruct = 0.0\n        numerical_epoch_reconstruct = 0.0\n\n        for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n            self.optimizer.zero_grad()\n            (\n                elbo,\n                reconstruct_loss,\n                divergence_loss,\n                categorical_reconstruc,\n                numerical_reconstruct,\n            ) = self.loss(Y_subset.to(self.encoder.device))\n            elbo.backward()\n            self.optimizer.step()\n\n            train_loss += elbo.item()\n            divergence_epoch_loss += divergence_loss.item()\n            reconstruction_epoch_loss += reconstruct_loss.item()\n            categorical_epoch_reconstruct += categorical_reconstruc.item()\n            numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n            # counter += 1\n            # l2_norm = 0\n            # for p in self.parameters():\n            #     if p.requires_grad:\n            #         p_norm = p.grad.detach().data.norm(2)\n            #         l2_norm += p_norm.item() ** 2\n            # l2_norm = l2_norm ** 0.5  # / Y_subset.shape[0]\n            # mean_norm = (mean_norm * (counter - 1) + l2_norm) / counter\n\n        log_elbo.append(train_loss)\n        log_reconstruct.append(reconstruction_epoch_loss)\n        log_divergence.append(divergence_epoch_loss)\n        log_cat_loss.append(categorical_epoch_reconstruct)\n        log_num_loss.append(numerical_epoch_reconstruct)\n\n        if epoch == 0:\n\n            min_elbo = train_loss\n\n        if train_loss &lt; (min_elbo - delta):\n\n            min_elbo = train_loss\n            stop_counter = 0  # Set counter to zero\n            if filepath != None:\n                self.save(filepath)  # Save best model if we want to\n\n        else:  # elbo has not improved\n\n            stop_counter += 1\n\n        if epoch % logging_freq == 0:\n            print(\n                f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n            )\n            # print(f\"\\tMean norm: {mean_norm}\")\n        # self.mean_norm = mean_norm\n\n        if stop_counter == patience:\n\n            n_epochs = epoch + 1\n\n            break\n\n    return (\n        n_epochs,\n        log_elbo,\n        log_reconstruct,\n        log_divergence,\n        log_cat_loss,\n        log_num_loss,\n    )\n</code></pre>"},{"location":"reference/modules/model/DPVAE/","title":"DPVAE","text":""},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\n\n    def __init__(\n        self,\n        latent_dim,\n        num_continuous,\n        num_categories=[0],\n        hidden_dim=32,\n        activation=nn.Tanh,\n        device=\"gpu\",\n    ):\n        super().__init__()\n\n        output_dim = num_continuous + sum(num_categories)\n        self.num_continuous = num_continuous\n        self.num_categories = num_categories\n\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, z):\n        return self.net(z)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\n    return self.net(z)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\n\n    def __init__(\n        self, input_dim, latent_dim, hidden_dim=32, activation=nn.Tanh, device=\"gpu\",\n    ):\n        super().__init__()\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        output_dim = 2 * latent_dim\n        self.latent_dim = latent_dim\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, x):\n        outs = self.net(x)\n        mu_z = outs[:, : self.latent_dim]\n        logsigma_z = outs[:, self.latent_dim :]\n        return mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\n    outs = self.net(x)\n    mu_z = outs[:, : self.latent_dim]\n    logsigma_z = outs[:, self.latent_dim :]\n    return mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\n    def __init__(self, num_continuous):\n        super().__init__()\n        self.output_logsigma_fn = nn.Linear(num_continuous, num_continuous, bias=True)\n        torch.nn.init.zeros_(self.output_logsigma_fn.weight)\n        torch.nn.init.zeros_(self.output_logsigma_fn.bias)\n        self.output_logsigma_fn.weight.requires_grad = False\n\n    def forward(self, X):\n        return self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\n    return self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\n\n    def __init__(self, encoder, decoder, lr=1e-3):\n        super().__init__()\n        self.encoder = encoder.to(encoder.device)\n        self.decoder = decoder.to(decoder.device)\n        self.device = encoder.device\n        self.num_categories = self.decoder.num_categories\n        self.num_continuous = self.decoder.num_continuous\n        self.noiser = Noiser(self.num_continuous).to(decoder.device)\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n        self.lr = lr\n\n    def reconstruct(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        x_recon = self.decoder(mu_z)\n        return x_recon\n\n    def generate(self, N):\n        z_samples = torch.randn_like(\n            torch.ones((N, self.encoder.latent_dim)), device=self.device\n        )\n        x_gen = self.decoder(z_samples)\n        x_gen_ = torch.ones_like(x_gen, device=self.device)\n        i = 0\n\n        for v in range(len(self.num_categories)):\n            x_gen_[\n                :, i : (i + self.num_categories[v])\n            ] = torch.distributions.one_hot_categorical.OneHotCategorical(\n                logits=x_gen[:, i : (i + self.num_categories[v])]\n            ).sample()\n            i = i + self.num_categories[v]\n\n        x_gen_[:, -self.num_continuous :] = x_gen[\n            :, -self.num_continuous :\n        ] + torch.exp(self.noiser(x_gen[:, -self.num_continuous :])) * torch.randn_like(\n            x_gen[:, -self.num_continuous :]\n        )\n        return x_gen_\n\n    def loss(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        p = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\n        q = Normal(mu_z, torch.exp(logsigma_z))\n\n        divergence_loss = torch.sum(torch.distributions.kl_divergence(q, p))\n\n        s = torch.randn_like(mu_z)\n        z_samples = mu_z + s * torch.exp(logsigma_z)\n\n        x_recon = self.decoder(z_samples)\n\n        categoric_loglik = 0\n        if sum(self.num_categories) != 0:\n            i = 0\n\n            for v in range(len(self.num_categories)):\n\n                categoric_loglik += -torch.nn.functional.cross_entropy(\n                    x_recon[:, i : (i + self.num_categories[v])],\n                    torch.max(X[:, i : (i + self.num_categories[v])], 1)[1],\n                ).sum()\n                i = i + self.decoder.num_categories[v]\n\n        gauss_loglik = 0\n        if self.decoder.num_continuous != 0:\n            gauss_loglik = (\n                Normal(\n                    loc=x_recon[:, -self.num_continuous :],\n                    scale=torch.exp(self.noiser(x_recon[:, -self.num_continuous :])),\n                )\n                .log_prob(X[:, -self.num_continuous :])\n                .sum()\n            )\n\n        reconstruct_loss = -(categoric_loglik + gauss_loglik)\n\n        elbo = divergence_loss + reconstruct_loss\n\n        return (elbo, reconstruct_loss, divergence_loss, categoric_loglik, gauss_loglik)\n\n    def train(\n        self,\n        x_dataloader,\n        n_epochs,\n        logging_freq=1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        # mean_norm = 0\n        # counter = 0\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(n_epochs):\n\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruc,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruc.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # counter += 1\n                # l2_norm = 0\n                # for p in self.parameters():\n                #     if p.requires_grad:\n                #         p_norm = p.grad.detach().data.norm(2)\n                #         l2_norm += p_norm.item() ** 2\n                # l2_norm = l2_norm ** 0.5  # / Y_subset.shape[0]\n                # mean_norm = (mean_norm * (counter - 1) + l2_norm) / counter\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n            # self.mean_norm = mean_norm\n\n            if stop_counter == patience:\n\n                n_epochs = epoch + 1\n\n                break\n\n        return (\n            n_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def diff_priv_train(\n        self,\n        x_dataloader,\n        n_epochs,\n        C=1e16,\n        noise_scale=None,\n        target_eps=1,\n        target_delta=1e-5,\n        logging_freq=1,\n        sample_rate=0.1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        if noise_scale is not None:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                noise_multiplier=noise_scale,\n                max_grad_norm=C,\n            )\n        else:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                target_epsilon=target_eps,\n                target_delta=target_delta,\n                epochs=n_epochs,\n                max_grad_norm=C,\n            )\n        self.privacy_engine.attach(self.optimizer)\n\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(n_epochs):\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n            # print(self.get_privacy_spent(target_delta))\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruct,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruct.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # print(self.get_privacy_spent(target_delta))\n                # print(loss.item())\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n\n            if stop_counter == patience:\n\n                n_epochs = epoch + 1\n                break\n\n        return (\n            n_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def get_privacy_spent(self, delta):\n        if hasattr(self, \"privacy_engine\"):\n            return self.privacy_engine.get_privacy_spent(delta)\n        else:\n            print(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n            )\n\n    def save(self, filename):\n        torch.save(self.state_dict(), filename)\n\n    def load(self, filename):\n        self.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, n_epochs, logging_freq=1, patience=5, delta=10, filepath=None)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\n    self,\n    x_dataloader,\n    n_epochs,\n    logging_freq=1,\n    patience=5,\n    delta=10,\n    filepath=None,\n):\n    # mean_norm = 0\n    # counter = 0\n    log_elbo = []\n    log_reconstruct = []\n    log_divergence = []\n    log_cat_loss = []\n    log_num_loss = []\n\n    # EARLY STOPPING #\n    min_elbo = 0.0  # For early stopping workflow\n    patience = patience  # How many epochs patience we give for early stopping\n    stop_counter = 0  # Counter for stops\n    delta = delta  # Difference in elbo value\n\n    for epoch in range(n_epochs):\n\n        train_loss = 0.0\n        divergence_epoch_loss = 0.0\n        reconstruction_epoch_loss = 0.0\n        categorical_epoch_reconstruct = 0.0\n        numerical_epoch_reconstruct = 0.0\n\n        for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n            self.optimizer.zero_grad()\n            (\n                elbo,\n                reconstruct_loss,\n                divergence_loss,\n                categorical_reconstruc,\n                numerical_reconstruct,\n            ) = self.loss(Y_subset.to(self.encoder.device))\n            elbo.backward()\n            self.optimizer.step()\n\n            train_loss += elbo.item()\n            divergence_epoch_loss += divergence_loss.item()\n            reconstruction_epoch_loss += reconstruct_loss.item()\n            categorical_epoch_reconstruct += categorical_reconstruc.item()\n            numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n            # counter += 1\n            # l2_norm = 0\n            # for p in self.parameters():\n            #     if p.requires_grad:\n            #         p_norm = p.grad.detach().data.norm(2)\n            #         l2_norm += p_norm.item() ** 2\n            # l2_norm = l2_norm ** 0.5  # / Y_subset.shape[0]\n            # mean_norm = (mean_norm * (counter - 1) + l2_norm) / counter\n\n        log_elbo.append(train_loss)\n        log_reconstruct.append(reconstruction_epoch_loss)\n        log_divergence.append(divergence_epoch_loss)\n        log_cat_loss.append(categorical_epoch_reconstruct)\n        log_num_loss.append(numerical_epoch_reconstruct)\n\n        if epoch == 0:\n\n            min_elbo = train_loss\n\n        if train_loss &lt; (min_elbo - delta):\n\n            min_elbo = train_loss\n            stop_counter = 0  # Set counter to zero\n            if filepath != None:\n                self.save(filepath)  # Save best model if we want to\n\n        else:  # elbo has not improved\n\n            stop_counter += 1\n\n        if epoch % logging_freq == 0:\n            print(\n                f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n            )\n            # print(f\"\\tMean norm: {mean_norm}\")\n        # self.mean_norm = mean_norm\n\n        if stop_counter == patience:\n\n            n_epochs = epoch + 1\n\n            break\n\n    return (\n        n_epochs,\n        log_elbo,\n        log_reconstruct,\n        log_divergence,\n        log_cat_loss,\n        log_num_loss,\n    )\n</code></pre>"},{"location":"reference/modules/model/run/","title":"run","text":""},{"location":"reference/modules/plotting/","title":"plotting","text":""},{"location":"reference/modules/plotting/plot/","title":"plot","text":""},{"location":"reference/modules/plotting/run/","title":"run","text":""},{"location":"reference/modules/structure/","title":"structure","text":""},{"location":"reference/modules/structure/run/","title":"run","text":""},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#nhssynth.utils.utils","title":"<code>utils</code>","text":""},{"location":"reference/utils/#nhssynth.utils.utils.filter_dict","title":"<code>filter_dict(d, excludes)</code>","text":"<p>Returns a new dictionary containing all key-value pairs from the input dictionary <code>d</code>, except those whose key is included in the set <code>excludes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The input dictionary to filter.</p> required <code>excludes</code> <code>set</code> <p>A set containing the keys to exclude from the filtered dictionary.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A new dictionary containing all key-value pairs from <code>d</code> except those whose key is included in <code>excludes</code>.</p> Source code in <code>utils/utils.py</code> <pre><code>def filter_dict(d: dict, excludes: set) -&gt; dict:\n\"\"\"\n    Returns a new dictionary containing all key-value pairs from the input dictionary `d`, except those whose key is\n    included in the set `excludes`.\n\n    Args:\n        d (dict): The input dictionary to filter.\n        excludes (set): A set containing the keys to exclude from the filtered dictionary.\n\n    Returns:\n        dict: A new dictionary containing all key-value pairs from `d` except those whose key is included in `excludes`.\n    \"\"\"\n    return {k: v for k, v in d.items() if k not in excludes}\n</code></pre>"},{"location":"reference/utils/#nhssynth.utils.utils.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary with possibly nested keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def flatten_dict(d: dict[str, Any | dict]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n\n    Args:\n        d: A dictionary with possibly nested keys.\n\n    Returns:\n        A flattened dictionary.\n\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v).items())\n        else:\n            items.append((k, v))\n    return dict(items)\n</code></pre>"},{"location":"reference/utils/#nhssynth.utils.utils.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <code>Any</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def get_key_by_value(d: dict[Any, Any], value: Any) -&gt; Any | None:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n\n    \"\"\"\n    for key, val in d.items():\n        if val == value:\n            return key\n    return None\n</code></pre>"},{"location":"reference/utils/constants/","title":"constants","text":""},{"location":"reference/utils/utils/","title":"utils","text":""},{"location":"reference/utils/utils/#nhssynth.utils.utils.filter_dict","title":"<code>filter_dict(d, excludes)</code>","text":"<p>Returns a new dictionary containing all key-value pairs from the input dictionary <code>d</code>, except those whose key is included in the set <code>excludes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The input dictionary to filter.</p> required <code>excludes</code> <code>set</code> <p>A set containing the keys to exclude from the filtered dictionary.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A new dictionary containing all key-value pairs from <code>d</code> except those whose key is included in <code>excludes</code>.</p> Source code in <code>utils/utils.py</code> <pre><code>def filter_dict(d: dict, excludes: set) -&gt; dict:\n\"\"\"\n    Returns a new dictionary containing all key-value pairs from the input dictionary `d`, except those whose key is\n    included in the set `excludes`.\n\n    Args:\n        d (dict): The input dictionary to filter.\n        excludes (set): A set containing the keys to exclude from the filtered dictionary.\n\n    Returns:\n        dict: A new dictionary containing all key-value pairs from `d` except those whose key is included in `excludes`.\n    \"\"\"\n    return {k: v for k, v in d.items() if k not in excludes}\n</code></pre>"},{"location":"reference/utils/utils/#nhssynth.utils.utils.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary with possibly nested keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def flatten_dict(d: dict[str, Any | dict]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n\n    Args:\n        d: A dictionary with possibly nested keys.\n\n    Returns:\n        A flattened dictionary.\n\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v).items())\n        else:\n            items.append((k, v))\n    return dict(items)\n</code></pre>"},{"location":"reference/utils/utils/#nhssynth.utils.utils.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <code>Any</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>utils/utils.py</code> <pre><code>def get_key_by_value(d: dict[Any, Any], value: Any) -&gt; Any | None:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n\n    \"\"\"\n    for key, val in d.items():\n        if val == value:\n            return key\n    return None\n</code></pre>"}]}
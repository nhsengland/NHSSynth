{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NHS Synth","text":"<p>Under construction, see the Code Reference or Model Card.</p>"},{"location":"model_card/","title":"Model Card: Variational AutoEncoder with Differential Privacy","text":"<p>Following Model Cards for Model Reporting (Mitchell et al.) and Lessons from Archives (Jo &amp; Gebru), we're providing some information about about the Variational AutoEncoder (VAE) with Differential Privacy within this repository.</p>"},{"location":"model_card/#model-details","title":"Model Details","text":"<p>The implementation of the Variational AutoEncoder (VAE) with Differential Privacy within this repository was created as part of an NHSX Analytics Unit PhD internship project undertaken by Dominic Danks (last commit to the repository: commit 88a4bdf). This model card describes the updated version of the model, released in March 2022.  Further information about the previous version created by Dom and its model implementation can be found in Section 5.4 of the associated report.</p>"},{"location":"model_card/#model-use","title":"Model Use","text":""},{"location":"model_card/#intended-use","title":"Intended Use","text":"<p>This model is intended for use in experimenting with differential privacy and VAEs.</p>"},{"location":"model_card/#training-data","title":"Training Data","text":"<p>Experiments in this repository are run against the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) dataset accessed via the pycox python library. We also performed further analysis on a single table that we extracted from MIMIC-III.</p>"},{"location":"model_card/#performance-and-limitations","title":"Performance and Limitations","text":"<p>A from-scratch VAE implementation was compared against various models available within the SDV framework using a variety of quality and privacy metrics on the SUPPORT dataset. The VAE was found to be competitive with all of these models across the various metrics. Differential Privacy (DP) was introduced via DP-SGD and the performance of the VAE for different levels of privacy was evaluated. It was found that as the level of Differential Privacy introduced by DP-SGD was increased, it became easier to distinguish between synthetic and real data.</p> <p>Proper evaluation of quality and privacy of synthetic data is challenging. In this work, we utilised metrics from the SDV library due to their natural integration with the rest of the codebase. A valuable extension of this work would be to apply a variety of external metrics, including more advanced adversarial attacks to more thoroughly evaluate the privacy of the considered methods, including as the level of DP is varied. It would also be of interest to apply DP-SGD and/or PATE to all of the considered methods and evaluate whether the performance drop as a function of implemented privacy is similar or different across the models.</p> <p>Currently the SynthVAE model only works for data which is 'clean'. I.e data that has no missingness or NaNs within its input. It can handle continuous, categorical and datetime variables. Special types such as nominal data cannot be handled properly however the model may still run. Column names have to be specified in the code for the variable group they belong to.</p> <p>Hyperparameter tuning of the model can result in errors if certain parameter values are selected. Most commonly, changing learning rate in our example results in errors during training. An extensive test to evaluate plausible ranges has not been performed as of yet. If you get errors during tuning then consider your hyperparameter values and adjust accordingly.</p>"},{"location":"modules/","title":"Modules","text":"<p>This folder contains all of the modules contained in this package. They can be used together or independently - through importing them into your existing codebase or using the CLI to select which / all modules to run.</p>"},{"location":"modules/#importing-a-module-from-this-package","title":"Importing a module from this package","text":"<p>After installing the package, you can simply do: <pre><code>from nhssynth.modules import &lt;module&gt;\n</code></pre> and you will be able to use it in your code!</p>"},{"location":"modules/#creating-a-new-module-and-folding-it-into-the-cli","title":"Creating a new module and folding it into the CLI","text":"<p>The following instructions specify how to extend this package with a new module:</p> <ol> <li>Create a folder for your module within the package, i.e. <code>src/nhssynth/modules/mymodule</code></li> <li> <p>Include within it a main executor function that accepts arguments from the CLI, i.e.</p> <pre><code>def myexecutor(args):\n...\n</code></pre> <p>In <code>mymodule/executor.py</code> and export it by adding <code>from .executor import myexecutor</code> to <code>mymodule/__init__.py</code>.</p> </li> <li> <p>In the <code>cli</code> folder, add a corresponding function to <code>arguments.py</code> and populate with arguments you want to expose in the CLI:</p> <pre><code>def add_mymodule_args(parser: argparse.ArgumentParser, group_title: str, overrides=False):\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(...)\ngroup.add_argument(...)\n...\n</code></pre> </li> <li> <p>Next, in <code>module_setup.py</code> make the following adjustments the following code:</p> <pre><code>from nhssynth.modules import ..., mymodule, ...\n</code></pre> <pre><code>MODULE_MAP = {\n...\n\"mymodule\": ModuleConfig(\nfunc=mymodule.myexecutor,\nadd_args_func=add_mymodule_args,\ndescription=\"...\",\nhelp=\"...\",\ncommon_parsers=[...]\n),\n...\n}\n</code></pre> <p>Where <code>common_parsers</code> is a subset of <code>COMMON_PARSERS</code> defined in <code>common_arguments.py</code>. Note that the \"dataset\" and \"core\" parsers are added automatically, so you don't need to specify them. These parsers can be used to add arguments to your module that are common to multiple modules, e.g. the <code>dataloader</code> and <code>evaluation</code> modules both use <code>--typed</code> to specify the path of the typed input dataset.</p> </li> <li> <p>You can (optionally) also edit the following block if you want your module to be included in a full pipeline run:</p> <pre><code>PIPELINE = [..., mymodule, ...]  # NOTE this determines the order of a pipeline run\n</code></pre> </li> <li> <p>Congrats, your module is implemented!</p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli<ul> <li>common_arguments</li> <li>config</li> <li>module_arguments</li> <li>module_setup</li> <li>run</li> </ul> </li> <li>common<ul> <li>common</li> <li>constants</li> <li>dicts</li> <li>io</li> </ul> </li> <li>modules<ul> <li>dataloader<ul> <li>io</li> <li>metadata</li> <li>metatransformer</li> <li>run</li> </ul> </li> <li>evaluation<ul> <li>full_report</li> <li>io</li> <li>metrics</li> <li>run</li> </ul> </li> <li>model<ul> <li>DPVAE</li> <li>io</li> <li>run</li> </ul> </li> <li>plotting<ul> <li>io</li> <li>plots</li> <li>run</li> </ul> </li> <li>structure<ul> <li>run</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/cli/","title":"cli","text":""},{"location":"reference/cli/#nhssynth.cli.common_arguments","title":"<code>common_arguments</code>","text":"<p>Functions to define the CLI's \"common\" arguments, i.e. those that can be applied to either:  - All module argument lists, e.g. --dataset, --seed, etc.  - A subset of module argument lists, e.g. --synthetic, --typed, etc.</p>"},{"location":"reference/cli/#nhssynth.cli.common_arguments.get_core_parser","title":"<code>get_core_parser(overrides=False)</code>","text":"<p>Create a common parser for specifying the core args (except for dataset which is separate)</p> Source code in <code>cli/common_arguments.py</code> <pre><code>def get_core_parser(overrides=False) -&gt; argparse.ArgumentParser:\n\"\"\"Create a common parser for specifying the core args (except for dataset which is separate)\"\"\"\ncore = argparse.ArgumentParser(add_help=False)\ncore_grp = core.add_argument_group(title=\"options\")\ncore_grp.add_argument(\n\"-e\",\n\"--experiment-name\",\ntype=str,\ndefault=TIME,\nhelp=f\"name the experiment run to affect logging, config, and default-behaviour io\",\n)\ncore_grp.add_argument(\n\"-s\",\n\"--seed\",\ntype=int,\nhelp=\"specify a seed for reproducibility, this is a recommended option for reproducibility\",\n)\ncore_grp.add_argument(\n\"--save-config\",\naction=\"store_true\",\nhelp=\"save the config provided via the cli, this is a recommended option for reproducibility\",\n)\nreturn core\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.common_arguments.get_dataset_parser","title":"<code>get_dataset_parser(overrides=False)</code>","text":"<p>Create a common parser for specifying the dataset</p> Source code in <code>cli/common_arguments.py</code> <pre><code>def get_dataset_parser(overrides=False) -&gt; argparse.ArgumentParser:\n\"\"\"Create a common parser for specifying the dataset\"\"\"\ndataset = argparse.ArgumentParser(add_help=False)\ndataset_grp = dataset.add_argument_group(title=\"options\")\ndataset_grp.add_argument(\n\"-d\",\n\"--dataset\",\nrequired=(not overrides),\ntype=str,\nhelp=\"the name of the dataset to experiment with, should be present in `&lt;DATA_DIR&gt;`\",\n)\nreturn dataset\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.common_arguments.suffix_parser_generator","title":"<code>suffix_parser_generator(name, help, required=False)</code>","text":"<p>Generator function for creating common parsers for specifying a potential suffix filename</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the argument</p> required <code>help</code> <code>str</code> <p>the help message for the argument</p> required <code>required</code> <code>bool</code> <p>whether the argument is required</p> <code>False</code> Source code in <code>cli/common_arguments.py</code> <pre><code>def suffix_parser_generator(name: str, help: str, required: bool = False) -&gt; argparse.ArgumentParser:\n\"\"\"Generator function for creating common parsers for specifying a potential suffix filename\n    Args:\n        name: the name of the argument\n        help: the help message for the argument\n        required: whether the argument is required\n    \"\"\"\ndef get_parser(overrides: bool = False) -&gt; argparse.ArgumentParser:\nparser = argparse.ArgumentParser(add_help=False)\nparser_grp = parser.add_argument_group(title=COMMON_TITLE)\nparser_grp.add_argument(\nf\"--{name}\",\nrequired=required and not overrides,\ntype=str,\ndefault=f\"_{name}\",\nhelp=help,\n)\nreturn parser\nreturn get_parser\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config","title":"<code>config</code>","text":"<p>Read, write and process config files, including handling of module-specific config overrides.</p>"},{"location":"reference/cli/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>cli/config.py</code> <pre><code>def assemble_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\nargs_dict = vars(args)\n# Filter out the keys that are not relevant to the config file\nargs_dict = filter_dict(\nargs_dict, {\"func\", \"experiment_name\", \"save_config\", \"save_config_path\", \"module_handover\"}\n)\nfor k in args_dict.copy().keys():\n# Remove empty metric lists from the config\nif \"_metrics\" in k and not args_dict[k]:\nargs_dict.pop(k)\nmodules_to_run = args_dict.pop(\"modules_to_run\")\nif len(modules_to_run) == 1:\nrun_type = modules_to_run[0]\nelif modules_to_run == PIPELINE:\nrun_type = \"pipeline\"\nelse:\nraise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n# Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\nmodule_args = {\nmodule_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\nfor module_name in modules_to_run\n}\n# Use the flat namespace to populate a nested (by module) dictionary of config args and values\nout_dict = {}\nfor module_name in modules_to_run:\nfor k in args_dict.copy().keys():\n# We want to keep dataset, experiment_name, seed and save_config at the top-level as they are core args\nif k in module_args[module_name] and k not in {\"dataset\", \"experiment_name\", \"seed\", \"save_config\"}:\nif out_dict.get(module_name):\nout_dict[module_name].update({k: args_dict.pop(k)})\nelse:\nout_dict[module_name] = {k: args_dict.pop(k)}\n# Assemble the final dictionary in YAML-compliant form\nreturn {**({\"run_type\": run_type} if run_type else {}), **args_dict, **out_dict}\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>ArgumentParser</code> <p>The top-level parser.</p> required <code>module_parsers</code> <code>dict</code> <p>The dict of module-level parsers mapped to their names.</p> required <p>Returns:</p> Type Description <code>A tuple containing two elements</code> <ul> <li>A dictionary containing all arguments and their default values.<ul> <li>A list of kvps of the required arguments and their associated module.</li> </ul> </li> </ul> Source code in <code>cli/config.py</code> <pre><code>def get_default_and_required_args(\ntop_parser: argparse.ArgumentParser,\nmodule_parsers: dict[str, argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n    Args:\n        top_parser: The top-level parser.\n        module_parsers: The dict of module-level parsers mapped to their names.\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of kvps of the required arguments and their associated module.\n    \"\"\"\nall_actions = {\n\"top-level\": top_parser._actions,\n**{m: p._actions for m, p in module_parsers.items()},\n}\ndefaults = {}\nrequired_args = []\nfor module, actions in all_actions.items():\nfor action in actions:\nif action.dest not in [\"help\", \"==SUPPRESS==\"]:\ndefaults[action.dest] = action.default\nif action.required:\nrequired_args.append({\"arg\": action.dest, \"module\": module})\nreturn defaults, required_args\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.get_modules_to_run","title":"<code>get_modules_to_run(executor)</code>","text":"<p>Get the list of modules to run from the passed executor function.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Callable</code> <p>The executor function to run.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of module names to run.</p> Source code in <code>cli/config.py</code> <pre><code>def get_modules_to_run(executor: Callable) -&gt; list[str]:\n\"\"\"\n    Get the list of modules to run from the passed executor function.\n    Args:\n        executor: The executor function to run.\n    Returns:\n        A list of module names to run.\n    \"\"\"\nif executor == run_pipeline:\nreturn PIPELINE\nelse:\nreturn [get_key_by_value({mn: mc.func for mn, mc in MODULE_MAP.items()}, executor)]\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config Namespace object for the inferred modules to run and executes.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default config from the parser and <code>all_subparsers</code> for the <code>modules_to_run</code></li> <li>Overwrite this config with the specified subset (or full set) of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting config</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>ArgumentParser</code> <p>top-level ArgumentParser object</p> required <code>all_subparsers</code> <code>dict</code> <p>dictionary of ArgumentParser objects, one for each module</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>Namespace object containing the assembled configuration settings</p> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file</p> Source code in <code>cli/config.py</code> <pre><code>def read_config(\nargs: argparse.Namespace,\nparser: argparse.ArgumentParser,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config Namespace object for the inferred modules to run and executes.\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default config from the parser and `all_subparsers` for the `modules_to_run`\n    4. Overwrite this config with the specified subset (or full set) of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting config\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level ArgumentParser object\n        all_subparsers: dictionary of ArgumentParser objects, one for each module\n    Returns:\n        Namespace object containing the assembled configuration settings\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file\n    \"\"\"\n# Open the passed yaml file and load into a dictionary\nwith open(f\"config/{args.input_config}.yaml\") as stream:\nconfig_dict = yaml.safe_load(stream)\nvalid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\nrun_type = config_dict.pop(\"run_type\", None)\nif run_type == \"pipeline\":\nmodules_to_run = PIPELINE\nelse:\nmodules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\nif not args.custom_pipeline:\nmodules_to_run = sorted(modules_to_run, key=lambda x: PIPELINE.index(x))\nif not modules_to_run:\nwarnings.warn(\n\"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n)\nmodules_to_run = PIPELINE\n# Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\nargs_dict, required_args = get_default_and_required_args(\nparser, filter_dict(all_subparsers, modules_to_run, include=True)\n)\n# Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\nnon_default_passed_args_dict = {\nk: v\nfor k, v in vars(args).items()\nif k in [\"input_config\", \"custom_pipeline\"] or (k in args_dict and k != \"func\" and v != args_dict[k])\n}\n# Overwrite the default arguments with the ones from the yaml file\nargs_dict.update(flatten_dict(config_dict))\n# Overwrite the result of the above with any non-default CLI args\nargs_dict.update(non_default_passed_args_dict)\n# Create a new Namespace using the assembled dictionary\nnew_args = argparse.Namespace(**args_dict)\nassert getattr(\nnew_args, \"dataset\"\n), \"No dataset specified in the passed config file, provide one with the `--dataset` argument or add it to the config file\"\nassert all(\ngetattr(new_args, req_arg[\"arg\"]) for req_arg in required_args\n), f\"Required arguments are missing from the passed config file: {[ra['module'] + ':' + ra['arg'] for ra in required_args if not getattr(new_args, ra['arg'])]}\"\n# Run the appropriate execution function(s)\nif not new_args.seed:\nwarnings.warn(\"No seed has been specified, meaning the results of this run may not be reproducible.\")\nnew_args.modules_to_run = modules_to_run\nnew_args.module_handover = {}\nfor module in new_args.modules_to_run:\nMODULE_MAP[module].func(new_args)\nreturn new_args\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary containing all subparsers for the config args.</p> required Source code in <code>cli/config.py</code> <pre><code>def write_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n    \"\"\"\nexperiment_name = args.experiment_name\nargs_dict = assemble_config(args, all_subparsers)\nwith open(f\"experiments/{experiment_name}/config_{experiment_name}.yaml\", \"w\") as yaml_file:\nyaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments","title":"<code>module_arguments</code>","text":"<p>Define arguments for each of the modules' CLI sub-parsers.</p>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.customAction","title":"<code> customAction            (Action)         </code>","text":"<p>Customized argparse action for defaulting to the full list of choices if only the flag is supplied.</p> <pre><code>1) If no `option_string` is supplied: set to default value (`self.default`)\n2) If `option_string` is supplied:\n    a) If `values` are supplied, set to list of values\n    b) If no `values` are supplied, set to `self.const`, if `self.const` is not set, set to `self.default`\n</code></pre> Source code in <code>cli/module_arguments.py</code> <pre><code>class customAction(argparse.Action):\n\"\"\"\n    Customized argparse action for defaulting to the full list of choices if only the flag is supplied.\n        1) If no `option_string` is supplied: set to default value (`self.default`)\n        2) If `option_string` is supplied:\n            a) If `values` are supplied, set to list of values\n            b) If no `values` are supplied, set to `self.const`, if `self.const` is not set, set to `self.default`\n    \"\"\"\ndef __call__(self, parser, namespace, values=None, option_string=None):\nif values:\nsetattr(namespace, self.dest, values)\nelif option_string:\nsetattr(namespace, self.dest, self.const if self.const else self.default)\nelse:\nsetattr(namespace, self.dest, self.default)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--data-dir\",\ntype=str,\ndefault=\"./data\",\nhelp=\"the directory containing the chosen dataset\",\n)\ngroup.add_argument(\n\"--index-col\",\ndefault=None,\nchoices=[None, 0],\nhelp=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n)\ngroup.add_argument(\n\"--allow-null-transformers\",\naction=\"store_true\",\nhelp=\"allow null / None transformers, i.e. leave some columns as they are\",\n)\ngroup.add_argument(\n\"--collapse-yaml\",\naction=\"store_true\",\nhelp=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n)\ngroup.add_argument(\n\"--synthesizer\",\ntype=str,\ndefault=\"TVAE\",\nchoices=list(SDV_SYNTHESIZER_CHOICES.keys()),\nhelp=\"pick a synthesizer to use (note this can also be specified in the model module, these must match)\",\n)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--diagnostic\",\naction=\"store_true\",\nhelp=\"run the diagnostic evaluation\",\n)\ngroup.add_argument(\n\"--quality\",\naction=\"store_true\",\nhelp=\"run the quality evaluation\",\n)\nfor name in SDV_METRIC_CHOICES:\ngenerate_evaluation_arg(group, name)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--model-file\",\ntype=str,\ndefault=\"_model\",\nhelp=\"specify the filename of the model to be saved in `experiments/&lt;EXPERIMENT_NAME&gt;/`, defaults to `&lt;args.real_data&gt;_model.pt`\",\n)\ngroup.add_argument(\n\"--use-gpu\",\naction=\"store_true\",\nhelp=\"use the GPU for training\",\n)\ngroup.add_argument(\n\"--non-private-training\",\naction=\"store_true\",\nhelp=\"train the model in a non-private way\",\n)\ngroup.add_argument(\n\"--secure-rng\",\naction=\"store_true\",\ndefault=False,\nhelp=\"Enable Secure RNG to have trustworthy privacy guarantees. Comes at a performance cost\",\n)\ngroup.add_argument(\n\"--num-epochs\",\ntype=int,\ndefault=100,\nhelp=\"number of epochs to train for\",\n)\ngroup.add_argument(\n\"--tracked-metrics\",\ntype=str,\nnargs=\"+\",\ndefault=TRACKED_METRIC_CHOICES,\nhelp=\"metrics to track during training of the DPVAE model\",\nchoices=TRACKED_METRIC_CHOICES,\n)\ngroup.add_argument(\n\"--latent-dim\",\ntype=int,\ndefault=256,\nhelp=\"the latent dimension of the model\",\n)\ngroup.add_argument(\n\"--hidden-dim\",\ntype=int,\ndefault=256,\nhelp=\"the hidden dimension of the model\",\n)\ngroup.add_argument(\n\"--learning-rate\",\ntype=float,\ndefault=1e-3,\nhelp=\"the learning rate for the model\",\n)\ngroup.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=\"the batch size for the model\",\n)\ngroup.add_argument(\n\"--patience\",\ntype=int,\ndefault=5,\nhelp=\"how many epochs the model is allowed to train for without improvement\",\n)\ngroup.add_argument(\n\"--delta\",\ntype=int,\ndefault=10,\nhelp=\"the difference in successive ELBO values that register as an 'improvement'\",\n)\ngroup.add_argument(\n\"--target-epsilon\",\ntype=float,\ndefault=1.0,\nhelp=\"the target epsilon for differential privacy\",\n)\ngroup.add_argument(\n\"--target-delta\",\ntype=float,\ndefault=1e-5,\nhelp=\"the target delta for differential privacy\",\n)\ngroup.add_argument(\n\"--max-grad-norm\",\ntype=int,\ndefault=10,\nhelp=\"the clipping threshold for gradients (only relevant under differential privacy)\",\n)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--plot-sdv-report\",\naction=\"store_true\",\nhelp=\"plot the SDV report\",\n)\ngroup.add_argument(\n\"--plot-tsne\",\naction=\"store_true\",\nhelp=\"plot the t-SNE embeddings of the real and synthetic data\",\n)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup","title":"<code>module_setup</code>","text":"<p>Specify the modules to be used in the CLI, and the pipeline to run by default, as well as special functions for the <code>config</code> and <code>pipeline</code> CLI options.</p>"},{"location":"reference/cli/#nhssynth.cli.module_setup.ModuleConfig","title":"<code> ModuleConfig        </code>","text":"Source code in <code>cli/module_setup.py</code> <pre><code>class ModuleConfig:\ndef __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\ncommon_parsers: list[str] = None,\n) -&gt; None:\n\"\"\"\n        Represents a module's configuration, containing the following attributes:\n        Args:\n            func: A callable that executes the module's functionality.\n            add_args_func: A callable that populates the module's sub-parser arguments.\n            description: A description of the module's functionality.\n            help: A help message for the module's command-line interface.\n            common_parsers: A list of common parsers to add to the module's sub-parser.\n        \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\nif common_parsers:\nassert set(common_parsers) &lt;= COMMON_PARSERS.keys(), \"Invalid common parser(s) specified.\"\nassert (\n\"dataset\" not in common_parsers\n), \"The 'dataset' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nassert (\n\"core\" not in common_parsers\n), \"The 'core' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nself.common_parsers = [\"dataset\", \"core\"] + common_parsers\nelse:\nself.common_parsers = [\"dataset\", \"core\"]\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.ModuleConfig.__init__","title":"<code>__init__(self, func, add_args_func, description, help, common_parsers=None)</code>  <code>special</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>A callable that executes the module's functionality.</p> required <code>add_args_func</code> <code>Callable[..., Any]</code> <p>A callable that populates the module's sub-parser arguments.</p> required <code>description</code> <code>str</code> <p>A description of the module's functionality.</p> required <code>help</code> <code>str</code> <p>A help message for the module's command-line interface.</p> required <code>common_parsers</code> <code>list</code> <p>A list of common parsers to add to the module's sub-parser.</p> <code>None</code> Source code in <code>cli/module_setup.py</code> <pre><code>def __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\ncommon_parsers: list[str] = None,\n) -&gt; None:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n    Args:\n        func: A callable that executes the module's functionality.\n        add_args_func: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n        common_parsers: A list of common parsers to add to the module's sub-parser.\n    \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\nif common_parsers:\nassert set(common_parsers) &lt;= COMMON_PARSERS.keys(), \"Invalid common parser(s) specified.\"\nassert (\n\"dataset\" not in common_parsers\n), \"The 'dataset' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nassert (\n\"core\" not in common_parsers\n), \"The 'core' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nself.common_parsers = [\"dataset\", \"core\"] + common_parsers\nelse:\nself.common_parsers = [\"dataset\", \"core\"]\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\nparser.add_argument(\n\"-c\",\n\"--input-config\",\nrequired=True,\nhelp=\"specify the config file name\",\n)\nparser.add_argument(\n\"-cp\",\n\"--custom-pipeline\",\naction=\"store_true\",\nhelp=\"infer a custom pipeline running order of modules from the config\",\n)\nfor module_name in VALID_MODULES:\nMODULE_MAP[module_name].add_args_func(parser, f\"{module_name} option overrides\", overrides=True)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> for each module in the pipeline.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` for each module in the pipeline.\"\"\"\nfor module_name in PIPELINE:\nMODULE_MAP[module_name].add_args_func(parser, f\"{module_name} options\")\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, module_config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>module_config</code> <code>ModuleConfig</code> <p>A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_subparser(\nsubparsers: argparse._SubParsersAction,\nname: str,\nmodule_config: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        module_config: A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.\n    Returns:\n        The newly created subparser.\n    \"\"\"\nparent_parsers = get_parent_parsers(name, module_config.common_parsers)\nparser = subparsers.add_parser(\nname=name,\ndescription=module_config.description,\nhelp=module_config.help,\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nparents=parent_parsers,\n)\nif name not in {\"pipeline\", \"config\"}:\nmodule_config.add_args_func(parser, f\"{name} options\")\nelse:\nmodule_config.add_args_func(parser)\nparser.set_defaults(func=module_config.func)\nreturn parser\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.get_parent_parsers","title":"<code>get_parent_parsers(name, module_parsers)</code>","text":"<p>Get a list of parent parsers for a given module, based on the module's <code>common_parsers</code> attribute.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def get_parent_parsers(name: str, module_parsers: list[str]) -&gt; list[argparse.ArgumentParser]:\n\"\"\"Get a list of parent parsers for a given module, based on the module's `common_parsers` attribute.\"\"\"\nif name in {\"pipeline\", \"config\"}:\nreturn [p(name == \"config\") for p in COMMON_PARSERS.values()]\nelse:\nreturn [COMMON_PARSERS[pn]() for pn in module_parsers]\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\nprint(\"Running full pipeline...\")\nargs.modules_to_run = PIPELINE\nfor module_name in PIPELINE:\nargs = MODULE_MAP[module_name].func(args)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.run","title":"<code>run</code>","text":""},{"location":"reference/cli/#nhssynth.cli.run.run","title":"<code>run()</code>","text":"<p>CLI for preparing, training and evaluating a synthetic data generator.</p> Source code in <code>cli/run.py</code> <pre><code>def run() -&gt; None:\n\"\"\"CLI for preparing, training and evaluating a synthetic data generator.\"\"\"\nparser = argparse.ArgumentParser(\nprog=\"nhssynth\",\ndescription=\"CLI for preparing, training and evaluating a synthetic data generator.\",\n)\n# Below we instantiate one subparser for each module + one for running with a config file and one for\n# doing a full pipeline run with CLI-specified config\nsubparsers = parser.add_subparsers()\nall_subparsers = {\nname: add_subparser(subparsers, name, option_config) for name, option_config in MODULE_MAP.items()\n}\nargs = parser.parse_args()\n# Use get to return None when no function has been set, i.e. user made no running choice\nexecutor = vars(args).get(\"func\")\nif executor:\nif not args.seed:\nwarnings.warn(\"No seed has been specified, meaning the results of this run may not be reproducible.\")\nargs.modules_to_run = get_modules_to_run(executor)\nargs.module_handover = {}\nexecutor(args)\nelif hasattr(args, \"input_config\"):\nargs = read_config(args, parser, all_subparsers)\nelse:\nreturn parser.print_help()\n# Whenever either are specified, we want to dump the configuration to allow for this run to be replicated\nif args.save_config:\nwrite_config(args, all_subparsers)\nprint(\"Finished!\")\n</code></pre>"},{"location":"reference/cli/common_arguments/","title":"common_arguments","text":"<p>Functions to define the CLI's \"common\" arguments, i.e. those that can be applied to either:  - All module argument lists, e.g. --dataset, --seed, etc.  - A subset of module argument lists, e.g. --synthetic, --typed, etc.</p>"},{"location":"reference/cli/common_arguments/#nhssynth.cli.common_arguments.get_core_parser","title":"<code>get_core_parser(overrides=False)</code>","text":"<p>Create a common parser for specifying the core args (except for dataset which is separate)</p> Source code in <code>cli/common_arguments.py</code> <pre><code>def get_core_parser(overrides=False) -&gt; argparse.ArgumentParser:\n\"\"\"Create a common parser for specifying the core args (except for dataset which is separate)\"\"\"\ncore = argparse.ArgumentParser(add_help=False)\ncore_grp = core.add_argument_group(title=\"options\")\ncore_grp.add_argument(\n\"-e\",\n\"--experiment-name\",\ntype=str,\ndefault=TIME,\nhelp=f\"name the experiment run to affect logging, config, and default-behaviour io\",\n)\ncore_grp.add_argument(\n\"-s\",\n\"--seed\",\ntype=int,\nhelp=\"specify a seed for reproducibility, this is a recommended option for reproducibility\",\n)\ncore_grp.add_argument(\n\"--save-config\",\naction=\"store_true\",\nhelp=\"save the config provided via the cli, this is a recommended option for reproducibility\",\n)\nreturn core\n</code></pre>"},{"location":"reference/cli/common_arguments/#nhssynth.cli.common_arguments.get_dataset_parser","title":"<code>get_dataset_parser(overrides=False)</code>","text":"<p>Create a common parser for specifying the dataset</p> Source code in <code>cli/common_arguments.py</code> <pre><code>def get_dataset_parser(overrides=False) -&gt; argparse.ArgumentParser:\n\"\"\"Create a common parser for specifying the dataset\"\"\"\ndataset = argparse.ArgumentParser(add_help=False)\ndataset_grp = dataset.add_argument_group(title=\"options\")\ndataset_grp.add_argument(\n\"-d\",\n\"--dataset\",\nrequired=(not overrides),\ntype=str,\nhelp=\"the name of the dataset to experiment with, should be present in `&lt;DATA_DIR&gt;`\",\n)\nreturn dataset\n</code></pre>"},{"location":"reference/cli/common_arguments/#nhssynth.cli.common_arguments.suffix_parser_generator","title":"<code>suffix_parser_generator(name, help, required=False)</code>","text":"<p>Generator function for creating common parsers for specifying a potential suffix filename</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the argument</p> required <code>help</code> <code>str</code> <p>the help message for the argument</p> required <code>required</code> <code>bool</code> <p>whether the argument is required</p> <code>False</code> Source code in <code>cli/common_arguments.py</code> <pre><code>def suffix_parser_generator(name: str, help: str, required: bool = False) -&gt; argparse.ArgumentParser:\n\"\"\"Generator function for creating common parsers for specifying a potential suffix filename\n    Args:\n        name: the name of the argument\n        help: the help message for the argument\n        required: whether the argument is required\n    \"\"\"\ndef get_parser(overrides: bool = False) -&gt; argparse.ArgumentParser:\nparser = argparse.ArgumentParser(add_help=False)\nparser_grp = parser.add_argument_group(title=COMMON_TITLE)\nparser_grp.add_argument(\nf\"--{name}\",\nrequired=required and not overrides,\ntype=str,\ndefault=f\"_{name}\",\nhelp=help,\n)\nreturn parser\nreturn get_parser\n</code></pre>"},{"location":"reference/cli/config/","title":"config","text":"<p>Read, write and process config files, including handling of module-specific config overrides.</p>"},{"location":"reference/cli/config/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>cli/config.py</code> <pre><code>def assemble_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\nargs_dict = vars(args)\n# Filter out the keys that are not relevant to the config file\nargs_dict = filter_dict(\nargs_dict, {\"func\", \"experiment_name\", \"save_config\", \"save_config_path\", \"module_handover\"}\n)\nfor k in args_dict.copy().keys():\n# Remove empty metric lists from the config\nif \"_metrics\" in k and not args_dict[k]:\nargs_dict.pop(k)\nmodules_to_run = args_dict.pop(\"modules_to_run\")\nif len(modules_to_run) == 1:\nrun_type = modules_to_run[0]\nelif modules_to_run == PIPELINE:\nrun_type = \"pipeline\"\nelse:\nraise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n# Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\nmodule_args = {\nmodule_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\nfor module_name in modules_to_run\n}\n# Use the flat namespace to populate a nested (by module) dictionary of config args and values\nout_dict = {}\nfor module_name in modules_to_run:\nfor k in args_dict.copy().keys():\n# We want to keep dataset, experiment_name, seed and save_config at the top-level as they are core args\nif k in module_args[module_name] and k not in {\"dataset\", \"experiment_name\", \"seed\", \"save_config\"}:\nif out_dict.get(module_name):\nout_dict[module_name].update({k: args_dict.pop(k)})\nelse:\nout_dict[module_name] = {k: args_dict.pop(k)}\n# Assemble the final dictionary in YAML-compliant form\nreturn {**({\"run_type\": run_type} if run_type else {}), **args_dict, **out_dict}\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>ArgumentParser</code> <p>The top-level parser.</p> required <code>module_parsers</code> <code>dict</code> <p>The dict of module-level parsers mapped to their names.</p> required <p>Returns:</p> Type Description <code>A tuple containing two elements</code> <ul> <li>A dictionary containing all arguments and their default values.<ul> <li>A list of kvps of the required arguments and their associated module.</li> </ul> </li> </ul> Source code in <code>cli/config.py</code> <pre><code>def get_default_and_required_args(\ntop_parser: argparse.ArgumentParser,\nmodule_parsers: dict[str, argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n    Args:\n        top_parser: The top-level parser.\n        module_parsers: The dict of module-level parsers mapped to their names.\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of kvps of the required arguments and their associated module.\n    \"\"\"\nall_actions = {\n\"top-level\": top_parser._actions,\n**{m: p._actions for m, p in module_parsers.items()},\n}\ndefaults = {}\nrequired_args = []\nfor module, actions in all_actions.items():\nfor action in actions:\nif action.dest not in [\"help\", \"==SUPPRESS==\"]:\ndefaults[action.dest] = action.default\nif action.required:\nrequired_args.append({\"arg\": action.dest, \"module\": module})\nreturn defaults, required_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_modules_to_run","title":"<code>get_modules_to_run(executor)</code>","text":"<p>Get the list of modules to run from the passed executor function.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Callable</code> <p>The executor function to run.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of module names to run.</p> Source code in <code>cli/config.py</code> <pre><code>def get_modules_to_run(executor: Callable) -&gt; list[str]:\n\"\"\"\n    Get the list of modules to run from the passed executor function.\n    Args:\n        executor: The executor function to run.\n    Returns:\n        A list of module names to run.\n    \"\"\"\nif executor == run_pipeline:\nreturn PIPELINE\nelse:\nreturn [get_key_by_value({mn: mc.func for mn, mc in MODULE_MAP.items()}, executor)]\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config Namespace object for the inferred modules to run and executes.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default config from the parser and <code>all_subparsers</code> for the <code>modules_to_run</code></li> <li>Overwrite this config with the specified subset (or full set) of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting config</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>ArgumentParser</code> <p>top-level ArgumentParser object</p> required <code>all_subparsers</code> <code>dict</code> <p>dictionary of ArgumentParser objects, one for each module</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>Namespace object containing the assembled configuration settings</p> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file</p> Source code in <code>cli/config.py</code> <pre><code>def read_config(\nargs: argparse.Namespace,\nparser: argparse.ArgumentParser,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config Namespace object for the inferred modules to run and executes.\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default config from the parser and `all_subparsers` for the `modules_to_run`\n    4. Overwrite this config with the specified subset (or full set) of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting config\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level ArgumentParser object\n        all_subparsers: dictionary of ArgumentParser objects, one for each module\n    Returns:\n        Namespace object containing the assembled configuration settings\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file\n    \"\"\"\n# Open the passed yaml file and load into a dictionary\nwith open(f\"config/{args.input_config}.yaml\") as stream:\nconfig_dict = yaml.safe_load(stream)\nvalid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\nrun_type = config_dict.pop(\"run_type\", None)\nif run_type == \"pipeline\":\nmodules_to_run = PIPELINE\nelse:\nmodules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\nif not args.custom_pipeline:\nmodules_to_run = sorted(modules_to_run, key=lambda x: PIPELINE.index(x))\nif not modules_to_run:\nwarnings.warn(\n\"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n)\nmodules_to_run = PIPELINE\n# Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\nargs_dict, required_args = get_default_and_required_args(\nparser, filter_dict(all_subparsers, modules_to_run, include=True)\n)\n# Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\nnon_default_passed_args_dict = {\nk: v\nfor k, v in vars(args).items()\nif k in [\"input_config\", \"custom_pipeline\"] or (k in args_dict and k != \"func\" and v != args_dict[k])\n}\n# Overwrite the default arguments with the ones from the yaml file\nargs_dict.update(flatten_dict(config_dict))\n# Overwrite the result of the above with any non-default CLI args\nargs_dict.update(non_default_passed_args_dict)\n# Create a new Namespace using the assembled dictionary\nnew_args = argparse.Namespace(**args_dict)\nassert getattr(\nnew_args, \"dataset\"\n), \"No dataset specified in the passed config file, provide one with the `--dataset` argument or add it to the config file\"\nassert all(\ngetattr(new_args, req_arg[\"arg\"]) for req_arg in required_args\n), f\"Required arguments are missing from the passed config file: {[ra['module'] + ':' + ra['arg'] for ra in required_args if not getattr(new_args, ra['arg'])]}\"\n# Run the appropriate execution function(s)\nif not new_args.seed:\nwarnings.warn(\"No seed has been specified, meaning the results of this run may not be reproducible.\")\nnew_args.modules_to_run = modules_to_run\nnew_args.module_handover = {}\nfor module in new_args.modules_to_run:\nMODULE_MAP[module].func(new_args)\nreturn new_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary containing all subparsers for the config args.</p> required Source code in <code>cli/config.py</code> <pre><code>def write_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n    \"\"\"\nexperiment_name = args.experiment_name\nargs_dict = assemble_config(args, all_subparsers)\nwith open(f\"experiments/{experiment_name}/config_{experiment_name}.yaml\", \"w\") as yaml_file:\nyaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/module_arguments/","title":"module_arguments","text":"<p>Define arguments for each of the modules' CLI sub-parsers.</p>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.customAction","title":"<code> customAction            (Action)         </code>","text":"<p>Customized argparse action for defaulting to the full list of choices if only the flag is supplied.</p> <pre><code>1) If no `option_string` is supplied: set to default value (`self.default`)\n2) If `option_string` is supplied:\n    a) If `values` are supplied, set to list of values\n    b) If no `values` are supplied, set to `self.const`, if `self.const` is not set, set to `self.default`\n</code></pre> Source code in <code>cli/module_arguments.py</code> <pre><code>class customAction(argparse.Action):\n\"\"\"\n    Customized argparse action for defaulting to the full list of choices if only the flag is supplied.\n        1) If no `option_string` is supplied: set to default value (`self.default`)\n        2) If `option_string` is supplied:\n            a) If `values` are supplied, set to list of values\n            b) If no `values` are supplied, set to `self.const`, if `self.const` is not set, set to `self.default`\n    \"\"\"\ndef __call__(self, parser, namespace, values=None, option_string=None):\nif values:\nsetattr(namespace, self.dest, values)\nelif option_string:\nsetattr(namespace, self.dest, self.const if self.const else self.default)\nelse:\nsetattr(namespace, self.dest, self.default)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--data-dir\",\ntype=str,\ndefault=\"./data\",\nhelp=\"the directory containing the chosen dataset\",\n)\ngroup.add_argument(\n\"--index-col\",\ndefault=None,\nchoices=[None, 0],\nhelp=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n)\ngroup.add_argument(\n\"--allow-null-transformers\",\naction=\"store_true\",\nhelp=\"allow null / None transformers, i.e. leave some columns as they are\",\n)\ngroup.add_argument(\n\"--collapse-yaml\",\naction=\"store_true\",\nhelp=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n)\ngroup.add_argument(\n\"--synthesizer\",\ntype=str,\ndefault=\"TVAE\",\nchoices=list(SDV_SYNTHESIZER_CHOICES.keys()),\nhelp=\"pick a synthesizer to use (note this can also be specified in the model module, these must match)\",\n)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--diagnostic\",\naction=\"store_true\",\nhelp=\"run the diagnostic evaluation\",\n)\ngroup.add_argument(\n\"--quality\",\naction=\"store_true\",\nhelp=\"run the quality evaluation\",\n)\nfor name in SDV_METRIC_CHOICES:\ngenerate_evaluation_arg(group, name)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--model-file\",\ntype=str,\ndefault=\"_model\",\nhelp=\"specify the filename of the model to be saved in `experiments/&lt;EXPERIMENT_NAME&gt;/`, defaults to `&lt;args.real_data&gt;_model.pt`\",\n)\ngroup.add_argument(\n\"--use-gpu\",\naction=\"store_true\",\nhelp=\"use the GPU for training\",\n)\ngroup.add_argument(\n\"--non-private-training\",\naction=\"store_true\",\nhelp=\"train the model in a non-private way\",\n)\ngroup.add_argument(\n\"--secure-rng\",\naction=\"store_true\",\ndefault=False,\nhelp=\"Enable Secure RNG to have trustworthy privacy guarantees. Comes at a performance cost\",\n)\ngroup.add_argument(\n\"--num-epochs\",\ntype=int,\ndefault=100,\nhelp=\"number of epochs to train for\",\n)\ngroup.add_argument(\n\"--tracked-metrics\",\ntype=str,\nnargs=\"+\",\ndefault=TRACKED_METRIC_CHOICES,\nhelp=\"metrics to track during training of the DPVAE model\",\nchoices=TRACKED_METRIC_CHOICES,\n)\ngroup.add_argument(\n\"--latent-dim\",\ntype=int,\ndefault=256,\nhelp=\"the latent dimension of the model\",\n)\ngroup.add_argument(\n\"--hidden-dim\",\ntype=int,\ndefault=256,\nhelp=\"the hidden dimension of the model\",\n)\ngroup.add_argument(\n\"--learning-rate\",\ntype=float,\ndefault=1e-3,\nhelp=\"the learning rate for the model\",\n)\ngroup.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=\"the batch size for the model\",\n)\ngroup.add_argument(\n\"--patience\",\ntype=int,\ndefault=5,\nhelp=\"how many epochs the model is allowed to train for without improvement\",\n)\ngroup.add_argument(\n\"--delta\",\ntype=int,\ndefault=10,\nhelp=\"the difference in successive ELBO values that register as an 'improvement'\",\n)\ngroup.add_argument(\n\"--target-epsilon\",\ntype=float,\ndefault=1.0,\nhelp=\"the target epsilon for differential privacy\",\n)\ngroup.add_argument(\n\"--target-delta\",\ntype=float,\ndefault=1e-5,\nhelp=\"the target delta for differential privacy\",\n)\ngroup.add_argument(\n\"--max-grad-norm\",\ntype=int,\ndefault=10,\nhelp=\"the clipping threshold for gradients (only relevant under differential privacy)\",\n)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--plot-sdv-report\",\naction=\"store_true\",\nhelp=\"plot the SDV report\",\n)\ngroup.add_argument(\n\"--plot-tsne\",\naction=\"store_true\",\nhelp=\"plot the t-SNE embeddings of the real and synthetic data\",\n)\n</code></pre>"},{"location":"reference/cli/module_setup/","title":"module_setup","text":"<p>Specify the modules to be used in the CLI, and the pipeline to run by default, as well as special functions for the <code>config</code> and <code>pipeline</code> CLI options.</p>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig","title":"<code> ModuleConfig        </code>","text":"Source code in <code>cli/module_setup.py</code> <pre><code>class ModuleConfig:\ndef __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\ncommon_parsers: list[str] = None,\n) -&gt; None:\n\"\"\"\n        Represents a module's configuration, containing the following attributes:\n        Args:\n            func: A callable that executes the module's functionality.\n            add_args_func: A callable that populates the module's sub-parser arguments.\n            description: A description of the module's functionality.\n            help: A help message for the module's command-line interface.\n            common_parsers: A list of common parsers to add to the module's sub-parser.\n        \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\nif common_parsers:\nassert set(common_parsers) &lt;= COMMON_PARSERS.keys(), \"Invalid common parser(s) specified.\"\nassert (\n\"dataset\" not in common_parsers\n), \"The 'dataset' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nassert (\n\"core\" not in common_parsers\n), \"The 'core' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nself.common_parsers = [\"dataset\", \"core\"] + common_parsers\nelse:\nself.common_parsers = [\"dataset\", \"core\"]\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig.__init__","title":"<code>__init__(self, func, add_args_func, description, help, common_parsers=None)</code>  <code>special</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>A callable that executes the module's functionality.</p> required <code>add_args_func</code> <code>Callable[..., Any]</code> <p>A callable that populates the module's sub-parser arguments.</p> required <code>description</code> <code>str</code> <p>A description of the module's functionality.</p> required <code>help</code> <code>str</code> <p>A help message for the module's command-line interface.</p> required <code>common_parsers</code> <code>list</code> <p>A list of common parsers to add to the module's sub-parser.</p> <code>None</code> Source code in <code>cli/module_setup.py</code> <pre><code>def __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\ncommon_parsers: list[str] = None,\n) -&gt; None:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n    Args:\n        func: A callable that executes the module's functionality.\n        add_args_func: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n        common_parsers: A list of common parsers to add to the module's sub-parser.\n    \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\nif common_parsers:\nassert set(common_parsers) &lt;= COMMON_PARSERS.keys(), \"Invalid common parser(s) specified.\"\nassert (\n\"dataset\" not in common_parsers\n), \"The 'dataset' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nassert (\n\"core\" not in common_parsers\n), \"The 'core' parser is automatically added to all modules, remove it from the ModuleConfig.\"\nself.common_parsers = [\"dataset\", \"core\"] + common_parsers\nelse:\nself.common_parsers = [\"dataset\", \"core\"]\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\nparser.add_argument(\n\"-c\",\n\"--input-config\",\nrequired=True,\nhelp=\"specify the config file name\",\n)\nparser.add_argument(\n\"-cp\",\n\"--custom-pipeline\",\naction=\"store_true\",\nhelp=\"infer a custom pipeline running order of modules from the config\",\n)\nfor module_name in VALID_MODULES:\nMODULE_MAP[module_name].add_args_func(parser, f\"{module_name} option overrides\", overrides=True)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> for each module in the pipeline.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` for each module in the pipeline.\"\"\"\nfor module_name in PIPELINE:\nMODULE_MAP[module_name].add_args_func(parser, f\"{module_name} options\")\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, module_config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>module_config</code> <code>ModuleConfig</code> <p>A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_subparser(\nsubparsers: argparse._SubParsersAction,\nname: str,\nmodule_config: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        module_config: A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.\n    Returns:\n        The newly created subparser.\n    \"\"\"\nparent_parsers = get_parent_parsers(name, module_config.common_parsers)\nparser = subparsers.add_parser(\nname=name,\ndescription=module_config.description,\nhelp=module_config.help,\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nparents=parent_parsers,\n)\nif name not in {\"pipeline\", \"config\"}:\nmodule_config.add_args_func(parser, f\"{name} options\")\nelse:\nmodule_config.add_args_func(parser)\nparser.set_defaults(func=module_config.func)\nreturn parser\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.get_parent_parsers","title":"<code>get_parent_parsers(name, module_parsers)</code>","text":"<p>Get a list of parent parsers for a given module, based on the module's <code>common_parsers</code> attribute.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def get_parent_parsers(name: str, module_parsers: list[str]) -&gt; list[argparse.ArgumentParser]:\n\"\"\"Get a list of parent parsers for a given module, based on the module's `common_parsers` attribute.\"\"\"\nif name in {\"pipeline\", \"config\"}:\nreturn [p(name == \"config\") for p in COMMON_PARSERS.values()]\nelse:\nreturn [COMMON_PARSERS[pn]() for pn in module_parsers]\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\nprint(\"Running full pipeline...\")\nargs.modules_to_run = PIPELINE\nfor module_name in PIPELINE:\nargs = MODULE_MAP[module_name].func(args)\n</code></pre>"},{"location":"reference/cli/run/","title":"run","text":""},{"location":"reference/cli/run/#nhssynth.cli.run.run","title":"<code>run()</code>","text":"<p>CLI for preparing, training and evaluating a synthetic data generator.</p> Source code in <code>cli/run.py</code> <pre><code>def run() -&gt; None:\n\"\"\"CLI for preparing, training and evaluating a synthetic data generator.\"\"\"\nparser = argparse.ArgumentParser(\nprog=\"nhssynth\",\ndescription=\"CLI for preparing, training and evaluating a synthetic data generator.\",\n)\n# Below we instantiate one subparser for each module + one for running with a config file and one for\n# doing a full pipeline run with CLI-specified config\nsubparsers = parser.add_subparsers()\nall_subparsers = {\nname: add_subparser(subparsers, name, option_config) for name, option_config in MODULE_MAP.items()\n}\nargs = parser.parse_args()\n# Use get to return None when no function has been set, i.e. user made no running choice\nexecutor = vars(args).get(\"func\")\nif executor:\nif not args.seed:\nwarnings.warn(\"No seed has been specified, meaning the results of this run may not be reproducible.\")\nargs.modules_to_run = get_modules_to_run(executor)\nargs.module_handover = {}\nexecutor(args)\nelif hasattr(args, \"input_config\"):\nargs = read_config(args, parser, all_subparsers)\nelse:\nreturn parser.print_help()\n# Whenever either are specified, we want to dump the configuration to allow for this run to be replicated\nif args.save_config:\nwrite_config(args, all_subparsers)\nprint(\"Finished!\")\n</code></pre>"},{"location":"reference/common/","title":"common","text":""},{"location":"reference/common/#nhssynth.common.common","title":"<code>common</code>","text":""},{"location":"reference/common/#nhssynth.common.common.set_seed","title":"<code>set_seed(seed=None)</code>","text":"<p>(Potentially) set the seed for numpy, torch and random.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>None | int</code> <p>The seed to set.</p> <code>None</code> Source code in <code>common/common.py</code> <pre><code>def set_seed(seed: None | int = None) -&gt; None:\n\"\"\"\n    (Potentially) set the seed for numpy, torch and random.\n    Args:\n        seed: The seed to set.\n    \"\"\"\nif seed:\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)\n</code></pre>"},{"location":"reference/common/#nhssynth.common.constants","title":"<code>constants</code>","text":"<p>Define all of the common constants used throughout the project.</p>"},{"location":"reference/common/#nhssynth.common.dicts","title":"<code>dicts</code>","text":"<p>Common functions for working with dictionaries.</p>"},{"location":"reference/common/#nhssynth.common.dicts.filter_dict","title":"<code>filter_dict(d, filter_keys, include=False)</code>","text":"<p>Given a dictionary, return a new dictionary either including or excluding keys in a given <code>filter</code> set.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to filter.</p> required <code>filter_keys</code> <code>set | list</code> <p>A list or set of keys to either include or exclude.</p> required <code>include</code> <code>bool</code> <p>Determine whether to return a dictionary including or excluding keys in <code>filter</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A filtered dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n{'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n{'a': 1, 'b': 2}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def filter_dict(d: dict, filter_keys: set | list, include: bool = False) -&gt; dict:\n\"\"\"\n    Given a dictionary, return a new dictionary either including or excluding keys in a given `filter` set.\n    Args:\n        d: A dictionary to filter.\n        filter_keys: A list or set of keys to either include or exclude.\n        include: Determine whether to return a dictionary including or excluding keys in `filter`.\n    Returns:\n        A filtered dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n        {'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n        {'a': 1, 'b': 2}\n    \"\"\"\nif include:\nfiltered_keys = set(filter_keys) &amp; set(d.keys())\nelse:\nfiltered_keys = set(d.keys()) - set(filter_keys)\nreturn {k: v for k, v in d.items() if k in filtered_keys}\n</code></pre>"},{"location":"reference/common/#nhssynth.common.dicts.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary with possibly nested keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def flatten_dict(d: dict[str, Any]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n    Args:\n        d: A dictionary with possibly nested keys.\n    Returns:\n        A flattened dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\nitems = []\nfor k, v in d.items():\nif isinstance(v, dict):\nitems.extend(flatten_dict(v).items())\nelse:\nitems.append((k, v))\nreturn dict(items)\n</code></pre>"},{"location":"reference/common/#nhssynth.common.dicts.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def get_key_by_value(d: dict, value) -&gt; Any | None:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n    \"\"\"\nfor key, val in d.items():\nif val == value:\nreturn key\nreturn None\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io","title":"<code>io</code>","text":"<p>Common building-block functions for handling module input and output.</p>"},{"location":"reference/common/#nhssynth.common.io.check_exists","title":"<code>check_exists(fns, dir)</code>","text":"<p>Checks if the files in <code>fns</code> exist in <code>dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir_experiment</code> <p>The directory the files should exist in.</p> required <p>Exceptions:</p> Type Description <code>FileNotFoundError</code> <p>If any of the files in <code>fns</code> do not exist in <code>dir_experiment</code>.</p> Source code in <code>common/io.py</code> <pre><code>def check_exists(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Checks if the files in `fns` exist in `dir`.\n    Args:\n        fns: The list of files to check.\n        dir_experiment: The directory the files should exist in.\n    Raises:\n        FileNotFoundError: If any of the files in `fns` do not exist in `dir_experiment`.\n    \"\"\"\nfor fn in fns:\nif not (dir / fn).exists():\nraise FileNotFoundError(f\"File {fn} does not exist at {dir}.\")\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.consistent_ending","title":"<code>consistent_ending(fn, ending='.pkl')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, removes any existing ending and appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".pkl\".</p> <code>'.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>common/io.py</code> <pre><code>def consistent_ending(fn: str, ending: str = \".pkl\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, removes any existing ending and appends `ending`.\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".pkl\".\n    Returns:\n        The filename with the correct ending.\n    \"\"\"\npath_fn = Path(fn)\nif path_fn.suffix == ending:\nreturn fn\nelse:\nreturn str(path_fn.parent / path_fn.stem) + ending\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.experiment_io","title":"<code>experiment_io(experiment_name, dir_experiments='experiments')</code>","text":"<p>Create an experiment's directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <code>dir_experiments</code> <code>str</code> <p>The name of the directory containing all experiments.</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the experiment directory.</p> Source code in <code>common/io.py</code> <pre><code>def experiment_io(experiment_name: str, dir_experiments: str = \"experiments\") -&gt; str:\n\"\"\"\n    Create an experiment's directory and return the path.\n    Args:\n        experiment_name: The name of the experiment.\n        dir_experiments: The name of the directory containing all experiments.\n    Returns:\n        The path to the experiment directory.\n    \"\"\"\ndir_experiment = Path(dir_experiments) / experiment_name\ndir_experiment.mkdir(parents=True, exist_ok=True)\nreturn dir_experiment\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.potential_suffix","title":"<code>potential_suffix(fn, fn_base)</code>","text":"<p>Checks if <code>fn</code> is a suffix (starts with an underscore) to append to <code>fn_base</code>, or a filename in its own right.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename / potential suffix to append to <code>fn_base</code>.</p> required <code>fn_base</code> <code>str</code> <p>The name of the file the suffix would attach to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The appropriately processed <code>fn</code></p> Source code in <code>common/io.py</code> <pre><code>def potential_suffix(fn: str, fn_base: str) -&gt; str:\n\"\"\"\n    Checks if `fn` is a suffix (starts with an underscore) to append to `fn_base`, or a filename in its own right.\n    Args:\n        fn: The filename / potential suffix to append to `fn_base`.\n        fn_base: The name of the file the suffix would attach to.\n    Returns:\n        The appropriately processed `fn`\n    \"\"\"\nfn_base = Path(fn_base).stem\nif fn[0] == \"_\":\nreturn fn_base + fn\nelse:\nreturn fn\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.warn_if_path_supplied","title":"<code>warn_if_path_supplied(fns, dir)</code>","text":"<p>Warns if the files in <code>fns</code> include directory separators.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir</code> <code>Path</code> <p>The directory the files should exist in.</p> required <p>!!! warnings     Raises a UserWarning when the path to any of the files in <code>fns</code> includes directory separators, as this may not work as intended.</p> Source code in <code>common/io.py</code> <pre><code>def warn_if_path_supplied(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Warns if the files in `fns` include directory separators.\n    Args:\n        fns: The list of files to check.\n        dir: The directory the files should exist in.\n    Warnings:\n        Raises a UserWarning when the path to any of the files in `fns` includes directory separators, as this may not work as intended.\n    \"\"\"\nfor fn in fns:\nif \"/\" in fn:\nwarnings.warn(\nf\"Using the path supplied appended to {dir}, i.e. attempting to read data from {dir / fn}\",\nUserWarning,\n)\n</code></pre>"},{"location":"reference/common/common/","title":"common","text":""},{"location":"reference/common/common/#nhssynth.common.common.set_seed","title":"<code>set_seed(seed=None)</code>","text":"<p>(Potentially) set the seed for numpy, torch and random.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>None | int</code> <p>The seed to set.</p> <code>None</code> Source code in <code>common/common.py</code> <pre><code>def set_seed(seed: None | int = None) -&gt; None:\n\"\"\"\n    (Potentially) set the seed for numpy, torch and random.\n    Args:\n        seed: The seed to set.\n    \"\"\"\nif seed:\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)\n</code></pre>"},{"location":"reference/common/constants/","title":"constants","text":"<p>Define all of the common constants used throughout the project.</p>"},{"location":"reference/common/dicts/","title":"dicts","text":"<p>Common functions for working with dictionaries.</p>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.filter_dict","title":"<code>filter_dict(d, filter_keys, include=False)</code>","text":"<p>Given a dictionary, return a new dictionary either including or excluding keys in a given <code>filter</code> set.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to filter.</p> required <code>filter_keys</code> <code>set | list</code> <p>A list or set of keys to either include or exclude.</p> required <code>include</code> <code>bool</code> <p>Determine whether to return a dictionary including or excluding keys in <code>filter</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A filtered dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n{'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n{'a': 1, 'b': 2}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def filter_dict(d: dict, filter_keys: set | list, include: bool = False) -&gt; dict:\n\"\"\"\n    Given a dictionary, return a new dictionary either including or excluding keys in a given `filter` set.\n    Args:\n        d: A dictionary to filter.\n        filter_keys: A list or set of keys to either include or exclude.\n        include: Determine whether to return a dictionary including or excluding keys in `filter`.\n    Returns:\n        A filtered dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n        {'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n        {'a': 1, 'b': 2}\n    \"\"\"\nif include:\nfiltered_keys = set(filter_keys) &amp; set(d.keys())\nelse:\nfiltered_keys = set(d.keys()) - set(filter_keys)\nreturn {k: v for k, v in d.items() if k in filtered_keys}\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary with possibly nested keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def flatten_dict(d: dict[str, Any]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n    Args:\n        d: A dictionary with possibly nested keys.\n    Returns:\n        A flattened dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\nitems = []\nfor k, v in d.items():\nif isinstance(v, dict):\nitems.extend(flatten_dict(v).items())\nelse:\nitems.append((k, v))\nreturn dict(items)\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def get_key_by_value(d: dict, value) -&gt; Any | None:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n    \"\"\"\nfor key, val in d.items():\nif val == value:\nreturn key\nreturn None\n</code></pre>"},{"location":"reference/common/io/","title":"io","text":"<p>Common building-block functions for handling module input and output.</p>"},{"location":"reference/common/io/#nhssynth.common.io.check_exists","title":"<code>check_exists(fns, dir)</code>","text":"<p>Checks if the files in <code>fns</code> exist in <code>dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir_experiment</code> <p>The directory the files should exist in.</p> required <p>Exceptions:</p> Type Description <code>FileNotFoundError</code> <p>If any of the files in <code>fns</code> do not exist in <code>dir_experiment</code>.</p> Source code in <code>common/io.py</code> <pre><code>def check_exists(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Checks if the files in `fns` exist in `dir`.\n    Args:\n        fns: The list of files to check.\n        dir_experiment: The directory the files should exist in.\n    Raises:\n        FileNotFoundError: If any of the files in `fns` do not exist in `dir_experiment`.\n    \"\"\"\nfor fn in fns:\nif not (dir / fn).exists():\nraise FileNotFoundError(f\"File {fn} does not exist at {dir}.\")\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.consistent_ending","title":"<code>consistent_ending(fn, ending='.pkl')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, removes any existing ending and appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".pkl\".</p> <code>'.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>common/io.py</code> <pre><code>def consistent_ending(fn: str, ending: str = \".pkl\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, removes any existing ending and appends `ending`.\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".pkl\".\n    Returns:\n        The filename with the correct ending.\n    \"\"\"\npath_fn = Path(fn)\nif path_fn.suffix == ending:\nreturn fn\nelse:\nreturn str(path_fn.parent / path_fn.stem) + ending\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.experiment_io","title":"<code>experiment_io(experiment_name, dir_experiments='experiments')</code>","text":"<p>Create an experiment's directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <code>dir_experiments</code> <code>str</code> <p>The name of the directory containing all experiments.</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the experiment directory.</p> Source code in <code>common/io.py</code> <pre><code>def experiment_io(experiment_name: str, dir_experiments: str = \"experiments\") -&gt; str:\n\"\"\"\n    Create an experiment's directory and return the path.\n    Args:\n        experiment_name: The name of the experiment.\n        dir_experiments: The name of the directory containing all experiments.\n    Returns:\n        The path to the experiment directory.\n    \"\"\"\ndir_experiment = Path(dir_experiments) / experiment_name\ndir_experiment.mkdir(parents=True, exist_ok=True)\nreturn dir_experiment\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.potential_suffix","title":"<code>potential_suffix(fn, fn_base)</code>","text":"<p>Checks if <code>fn</code> is a suffix (starts with an underscore) to append to <code>fn_base</code>, or a filename in its own right.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename / potential suffix to append to <code>fn_base</code>.</p> required <code>fn_base</code> <code>str</code> <p>The name of the file the suffix would attach to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The appropriately processed <code>fn</code></p> Source code in <code>common/io.py</code> <pre><code>def potential_suffix(fn: str, fn_base: str) -&gt; str:\n\"\"\"\n    Checks if `fn` is a suffix (starts with an underscore) to append to `fn_base`, or a filename in its own right.\n    Args:\n        fn: The filename / potential suffix to append to `fn_base`.\n        fn_base: The name of the file the suffix would attach to.\n    Returns:\n        The appropriately processed `fn`\n    \"\"\"\nfn_base = Path(fn_base).stem\nif fn[0] == \"_\":\nreturn fn_base + fn\nelse:\nreturn fn\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.warn_if_path_supplied","title":"<code>warn_if_path_supplied(fns, dir)</code>","text":"<p>Warns if the files in <code>fns</code> include directory separators.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir</code> <code>Path</code> <p>The directory the files should exist in.</p> required <p>!!! warnings     Raises a UserWarning when the path to any of the files in <code>fns</code> includes directory separators, as this may not work as intended.</p> Source code in <code>common/io.py</code> <pre><code>def warn_if_path_supplied(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Warns if the files in `fns` include directory separators.\n    Args:\n        fns: The list of files to check.\n        dir: The directory the files should exist in.\n    Warnings:\n        Raises a UserWarning when the path to any of the files in `fns` includes directory separators, as this may not work as intended.\n    \"\"\"\nfor fn in fns:\nif \"/\" in fn:\nwarnings.warn(\nf\"Using the path supplied appended to {dir}, i.e. attempting to read data from {dir / fn}\",\nUserWarning,\n)\n</code></pre>"},{"location":"reference/modules/","title":"modules","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader","title":"<code>dataloader</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.io","title":"<code>io</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_input, fn_metadata, dir_data)</code>","text":"<p>Formats the input filenames and directory for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_input</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_input</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_input_paths(\nfn_input: str,\nfn_metadata: str,\ndir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input filenames and directory for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_input`.\n        dir_data: The directory that should contain both of the above.\n    Returns:\n        A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).\n    Warnings:\n        Raises a UserWarning when the path to `fn_input` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_input, fn_metadata = consistent_endings([(fn_input, \".csv\"), (fn_metadata, \".yaml\")])\ndir_data = Path(dir_data)\nfn_metadata = potential_suffix(fn_metadata, fn_input)\nwarn_if_path_supplied([fn_input, fn_metadata], dir_data)\ncheck_exists([fn_input], dir_data)\nreturn dir_data, fn_input, fn_metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_input, fn_typed, fn_prepared, fn_transformer, dir_experiment)</code>","text":"<p>Formats the output filenames for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_typed</code> <code>str</code> <p>The typed input data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_prepared</code> <code>str</code> <p>The output data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_transformer</code> <code>str</code> <p>The transformer filename/suffix to append to <code>fn_input</code>.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the formatted output filenames.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_prepared</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_transformer</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_output_paths(\nfn_input: str,\nfn_typed: str,\nfn_prepared: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; tuple[str, str, str]:\n\"\"\"\n    Formats the output filenames for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_typed: The typed input data filename/suffix to append to `fn_input`.\n        fn_prepared: The output data filename/suffix to append to `fn_input`.\n        fn_transformer: The transformer filename/suffix to append to `fn_input`.\n        dir_experiment: The experiment directory to write the outputs to.\n    Returns:\n        A tuple containing the formatted output filenames.\n    Warnings:\n        Raises a UserWarning when the path to `fn_prepared` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_transformer` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_typed, fn_prepared, fn_transformer = consistent_endings([fn_typed, fn_prepared, fn_transformer])\nfn_typed, fn_prepared, fn_transformer = potential_suffixes([fn_typed, fn_prepared, fn_transformer], fn_input)\nwarn_if_path_supplied([fn_typed, fn_prepared, fn_transformer], dir_experiment)\nreturn fn_typed, fn_prepared, fn_transformer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(typed_dataset, prepared_dataset, metatransformer, fn_dataset, fn_metadata, dir_experiment, args)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>typed_dataset</code> <code>DataFrame</code> <p>The typed version of the input dataset.</p> required <code>prepared_dataset</code> <code>DataFrame</code> <p>The prepared version of the input dataset.</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its prepared state.</p> required <code>fn_dataset</code> <code>str</code> <p>The base dataset filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required Source code in <code>modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\ntyped_dataset: pd.DataFrame,\nprepared_dataset: pd.DataFrame,\nmetatransformer: MetaTransformer,\nfn_dataset: str,\nfn_metadata: str,\ndir_experiment: Path,\nargs: argparse.Namespace,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n    Args:\n        typed_dataset: The typed version of the input dataset.\n        prepared_dataset: The prepared version of the input dataset.\n        metatransformer: The metatransformer used to transform the data into its prepared state.\n        fn_dataset: The base dataset filename.\n        fn_metadata: The metadata filename.\n        dir_experiment: The experiment directory to write the outputs to.\n        args: The parsed command line arguments.\n    \"\"\"\nfn_typed, fn_prepared, fn_transformer = check_output_paths(\nfn_dataset, args.typed, args.prepared, args.metatransformer, dir_experiment\n)\noutput_metadata(dir_experiment / fn_metadata, metatransformer.get_assembled_metadata(), args.collapse_yaml)\ntyped_dataset.to_pickle(dir_experiment / fn_typed)\nprepared_dataset.to_pickle(dir_experiment / fn_prepared)\nprepared_dataset.to_csv(dir_experiment / (fn_prepared[:-3] + \"csv\"), index=False)\nwith open(dir_experiment / fn_transformer, \"wb\") as f:\npickle.dump(metatransformer, f)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata","title":"<code>metadata</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the <code>metadata</code> correspond to valid columns in the <code>data</code>. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the passed <code>data</code>.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns that are in metadata are not present in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict[str, Any]], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the `metadata` correspond to valid columns in the `data`.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n    Args:\n        metadata: A dictionary containing metadata for the columns in the passed `data`.\n        data: The DataFrame to check against the metadata.\n    Raises:\n        AssertionError: If any columns that *are* in metadata are *not* present in the `data`.\n    \"\"\"\nassert all([k in data.columns for k in metadata.keys()])\nmetadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # one entry for each column that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # one entry for each column that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\nc_index = 1\ncolumn_types = {}\nt_index = 1\ntransformers = {}\nfor cn, cd in metadata.items():\nif cd not in column_types.values():\ncolumn_types[c_index] = cd.copy()\nmetadata[cn] = column_types[c_index]\nc_index += 1\nelse:\ncix = get_key_by_value(column_types, cd)\nmetadata[cn] = column_types[cix]\nif cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\ntransformers[t_index] = cd[\"transformer\"].copy()\nmetadata[cn][\"transformer\"] = transformers[t_index]\nt_index += 1\nelif cd[\"transformer\"]:\ntix = get_key_by_value(transformers, cd[\"transformer\"])\nmetadata[cn][\"transformer\"] = transformers[tix]\nreturn {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame in question.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n    Args:\n        data: The DataFrame in question.\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\nreturn {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.get_sdtypes","title":"<code>get_sdtypes(metadata)</code>","text":"<p>Extracts the <code>sdtype</code> for each column from a valid assembled metadata dictionary and reformats them the correct format for use with SDMetrics.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to extract the <code>sdtype</code>s from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping column names to a dict containing <code>sdtype</code> value for that column.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def get_sdtypes(metadata: dict[str, dict[str, Any]]) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n    Extracts the `sdtype` for each column from a valid assembled metadata dictionary and reformats them the correct format for use with SDMetrics.\n    Args:\n        metadata: The metadata dictionary to extract the `sdtype`s from.\n    Returns:\n        A dictionary mapping column names to a dict containing `sdtype` value for that column.\n    \"\"\"\nreturn {\n\"columns\": {\ncn: {\n\"sdtype\": cd[\"sdtype\"],\n}\nfor cn, cd in metadata.items()\n}\n}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data`.\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n    Returns:\n        A metadata dictionary containing information about the columns in the `data`.\n    \"\"\"\nif in_path.exists():\nwith open(in_path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out expanded alias/anchor groups\nmetadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\ncheck_metadata_columns(metadata, data)\nelse:\nmetadata = create_empty_metadata(data)\nreturn metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, metadata, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be written.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\nout_path: pathlib.Path,\nmetadata: dict[str, dict[str, Any]],\ncollapse_yaml: bool,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        metadata: The metadata dictionary to be written.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\nif collapse_yaml:\nmetadata = collapse(metadata)\nwith open(out_path, \"w\") as yaml_file:\nyaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer","title":"<code>metatransformer</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code> MetaTransformer        </code>","text":"<p>A metatransformer object that can wrap a <code>BaseSingleTableSynthesizer</code> from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back to the original format of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>A dictionary mapping column names to their metadata.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> required <code>synthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use as the \"host\" for the MetaTransformer.</p> required <p>Once instantiated via <code>mt = MetaTransformer(&lt;parameters&gt;)</code>, the following attributes will be available:</p> <p>Attributes:</p> Name Type Description <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> <code>Synthesizer</code> <code>BaseSingleTableSynthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> host class.</p> <code>dtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).</p> <code>sdtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to the appropriate SDV-specific data type.</p> <code>transformers</code> <code>dict[str, BaseTransformer | None]</code> <p>A dictionary mapping each column to their assigned (if any) transformer.</p> <p>After preparing some data with the MetaTransformer, i.e. <code>prepared_data = mt.apply(data)</code>, the following attributes and methods will be available:</p> <p>Attributes:</p> Name Type Description <code>metatransformer</code> <code>self.Synthesizer</code> <p>An instanatiated <code>self.Synthesizer</code> object, ready to use on data.</p> <code>assembled_metadata</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the formatted and complete metadata for the MetaTransformer.</p> <code>onehots</code> <code>list[list[int]]</code> <p>The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).</p> <code>singles</code> <code>list[int]</code> <p>The indices of non-one-hotted columns.</p> <p>Methods:</p> <ul> <li><code>get_assembled_metadata()</code>: Returns the assembled metadata.</li> <li><code>get_sdtypes()</code>: Returns the sdtypes from the assembled metadata in the correct format for SDMetrics.</li> <li><code>get_onehots_and_singles()</code>: Returns the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</li> <li><code>inverse_apply(synthetic_data)</code>: Apply the inverse of the MetaTransformer to the given data.</li> </ul> <p>Note that <code>mt.apply</code> is a helper function that runs <code>mt.apply_dtypes</code>, <code>mt.instaniate</code>, <code>mt.assemble</code>, <code>mt.prepare</code> and finally <code>mt.count_onehots_and_singles</code> in sequence on a given raw dataset. Along the way it assigns the attributes listed above. This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    A metatransformer object that can wrap a [`BaseSingleTableSynthesizer`](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers)\n    from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming\n    the module's output back to the original format of the input data.\n    Args:\n        metadata: A dictionary mapping column names to their metadata.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        synthesizer: The `BaseSingleTableSynthesizer` class to use as the \"host\" for the MetaTransformer.\n    Once instantiated via `mt = MetaTransformer(&lt;parameters&gt;)`, the following attributes will be available:\n    Attributes:\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        Synthesizer: The `BaseSingleTableSynthesizer` host class.\n        dtypes: A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).\n        sdtypes: A dictionary mapping each column to the appropriate SDV-specific data type.\n        transformers: A dictionary mapping each column to their assigned (if any) transformer.\n    After preparing some data with the MetaTransformer, i.e. `prepared_data = mt.apply(data)`, the following attributes and methods will be available:\n    Attributes:\n        metatransformer (self.Synthesizer): An instanatiated `self.Synthesizer` object, ready to use on data.\n        assembled_metadata (dict[str, dict[str, Any]]): A dictionary containing the formatted and complete metadata for the MetaTransformer.\n        onehots (list[list[int]]): The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).\n        singles (list[int]): The indices of non-one-hotted columns.\n    **Methods:**\n    - `get_assembled_metadata()`: Returns the assembled metadata.\n    - `get_sdtypes()`: Returns the sdtypes from the assembled metadata in the correct format for SDMetrics.\n    - `get_onehots_and_singles()`: Returns the values of the MetaTransformer's `onehots` and `singles` attributes.\n    - `inverse_apply(synthetic_data)`: Apply the inverse of the MetaTransformer to the given data.\n    Note that `mt.apply` is a helper function that runs `mt.apply_dtypes`, `mt.instaniate`, `mt.assemble`, `mt.prepare` and finally\n    `mt.count_onehots_and_singles` in sequence on a given raw dataset. Along the way it assigns the attributes listed above. *This workflow is highly\n    encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.*\n    \"\"\"\ndef __init__(self, metadata, allow_null_transformers, synthesizer) -&gt; None:\nself.allow_null_transformers: bool = allow_null_transformers\nself.Synthesizer: BaseSingleTableSynthesizer = SDV_SYNTHESIZER_CHOICES[synthesizer]\nself.dtypes: dict[str, dict[str, Any]] = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\nself.sdtypes: dict[str, dict[str, Any]] = {\ncn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()\n}\nself.transformers: dict[str, BaseTransformer | None] = {cn: get_transformer(cd) for cn, cd in metadata.items()}\ndef apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n        Args:\n            data: The raw input DataFrame.\n        Returns:\n            The data with the dtypes applied.\n        \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\ndef _instantiate_ohe_component_transformers(\nself, transformers: dict[str, BaseTransformer | None]\n) -&gt; dict[str, BaseTransformer]:\n\"\"\"\n        Instantiates a OneHotEncoder for each resulting `*.component` column that arises from a ClusterBasedNormalizer.\n        Args:\n            transformers: A dictionary mapping column names to their assigned transformers.\n        Returns:\n            A dictionary mapping each `*.component` column to a OneHotEncoder.\n        \"\"\"\nreturn {\nf\"{cn}.component\": OneHotEncoder()\nfor cn, transformer in transformers.items()\nif transformer.get_name() == \"ClusterBasedNormalizer\"\n}\ndef instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n        Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete (and `self.allow_null_transformers` is `False`) in the case of missing transformer metadata.\n        \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\ndef _get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\nreturn self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\ndef assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in self.metatransformer.metadata.columns.items()\n}\ndef prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the data by processing it via the metatransformer.\n        Args:\n            data: The data to fit and apply the transformer to.\n        Returns:\n            The transformed data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nprepared_data = self.metatransformer.preprocess(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\ndef count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n        Also records the indices of non-one-hotted columns in a separate list.\n        Args:\n            data: The data to extract column indices from.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".value\").columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component.value\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelif cd[\"transformer\"].get(\"name\") != \"RegexGenerator\":\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\ndef apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n        Args:\n            data: The DataFrame to transform.\n        Returns:\n            The transformed data.\n        \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn typed_data, prepared_data\ndef get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Returns the assembled metadata for the transformer.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\ndef get_sdtypes(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n        Returns the sdtypes extracted from the assembled metadata for SDMetrics.\n        Returns:\n            A dictionary mapping column names to sdtypes.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn get_sdtypes(self.assembled_metadata)\ndef get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        Raises:\n            ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n        \"\"\"\nif not hasattr(self, \"onehots\") or not hasattr(self, \"singles\"):\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\ndef inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n        Args:\n            data: The transformed data.\n        Returns:\n            The original data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not hasattr(self, \"metatransformer\"):\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nreturn self.metatransformer._data_processor.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply(self, data)</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n    Args:\n        data: The DataFrame to transform.\n    Returns:\n        The transformed data.\n    \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn typed_data, prepared_data\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(self, data)</code>","text":"<p>Applies dtypes from the metadata to <code>data</code> and infers missing dtypes by reading pandas defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the dtypes applied.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n    Args:\n        data: The raw input DataFrame.\n    Returns:\n        The data with the dtypes applied.\n    \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.assemble","title":"<code>assemble(self)</code>","text":"<p>Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in self.metatransformer.metadata.columns.items()\n}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.count_onehots_and_singles","title":"<code>count_onehots_and_singles(self, data)</code>","text":"<p>Uses the assembled metadata to identify and record the indices of one-hotted column groups. Also records the indices of non-one-hotted columns in a separate list.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to extract column indices from.</p> required <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n    Also records the indices of non-one-hotted columns in a separate list.\n    Args:\n        data: The data to extract column indices from.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".value\").columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component.value\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelif cd[\"transformer\"].get(\"name\") != \"RegexGenerator\":\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_assembled_metadata","title":"<code>get_assembled_metadata(self)</code>","text":"<p>Returns the assembled metadata for the transformer.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Returns the assembled metadata for the transformer.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_onehots_and_singles","title":"<code>get_onehots_and_singles(self)</code>","text":"<p>Get the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</p> <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>self.onehots</code> and <code>self.singles</code> have yet to be counted.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    Raises:\n        ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n    \"\"\"\nif not hasattr(self, \"onehots\") or not hasattr(self, \"singles\"):\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_sdtypes","title":"<code>get_sdtypes(self)</code>","text":"<p>Returns the sdtypes extracted from the assembled metadata for SDMetrics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping column names to sdtypes.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_sdtypes(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n    Returns the sdtypes extracted from the assembled metadata for SDMetrics.\n    Returns:\n        A dictionary mapping column names to sdtypes.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn get_sdtypes(self.assembled_metadata)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate","title":"<code>instantiate(self, data)</code>","text":"<p>Instantiates a <code>self.Synthesizer</code> object from the given metadata and data. Infers missing metadata (sdtypes and transformers).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Type Description <code>BaseSingleTableSynthesizer</code> <p>A fully instantiated <code>self.Synthesizer</code> object and a transformer for the <code>*.component</code> columns.</p> <p>Exceptions:</p> Type Description <code>UserWarning</code> <p>If the metadata is incomplete (and <code>self.allow_null_transformers</code> is <code>False</code>) in the case of missing transformer metadata.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n    Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n    Args:\n        data: The input DataFrame.\n    Returns:\n        A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n    Raises:\n        UserWarning: If the metadata is incomplete (and `self.allow_null_transformers` is `False`) in the case of missing transformer metadata.\n    \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(self, data)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The transformed data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n    Args:\n        data: The transformed data.\n    Returns:\n        The original data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not hasattr(self, \"metatransformer\"):\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nreturn self.metatransformer._data_processor.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.prepare","title":"<code>prepare(self, data)</code>","text":"<p>Prepares the data by processing it via the metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to fit and apply the transformer to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the data by processing it via the metatransformer.\n    Args:\n        data: The data to fit and apply the transformer to.\n    Returns:\n        The transformed data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nprepared_data = self.metatransformer.preprocess(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object constructed from data in the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>rdt.transformers.base.BaseTransformer | None</code> <p>An instantiated <code>BaseTransformer</code> if the dictionary contains valid transformer data, else None.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_transformer(d: dict) -&gt; BaseTransformer | None:\n\"\"\"\n    Return a callable transformer object constructed from data in the given dictionary.\n    Args:\n        d: A dictionary containing the transformer data.\n    Returns:\n        An instantiated `BaseTransformer` if the dictionary contains valid transformer data, else None.\n    \"\"\"\ntransformer_data = d.get(\"transformer\", None)\nif isinstance(transformer_data, dict) and \"name\" in transformer_data:\n# Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\ntransformer_data = transformer_data.copy()\ntransformer_name = transformer_data.pop(\"name\")\nreturn eval(transformer_name)(**transformer_data)\nelif isinstance(transformer_data, str):\nreturn eval(transformer_data)()\nelse:\nreturn None\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.make_transformer_dict","title":"<code>make_transformer_dict(transformer)</code>","text":"<p>Deconstruct a <code>transformer</code> into a dictionary of config.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>BaseTransformer</code> <p>A BaseTransformer object from RDT (SDV).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the transformer's name and arguments.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def make_transformer_dict(transformer: BaseTransformer) -&gt; dict[str, Any]:\n\"\"\"\n    Deconstruct a `transformer` into a dictionary of config.\n    Args:\n        transformer: A BaseTransformer object from RDT (SDV).\n    Returns:\n        A dictionary containing the transformer's name and arguments.\n    \"\"\"\nreturn {\n\"name\": type(transformer).__name__,\n**filter_dict(\ntransformer.__dict__,\n{\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n),\n}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.run","title":"<code>run</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforms the dataset and writes the output and transformer used to disk.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforms the dataset and writes the output and transformer used to disk.\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n    \"\"\"\nprint(\"Running dataloader module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\ndir_input, fn_dataset, fn_metadata = check_input_paths(args.dataset, args.metadata, args.data_dir)\ndataset = pd.read_csv(dir_input / fn_dataset, index_col=args.index_col)\nmetadata = load_metadata(dir_input / fn_metadata, dataset)\nmt = MetaTransformer(metadata, args.allow_null_transformers, args.synthesizer)\ntyped_dataset, prepared_dataset = mt.apply(dataset)\nwrite_data_outputs(typed_dataset, prepared_dataset, mt, fn_dataset, fn_metadata, dir_experiment, args)\nif \"model\" in args.modules_to_run:\nargs.module_handover.update(\n{\n\"fn_dataset\": fn_dataset,\n\"prepared_dataset\": prepared_dataset,\n\"metatransformer\": mt,\n}\n)\nif \"evaluation\" in args.modules_to_run:\nargs.module_handover.update({\"typed_dataset\": typed_dataset, \"sdtypes\": mt.get_sdtypes()})\nprint(\"\")\nreturn args\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation","title":"<code>evaluation</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report","title":"<code>full_report</code>","text":"<p>Single table quality report.</p>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport","title":"<code> FullReport        </code>","text":"<p>Single table full report.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>class FullReport:\n\"\"\"Single table full report.\"\"\"\ndef __init__(self, metrics, metric_args={}):\nself._overall_quality_score = None\nself._metric_results = {}\nself._metric_averages = {}\nself._property_breakdown = {}\nself._property_errors = {}\nself._metrics = metrics\nself._metric_args = metric_args\ndef _get_metric_scores(self, metric_name):\n\"\"\"Aggregate the scores and errors in a metric results mapping.\n        Args:\n            The metric results to aggregate.\n        Returns:\n            The average of the metric scores, and the number of errors.\n        \"\"\"\nmetric_results = self._metric_results.get(metric_name, {})\nif len(metric_results) == 0:\nreturn np.nan\nmetric_scores = []\nfor breakdown in metric_results.values():\nif isinstance(breakdown, dict):\nmetric_score = breakdown.get(\"score\", np.nan)\nif not np.isnan(metric_score):\nmetric_scores.append(metric_score)\nelse:\nreturn [metric_results.get(\"score\", np.nan)]\nreturn metric_scores\ndef _print_results(self):\n\"\"\"Print the quality report results.\"\"\"\nif pd.isna(self._overall_quality_score) &amp; any(self._property_errors.values()):\nprint(\"\\nOverall Score: Error computing report.\")\nelse:\nprint(f\"\\nOverall Score: {round(self._overall_quality_score * 100, 2)}%\")\nif len(self._property_breakdown) &gt; 0:\nprint(\"\\nProperties:\")\nfor prop, score in self._property_breakdown.items():\nif not pd.isna(score):\nprint(f\"{prop}: {round(score * 100, 2)}%\")\nelif self._property_errors[prop] &gt; 0:\nprint(f\"{prop}: Error computing property.\")\nelse:\nprint(f\"{prop}: NaN\")\nprint(\"\")\ndef generate(self, real_data, synthetic_data, metadata, verbose=True):\n\"\"\"Generate report.\n        Args:\n            real_data (pandas.DataFrame):\n                The real data.\n            synthetic_data (pandas.DataFrame):\n                The synthetic data.\n            metadata (dict):\n                The metadata, which contains each column's data type as well as relationships.\n            verbose (bool):\n                Whether or not to print report summary and progress.\n        \"\"\"\nprint(\"\")\nvalidate_single_table_inputs(real_data, synthetic_data, metadata)\nself._property_breakdown = {}\nfor prop, metrics in tqdm(\nself._metrics.items(), desc=\"Creating report\", position=0, disable=(not verbose), leave=True\n):\nnum_prop_errors = 0\nif \"NewRowSynthesis\" in SDV_METRIC_CHOICES[prop]:\nif \"NewRowSynthesis\" not in self._metric_args:\nself._metric_args[\"NewRowSynthesis\"] = {}\nself._metric_args[\"NewRowSynthesis\"][\"synthetic_sample_size\"] = min(\nmin(len(real_data), len(synthetic_data)),\nself._metric_args[\"NewRowSynthesis\"].get(\"synthetic_sample_size\", len(real_data)),\n)\nfor metric in tqdm(metrics, desc=prop + \" metrics\", position=1, disable=(not verbose), leave=False):\nmetric_name = metric.__name__\ntry:\nmetric_args = self._metric_args.get(metric_name, {})\nmetric_results = metric.compute_breakdown(real_data, synthetic_data, metadata, **metric_args)\nif \"score\" in metric_results:\nmetric_average = metric_results[\"score\"]\nnum_prop_errors += metric_results.get(\"error\", 0)\nelse:\nmetric_average, num_metric_errors = aggregate_metric_results(metric_results)\nnum_prop_errors += num_metric_errors\nexcept IncomputableMetricError:\n# Metric is not compatible with this dataset.\nmetric_results = {}\nmetric_average = np.nan\nnum_prop_errors += 1\nself._metric_averages[metric_name] = metric_average\nself._metric_results[metric_name] = metric_results\nif (\nprop == \"Column Similarity\"\nand \"ContingencySimilarity\" in self._metric_results\nand \"CorrelationSimilarity\" in self._metric_results\n):\nexisting_column_pairs = list(self._metric_results[\"ContingencySimilarity\"].keys())\nexisting_column_pairs.extend(list(self._metric_results[\"CorrelationSimilarity\"].keys()))\nadditional_results = discretize_and_apply_metric(\nreal_data, synthetic_data, metadata, ContingencySimilarity, existing_column_pairs\n)\nself._metric_results[\"ContingencySimilarity\"].update(additional_results)\nself._metric_averages[\"ContingencySimilarity\"], _ = aggregate_metric_results(\nself._metric_results[\"ContingencySimilarity\"]\n)\nself._property_breakdown[prop] = np.mean([s for m in metrics for s in self._get_metric_scores(m.__name__)])\nself._property_errors[prop] = num_prop_errors\nself._overall_quality_score = np.nanmean(list(self._property_breakdown.values()))\nif verbose:\nself._print_results()\ndef get_score(self):\n\"\"\"Return the overall quality score.\n        Returns:\n            float\n                The overall quality score.\n        \"\"\"\nreturn self._overall_quality_score\ndef get_properties(self):\n\"\"\"Return the property score breakdown.\n        Returns:\n            pandas.DataFrame\n                The property score breakdown.\n        \"\"\"\nreturn pd.DataFrame(\n{\n\"Property\": self._property_breakdown.keys(),\n\"Score\": self._property_breakdown.values(),\n}\n)\ndef get_visualization(self, property_name):\n\"\"\"Return a visualization for each score for the given property name.\n        Args:\n            property_name (str):\n                The name of the property to return score details for.\n        Returns:\n            plotly.graph_objects._figure.Figure\n                The visualization for the requested property.\n        \"\"\"\nscore_breakdowns = {\nmetric.__name__: self._metric_results[metric.__name__] for metric in self._metrics.get(property_name, [])\n}\nif property_name == \"Column Shape\":\nfig = get_column_shapes_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Column Similarity\":\nfig = get_column_pairs_plot(\nscore_breakdowns,\nself._property_breakdown[property_name],\n)\nfig.show()\nelif property_name == \"Coverage\":\nfig = get_column_coverage_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Boundary\":\nfig = get_column_boundaries_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Synthesis\":\nfig = get_synthesis_plot(score_breakdowns.get(\"NewRowSynthesis\", {}))\nfig.show()\nelif property_name == \"Detection\":\nprint(\"WARNING: Detection plots not currently implemented.\")\nelif property_name == \"Divergence\":\nprint(\"WARNING: Divergence plots not currently implemented.\")\nelse:\nraise ValueError(f\"Property name `{property_name}` is not recognized / supported.\")\ndef get_details(self, property_name):\n\"\"\"Return the details for each score for the given property name.\n        Args:\n            property_name (str):\n                The name of the property to return score details for.\n        Returns:\n            pandas.DataFrame\n                The score breakdown.\n        \"\"\"\ncolumns = []\nmetrics = []\nscores = []\nerrors = []\ndetails = pd.DataFrame()\nif property_name == \"Detection\":\nfor metric in self._metrics[property_name]:\nmetric_results = self._metric_results[metric.__name__]\nif \"score\" in metric_results and pd.isna(metric_results[\"score\"]):\ncontinue\nmetrics.append(metric.__name__)\nscores.append(metric_results.get(\"score\", np.nan))\nerrors.append(metric_results.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Shape\":\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nif \"score\" in score_breakdown and pd.isna(score_breakdown[\"score\"]):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Similarity\" or property_name == \"Divergence\":\nreal_scores = []\nsynthetic_scores = []\nfor metric in self._metrics[property_name]:\nfor column_pair, score_breakdown in self._metric_results[metric.__name__].items():\ncolumns.append(column_pair)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nif property_name == \"Column Similarity\":\nreal_scores.append(score_breakdown.get(\"real\", np.nan))\nsynthetic_scores.append(score_breakdown.get(\"synthetic\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column 1\": [col1 for col1, _ in columns],\n\"Column 2\": [col2 for _, col2 in columns],\n\"Metric\": metrics,\n\"Overall Score\": scores,\n\"Real Correlation\": real_scores,\n\"Synthetic Correlation\": synthetic_scores,\n}\n)\nelif property_name == \"Synthesis\":\nmetric_name = self._metrics[property_name][0].__name__\nmetric_result = self._metric_results[metric_name]\ndetails = pd.DataFrame(\n{\n\"Metric\": [metric_name],\n\"Overall Score\": [metric_result.get(\"score\", np.nan)],\n\"Num Matched Rows\": [metric_result.get(\"num_matched_rows\", np.nan)],\n\"Num New Rows\": [metric_result.get(\"num_new_rows\", np.nan)],\n}\n)\nerrors.append(metric_result.get(\"error\", np.nan))\nelse:\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nmetric_score = score_breakdown.get(\"score\", np.nan)\nmetric_error = score_breakdown.get(\"error\", np.nan)\nif pd.isna(metric_score) and pd.isna(metric_error):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(metric_score)\nerrors.append(metric_error)\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nif pd.Series(errors).notna().sum() &gt; 0:\ndetails[\"Error\"] = errors\nreturn details\ndef get_raw_result(self, metric_name):\n\"\"\"Return the raw result of the given metric name.\n        Args:\n            metric_name (str):\n                The name of the desired metric.\n        Returns:\n            dict\n                The raw results\n        \"\"\"\nmetrics = list(itertools.chain.from_iterable(self._metrics.values()))\nfor metric in metrics:\nif metric.__name__ == metric_name:\nreturn [\n{\n\"metric\": {\n\"method\": f\"{metric.__module__}.{metric.__name__}\",\n\"parameters\": {},\n},\n\"results\": {\nkey: result\nfor key, result in self._metric_results[metric_name].items()\nif not pd.isna(result.get(\"score\", np.nan))\n},\n},\n]\ndef save(self, filepath):\n\"\"\"Save this report instance to the given path using pickle.\n        Args:\n            filepath (str):\n                The path to the file where the report instance will be serialized.\n        \"\"\"\nwith open(filepath, \"wb\") as output:\npickle.dump(self, output)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport.generate","title":"<code>generate(self, real_data, synthetic_data, metadata, verbose=True)</code>","text":"<p>Generate report.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>pandas.DataFrame</code> <p>The real data.</p> required <code>synthetic_data</code> <code>pandas.DataFrame</code> <p>The synthetic data.</p> required <code>metadata</code> <code>dict</code> <p>The metadata, which contains each column's data type as well as relationships.</p> required <code>verbose</code> <code>bool</code> <p>Whether or not to print report summary and progress.</p> <code>True</code> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def generate(self, real_data, synthetic_data, metadata, verbose=True):\n\"\"\"Generate report.\n    Args:\n        real_data (pandas.DataFrame):\n            The real data.\n        synthetic_data (pandas.DataFrame):\n            The synthetic data.\n        metadata (dict):\n            The metadata, which contains each column's data type as well as relationships.\n        verbose (bool):\n            Whether or not to print report summary and progress.\n    \"\"\"\nprint(\"\")\nvalidate_single_table_inputs(real_data, synthetic_data, metadata)\nself._property_breakdown = {}\nfor prop, metrics in tqdm(\nself._metrics.items(), desc=\"Creating report\", position=0, disable=(not verbose), leave=True\n):\nnum_prop_errors = 0\nif \"NewRowSynthesis\" in SDV_METRIC_CHOICES[prop]:\nif \"NewRowSynthesis\" not in self._metric_args:\nself._metric_args[\"NewRowSynthesis\"] = {}\nself._metric_args[\"NewRowSynthesis\"][\"synthetic_sample_size\"] = min(\nmin(len(real_data), len(synthetic_data)),\nself._metric_args[\"NewRowSynthesis\"].get(\"synthetic_sample_size\", len(real_data)),\n)\nfor metric in tqdm(metrics, desc=prop + \" metrics\", position=1, disable=(not verbose), leave=False):\nmetric_name = metric.__name__\ntry:\nmetric_args = self._metric_args.get(metric_name, {})\nmetric_results = metric.compute_breakdown(real_data, synthetic_data, metadata, **metric_args)\nif \"score\" in metric_results:\nmetric_average = metric_results[\"score\"]\nnum_prop_errors += metric_results.get(\"error\", 0)\nelse:\nmetric_average, num_metric_errors = aggregate_metric_results(metric_results)\nnum_prop_errors += num_metric_errors\nexcept IncomputableMetricError:\n# Metric is not compatible with this dataset.\nmetric_results = {}\nmetric_average = np.nan\nnum_prop_errors += 1\nself._metric_averages[metric_name] = metric_average\nself._metric_results[metric_name] = metric_results\nif (\nprop == \"Column Similarity\"\nand \"ContingencySimilarity\" in self._metric_results\nand \"CorrelationSimilarity\" in self._metric_results\n):\nexisting_column_pairs = list(self._metric_results[\"ContingencySimilarity\"].keys())\nexisting_column_pairs.extend(list(self._metric_results[\"CorrelationSimilarity\"].keys()))\nadditional_results = discretize_and_apply_metric(\nreal_data, synthetic_data, metadata, ContingencySimilarity, existing_column_pairs\n)\nself._metric_results[\"ContingencySimilarity\"].update(additional_results)\nself._metric_averages[\"ContingencySimilarity\"], _ = aggregate_metric_results(\nself._metric_results[\"ContingencySimilarity\"]\n)\nself._property_breakdown[prop] = np.mean([s for m in metrics for s in self._get_metric_scores(m.__name__)])\nself._property_errors[prop] = num_prop_errors\nself._overall_quality_score = np.nanmean(list(self._property_breakdown.values()))\nif verbose:\nself._print_results()\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport.get_details","title":"<code>get_details(self, property_name)</code>","text":"<p>Return the details for each score for the given property name.</p> <p>Parameters:</p> Name Type Description Default <code>property_name</code> <code>str</code> <p>The name of the property to return score details for.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame     The score breakdown.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_details(self, property_name):\n\"\"\"Return the details for each score for the given property name.\n    Args:\n        property_name (str):\n            The name of the property to return score details for.\n    Returns:\n        pandas.DataFrame\n            The score breakdown.\n    \"\"\"\ncolumns = []\nmetrics = []\nscores = []\nerrors = []\ndetails = pd.DataFrame()\nif property_name == \"Detection\":\nfor metric in self._metrics[property_name]:\nmetric_results = self._metric_results[metric.__name__]\nif \"score\" in metric_results and pd.isna(metric_results[\"score\"]):\ncontinue\nmetrics.append(metric.__name__)\nscores.append(metric_results.get(\"score\", np.nan))\nerrors.append(metric_results.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Shape\":\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nif \"score\" in score_breakdown and pd.isna(score_breakdown[\"score\"]):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Similarity\" or property_name == \"Divergence\":\nreal_scores = []\nsynthetic_scores = []\nfor metric in self._metrics[property_name]:\nfor column_pair, score_breakdown in self._metric_results[metric.__name__].items():\ncolumns.append(column_pair)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nif property_name == \"Column Similarity\":\nreal_scores.append(score_breakdown.get(\"real\", np.nan))\nsynthetic_scores.append(score_breakdown.get(\"synthetic\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column 1\": [col1 for col1, _ in columns],\n\"Column 2\": [col2 for _, col2 in columns],\n\"Metric\": metrics,\n\"Overall Score\": scores,\n\"Real Correlation\": real_scores,\n\"Synthetic Correlation\": synthetic_scores,\n}\n)\nelif property_name == \"Synthesis\":\nmetric_name = self._metrics[property_name][0].__name__\nmetric_result = self._metric_results[metric_name]\ndetails = pd.DataFrame(\n{\n\"Metric\": [metric_name],\n\"Overall Score\": [metric_result.get(\"score\", np.nan)],\n\"Num Matched Rows\": [metric_result.get(\"num_matched_rows\", np.nan)],\n\"Num New Rows\": [metric_result.get(\"num_new_rows\", np.nan)],\n}\n)\nerrors.append(metric_result.get(\"error\", np.nan))\nelse:\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nmetric_score = score_breakdown.get(\"score\", np.nan)\nmetric_error = score_breakdown.get(\"error\", np.nan)\nif pd.isna(metric_score) and pd.isna(metric_error):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(metric_score)\nerrors.append(metric_error)\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nif pd.Series(errors).notna().sum() &gt; 0:\ndetails[\"Error\"] = errors\nreturn details\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport.get_properties","title":"<code>get_properties(self)</code>","text":"<p>Return the property score breakdown.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame     The property score breakdown.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_properties(self):\n\"\"\"Return the property score breakdown.\n    Returns:\n        pandas.DataFrame\n            The property score breakdown.\n    \"\"\"\nreturn pd.DataFrame(\n{\n\"Property\": self._property_breakdown.keys(),\n\"Score\": self._property_breakdown.values(),\n}\n)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport.get_raw_result","title":"<code>get_raw_result(self, metric_name)</code>","text":"<p>Return the raw result of the given metric name.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the desired metric.</p> required <p>Returns:</p> Type Description <p>dict     The raw results</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_raw_result(self, metric_name):\n\"\"\"Return the raw result of the given metric name.\n    Args:\n        metric_name (str):\n            The name of the desired metric.\n    Returns:\n        dict\n            The raw results\n    \"\"\"\nmetrics = list(itertools.chain.from_iterable(self._metrics.values()))\nfor metric in metrics:\nif metric.__name__ == metric_name:\nreturn [\n{\n\"metric\": {\n\"method\": f\"{metric.__module__}.{metric.__name__}\",\n\"parameters\": {},\n},\n\"results\": {\nkey: result\nfor key, result in self._metric_results[metric_name].items()\nif not pd.isna(result.get(\"score\", np.nan))\n},\n},\n]\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport.get_score","title":"<code>get_score(self)</code>","text":"<p>Return the overall quality score.</p> <p>Returns:</p> Type Description <p>float     The overall quality score.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_score(self):\n\"\"\"Return the overall quality score.\n    Returns:\n        float\n            The overall quality score.\n    \"\"\"\nreturn self._overall_quality_score\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport.get_visualization","title":"<code>get_visualization(self, property_name)</code>","text":"<p>Return a visualization for each score for the given property name.</p> <p>Parameters:</p> Name Type Description Default <code>property_name</code> <code>str</code> <p>The name of the property to return score details for.</p> required <p>Returns:</p> Type Description <p>plotly.graph_objects._figure.Figure     The visualization for the requested property.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_visualization(self, property_name):\n\"\"\"Return a visualization for each score for the given property name.\n    Args:\n        property_name (str):\n            The name of the property to return score details for.\n    Returns:\n        plotly.graph_objects._figure.Figure\n            The visualization for the requested property.\n    \"\"\"\nscore_breakdowns = {\nmetric.__name__: self._metric_results[metric.__name__] for metric in self._metrics.get(property_name, [])\n}\nif property_name == \"Column Shape\":\nfig = get_column_shapes_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Column Similarity\":\nfig = get_column_pairs_plot(\nscore_breakdowns,\nself._property_breakdown[property_name],\n)\nfig.show()\nelif property_name == \"Coverage\":\nfig = get_column_coverage_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Boundary\":\nfig = get_column_boundaries_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Synthesis\":\nfig = get_synthesis_plot(score_breakdowns.get(\"NewRowSynthesis\", {}))\nfig.show()\nelif property_name == \"Detection\":\nprint(\"WARNING: Detection plots not currently implemented.\")\nelif property_name == \"Divergence\":\nprint(\"WARNING: Divergence plots not currently implemented.\")\nelse:\nraise ValueError(f\"Property name `{property_name}` is not recognized / supported.\")\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.full_report.FullReport.save","title":"<code>save(self, filepath)</code>","text":"<p>Save this report instance to the given path using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file where the report instance will be serialized.</p> required Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def save(self, filepath):\n\"\"\"Save this report instance to the given path using pickle.\n    Args:\n        filepath (str):\n            The path to the file where the report instance will be serialized.\n    \"\"\"\nwith open(filepath, \"wb\") as output:\npickle.dump(self, output)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.io","title":"<code>io</code>","text":""},{"location":"reference/modules/#nhssynth.modules.evaluation.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_synthetic, fn_metadata, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed data file.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>fn_metadata</code> <code>str</code> <p>The name of the metadata file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/evaluation/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_typed: str, fn_synthetic: str, fn_metadata: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed data file.\n        fn_synthetic: The name of the metatransformer file.\n        fn_metadata: The name of the metadata file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_typed, fn_synthetic, fn_metadata = consistent_endings(\n[fn_dataset, fn_typed, fn_synthetic, (fn_metadata, \".yaml\")]\n)\nfn_typed, fn_synthetic, fn_metadata = potential_suffixes([fn_typed, fn_synthetic, fn_metadata], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_typed, fn_synthetic, fn_metadata], dir_experiment)\ncheck_exists([fn_typed, fn_synthetic, fn_metadata], dir_experiment)\nreturn fn_dataset, fn_typed, fn_synthetic, fn_metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.evaluation.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/evaluation/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"typed_dataset\", \"synthetic\", \"sdtypes\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"typed_dataset\"],\nargs.module_handover[\"synthetic\"],\nargs.module_handover[\"sdtypes\"],\n)\nelse:\nfn_dataset, fn_typed, fn_synthetic, fn_metadata = check_input_paths(\nargs.dataset, args.typed, args.synthetic, args.metadata, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f)\nwith open(dir_experiment / fn_synthetic, \"rb\") as f:\nsynthetic_data = pickle.load(f)\nsdtypes = get_sdtypes(load_metadata(dir_experiment / fn_metadata, real_data))\nreturn fn_dataset, real_data, synthetic_data, sdtypes\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model","title":"<code>model</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.DPVAE","title":"<code>DPVAE</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\ndef __init__(\nself,\nlatent_dim,\nonehots=[[]],\nsingles=[],\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = len(singles) + sum([len(x) for x in onehots])\nself.singles = singles\nself.onehots = onehots\nself.net = nn.Sequential(\nnn.Linear(latent_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nlatent_dim,\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = 2 * latent_dim\nself.latent_dim = latent_dim\nself.net = nn.Sequential(\nnn.Linear(input_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\ndef __init__(self, num_singles):\nsuper().__init__()\nself.output_logsigma_fn = nn.Linear(num_singles, num_singles, bias=True)\ntorch.nn.init.zeros_(self.output_logsigma_fn.weight)\ntorch.nn.init.zeros_(self.output_logsigma_fn.bias)\nself.output_logsigma_fn.weight.requires_grad = False\ndef forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder.to(encoder.device)\nself.decoder = decoder.to(decoder.device)\nself.device = encoder.device\nself.onehots = self.decoder.onehots\nself.singles = self.decoder.singles\nself.noiser = Noiser(len(self.singles)).to(decoder.device)\ndef reconstruct(self, X):\nmu_z, logsigma_z = self.encoder(X)\nx_recon = self.decoder(mu_z)\nreturn x_recon\ndef generate(self, N):\nz_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\nx_gen = self.decoder(z_samples)\nx_gen_ = torch.ones_like(x_gen, device=self.device)\nfor cat_idxs in self.onehots:\nx_gen_[:, cat_idxs] = torch.distributions.one_hot_categorical.OneHotCategorical(\nlogits=x_gen[:, cat_idxs]\n).sample()\nx_gen_[:, self.singles] = x_gen[:, self.singles] + torch.exp(\nself.noiser(x_gen[:, self.singles])\n) * torch.randn_like(x_gen[:, self.singles])\nif torch.cuda.is_available():\nx_gen_ = x_gen_.cpu()\nreturn x_gen_.detach()\ndef loss(self, X):\nmu_z, logsigma_z = self.encoder(X)\np = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\nq = Normal(mu_z, torch.exp(logsigma_z))\nkld = torch.sum(torch.distributions.kl_divergence(q, p))\ns = torch.randn_like(mu_z)\nz_samples = mu_z + s * torch.exp(logsigma_z)\nx_recon = self.decoder(z_samples)\ncategoric_loglik = 0\nif len(self.onehots):\nfor cat_idxs in self.onehots:\ncategoric_loglik += -torch.nn.functional.cross_entropy(\nx_recon[:, cat_idxs],\ntorch.max(X[:, cat_idxs], 1)[1],\n).sum()\ngauss_loglik = 0\nif len(self.singles):\ngauss_loglik = (\nNormal(\nloc=x_recon[:, self.singles],\nscale=torch.exp(self.noiser(x_recon[:, self.singles])),\n)\n.log_prob(X[:, self.singles])\n.sum()\n)\nreconstruction_loss = -(categoric_loglik + gauss_loglik)\nelbo = kld + reconstruction_loss\nreturn {\n\"ELBO\": elbo,\n\"ReconstructionLoss\": reconstruction_loss,\n\"KLD\": kld,\n\"CategoricalLoss\": categoric_loglik,\n\"NumericalLoss\": gauss_loglik,\n}\ndef train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\ntracked_metrics: list[str] = [\"ELBO\"],\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nprint(\"\")\nself.start_time = time.time()\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\nelif \"Privacy\" in tracked_metrics:\ntracked_metrics.remove(\"Privacy\")\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nmetrics = {metric: [] for metric in tracked_metrics}\nstats_bars = {\nmetric: tqdm(total=0, desc=\"\", position=i, bar_format=\"{desc}\", leave=True)\nfor i, metric in enumerate(tracked_metrics)\n}\nmax_length = max(len(s) for s in tracked_metrics) + 1\nepoch_bar = tqdm(range(num_epochs), desc=\"Epochs\", position=len(stats_bars), leave=False)\nfor epoch in epoch_bar:\nfor key in metrics.keys():\nif key != \"Privacy\":\nmetrics[key].append(0.0)\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=len(stats_bars) + 1, leave=False):\nself.optimizer.zero_grad()\nlosses = self.loss(Y_subset.to(self.encoder.device))\nlosses[\"ELBO\"].backward()\nself.optimizer.step()\nfor key in metrics.keys():\nif key in losses:\nif losses[key]:\nmetrics[key][-1] += losses[key].item()\nelse:\nmetrics[key][-1] += 0.0\nfor key, stats_bar in stats_bars.items():\nif key == \"Privacy\" and privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar.set_description_str(\nf\"{(key + ':').ljust(max_length)}  \\u03B5 = {epsilon_e[0]:.2f}\\tbest \\u03B1 = {epsilon_e[1]:.2f}\"\n)\nmetrics[\"Privacy\"].append(epsilon_e)\nelse:\nstats_bar.set_description_str(f\"{(key + ':').ljust(max_length)}  {metrics[key][-1]:.2f}\")\nif epoch == 0:\nmin_elbo = metrics[\"ELBO\"][-1]\nif metrics[\"ELBO\"][-1] &lt; (min_elbo - delta):\nmin_elbo = metrics[\"ELBO\"][-1]\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nfor stats_bar in stats_bars.values():\nstats_bar.close()\ntqdm.write(f\"Completed {num_epochs} epochs in {time.time() - self.start_time:.2f} seconds.\")\nreturn (num_epochs, metrics)\ndef get_privacy_spent(self, delta):\nif hasattr(self, \"privacy_engine\"):\nreturn self.privacy_engine.get_privacy_spent(delta)\nelse:\nprint(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n)\ndef save(self, filename):\ntorch.save(self.state_dict(), filename)\ndef load(self, filename):\nself.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, num_epochs, tracked_metrics=['ELBO'], privacy_engine=None, patience=5, delta=10)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\ntracked_metrics: list[str] = [\"ELBO\"],\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nprint(\"\")\nself.start_time = time.time()\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\nelif \"Privacy\" in tracked_metrics:\ntracked_metrics.remove(\"Privacy\")\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nmetrics = {metric: [] for metric in tracked_metrics}\nstats_bars = {\nmetric: tqdm(total=0, desc=\"\", position=i, bar_format=\"{desc}\", leave=True)\nfor i, metric in enumerate(tracked_metrics)\n}\nmax_length = max(len(s) for s in tracked_metrics) + 1\nepoch_bar = tqdm(range(num_epochs), desc=\"Epochs\", position=len(stats_bars), leave=False)\nfor epoch in epoch_bar:\nfor key in metrics.keys():\nif key != \"Privacy\":\nmetrics[key].append(0.0)\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=len(stats_bars) + 1, leave=False):\nself.optimizer.zero_grad()\nlosses = self.loss(Y_subset.to(self.encoder.device))\nlosses[\"ELBO\"].backward()\nself.optimizer.step()\nfor key in metrics.keys():\nif key in losses:\nif losses[key]:\nmetrics[key][-1] += losses[key].item()\nelse:\nmetrics[key][-1] += 0.0\nfor key, stats_bar in stats_bars.items():\nif key == \"Privacy\" and privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar.set_description_str(\nf\"{(key + ':').ljust(max_length)}  \\u03B5 = {epsilon_e[0]:.2f}\\tbest \\u03B1 = {epsilon_e[1]:.2f}\"\n)\nmetrics[\"Privacy\"].append(epsilon_e)\nelse:\nstats_bar.set_description_str(f\"{(key + ':').ljust(max_length)}  {metrics[key][-1]:.2f}\")\nif epoch == 0:\nmin_elbo = metrics[\"ELBO\"][-1]\nif metrics[\"ELBO\"][-1] &lt; (min_elbo - delta):\nmin_elbo = metrics[\"ELBO\"][-1]\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nfor stats_bar in stats_bars.values():\nstats_bar.close()\ntqdm.write(f\"Completed {num_epochs} epochs in {time.time() - self.start_time:.2f} seconds.\")\nreturn (num_epochs, metrics)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.io","title":"<code>io</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_prepared, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_prepared</code> <code>str</code> <p>The name of the prepared data file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_prepared: str, fn_metatransformer: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_prepared: The name of the prepared data file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_prepared, fn_metatransformer = consistent_endings([fn_dataset, fn_prepared, fn_metatransformer])\nfn_prepared, fn_metatransformer = potential_suffixes([fn_prepared, fn_metatransformer], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_prepared, fn_metatransformer], dir_experiment)\ncheck_exists([fn_prepared, fn_metatransformer], dir_experiment)\nreturn fn_dataset, fn_prepared, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.io.check_output_paths","title":"<code>check_output_paths(fn_dataset, fn_synthetic, fn_model, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>Path</code> <p>The base name of the dataset.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the synthetic data file.</p> required <code>fn_model</code> <code>str</code> <p>The name of the model file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The path to output the model.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_output_paths(fn_dataset: Path, fn_synthetic: str, fn_model: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_synthetic: The name of the synthetic data file.\n        fn_model: The name of the model file.\n        dir_experiment: The path to the experiment output directory.\n    Returns:\n        The path to output the model.\n    \"\"\"\nfn_synthetic, fn_model = consistent_endings([fn_synthetic, (fn_model, \".pt\")])\nfn_synthetic, fn_model = potential_suffixes([fn_synthetic, fn_model], fn_dataset)\nwarn_if_path_supplied([fn_synthetic, fn_model], dir_experiment)\nreturn fn_synthetic, fn_model\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/model/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"prepared_dataset\", \"metatransformer\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"prepared_dataset\"],\nargs.module_handover[\"metatransformer\"],\n)\nelse:\nfn_dataset, fn_prepared, fn_metatransformer = check_input_paths(\nargs.dataset, args.prepared, args.metatransformer, dir_experiment\n)\nwith open(dir_experiment / fn_prepared, \"rb\") as f:\ndata = pickle.load(f)\nwith open(dir_experiment / fn_metatransformer, \"rb\") as f:\nmt = pickle.load(f)\nreturn fn_dataset, data, mt\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.run","title":"<code>run</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.run.run","title":"<code>run(args)</code>","text":"<p>Run the model architecture module.</p> Source code in <code>modules/model/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"Run the model architecture module.\"\"\"\nprint(\"Running model architecture module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\nfn_dataset, prepared_dataset, mt = load_required_data(args, dir_experiment)\nonehots, singles = mt.get_onehots_and_singles()\nnrows, ncols = prepared_dataset.shape\n# Should the data also all be turned into floats?\ntorch_data = TensorDataset(torch.Tensor(prepared_dataset.to_numpy()))\nsample_rate = args.batch_size / nrows\nmodel = VAE(\nEncoder(input_dim=ncols, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim, use_gpu=args.use_gpu),\nDecoder(args.latent_dim, onehots=onehots, singles=singles, use_gpu=args.use_gpu),\n)\nmodel.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\ndata_loader = DataLoader(\ntorch_data,\nbatch_sampler=UniformWithReplacementSampler(num_samples=nrows, sample_rate=sample_rate),\npin_memory=True,\n# batch_size=args.batch_size,\n)\nif not args.non_private_training:\nprivacy_engine = PrivacyEngine(\n# secure_rng=args.secure_rng,\nmodule=model,\nsample_rate=sample_rate,\nalphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\ntarget_epsilon=args.target_epsilon,\ntarget_delta=args.target_delta,\nepochs=args.num_epochs,\nmax_grad_norm=args.max_grad_norm,\n)\n# model.privacy_engine = PrivacyEngine(secure_mode=args.secure_rng)\n# model, optimizer, data_loader = model.privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=data_loader,\n#     epochs=args.num_epochs,\n#     target_epsilon=args.target_epsilon,\n#     target_delta=args.target_delta,\n#     max_grad_norm=args.max_grad_norm,\n# )\n# print(model)\n# print(f\"Using sigma={optimizer.noise_multiplier} and C={args.max_grad_norm}\")\nnum_epochs, results = model.train(\ndata_loader, args.num_epochs, args.tracked_metrics, privacy_engine=privacy_engine\n)\nelse:\nnum_epochs, results = model.train(data_loader, args.num_epochs, args.tracked_metrics)\nsynthetic = pd.DataFrame(model.generate(nrows), columns=prepared_dataset.columns)\nsynthetic = mt.inverse_apply(synthetic)\nfn_output, fn_model = check_output_paths(fn_dataset, args.synthetic, args.model_file, dir_experiment)\nsynthetic.to_pickle(dir_experiment / fn_output)\nsynthetic.to_csv(dir_experiment / (fn_output[:-3] + \"csv\"), index=False)\nmodel.save(dir_experiment / fn_model)\nif \"evaluation\" in args.modules_to_run:\nargs.module_handover.update({\"fn_dataset\": fn_dataset, \"synthetic\": synthetic})\nif \"plotting\" in args.modules_to_run:\nargs.module_handover.update({\"results\": results, \"num_epochs\": num_epochs})\nprint(\"\")\nreturn args\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.plotting","title":"<code>plotting</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.plotting.io","title":"<code>io</code>","text":""},{"location":"reference/modules/#nhssynth.modules.plotting.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_synthetic, fn_report, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed data file.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>fn_report</code> <code>str</code> <p>The name of the report file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/plotting/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_typed: str, fn_synthetic: str, fn_report: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed data file.\n        fn_synthetic: The name of the metatransformer file.\n        fn_report: The name of the report file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_typed, fn_synthetic, fn_report = consistent_endings([fn_dataset, fn_typed, fn_synthetic, fn_report])\nfn_typed, fn_synthetic, fn_report = potential_suffixes([fn_typed, fn_synthetic, fn_report], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_typed, fn_synthetic, fn_report], dir_experiment)\ncheck_exists([fn_typed, fn_synthetic], dir_experiment)\nreturn fn_dataset, fn_typed, fn_synthetic, fn_report\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.plotting.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/plotting/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"typed_dataset\", \"synthetic\", \"report\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"typed_dataset\"],\nargs.module_handover[\"synthetic\"],\nargs.module_handover[\"report\"],\n)\nelse:\nfn_dataset, fn_typed, fn_synthetic, fn_report = check_input_paths(\nargs.dataset, args.typed, args.synthetic, args.report, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f)\nwith open(dir_experiment / fn_synthetic, \"rb\") as f:\nsynthetic_data = pickle.load(f)\nif (dir_experiment / fn_report).exists():\nwith open(dir_experiment / fn_report, \"rb\") as f:\nreport = pickle.load(f)\nelse:\nreport = None\nreturn fn_dataset, real_data, synthetic_data, report\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.plotting.plots","title":"<code>plots</code>","text":""},{"location":"reference/modules/#nhssynth.modules.plotting.plots.factorize_all_categoricals","title":"<code>factorize_all_categoricals(df)</code>","text":"<p>Factorize all categorical columns in a dataframe.</p> Source code in <code>modules/plotting/plots.py</code> <pre><code>def factorize_all_categoricals(\ndf: pd.DataFrame,\n) -&gt; pd.DataFrame:\n\"\"\"Factorize all categorical columns in a dataframe.\"\"\"\nfor col in df.columns:\nif df[col].dtype == \"object\":\ndf[col] = pd.factorize(df[col])[0]\nelif df[col].dtype == \"datetime64[ns]\":\ndf[col] = pd.to_numeric(df[col])\nmin_val = df[col].min()\nmax_val = df[col].max()\ndf[col] = (df[col] - min_val) / (max_val - min_val)\nreturn df\n</code></pre>"},{"location":"reference/modules/dataloader/","title":"dataloader","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io","title":"<code>io</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_input, fn_metadata, dir_data)</code>","text":"<p>Formats the input filenames and directory for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_input</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_input</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_input_paths(\nfn_input: str,\nfn_metadata: str,\ndir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input filenames and directory for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_input`.\n        dir_data: The directory that should contain both of the above.\n    Returns:\n        A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).\n    Warnings:\n        Raises a UserWarning when the path to `fn_input` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_input, fn_metadata = consistent_endings([(fn_input, \".csv\"), (fn_metadata, \".yaml\")])\ndir_data = Path(dir_data)\nfn_metadata = potential_suffix(fn_metadata, fn_input)\nwarn_if_path_supplied([fn_input, fn_metadata], dir_data)\ncheck_exists([fn_input], dir_data)\nreturn dir_data, fn_input, fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_input, fn_typed, fn_prepared, fn_transformer, dir_experiment)</code>","text":"<p>Formats the output filenames for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_typed</code> <code>str</code> <p>The typed input data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_prepared</code> <code>str</code> <p>The output data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_transformer</code> <code>str</code> <p>The transformer filename/suffix to append to <code>fn_input</code>.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the formatted output filenames.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_prepared</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_transformer</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_output_paths(\nfn_input: str,\nfn_typed: str,\nfn_prepared: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; tuple[str, str, str]:\n\"\"\"\n    Formats the output filenames for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_typed: The typed input data filename/suffix to append to `fn_input`.\n        fn_prepared: The output data filename/suffix to append to `fn_input`.\n        fn_transformer: The transformer filename/suffix to append to `fn_input`.\n        dir_experiment: The experiment directory to write the outputs to.\n    Returns:\n        A tuple containing the formatted output filenames.\n    Warnings:\n        Raises a UserWarning when the path to `fn_prepared` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_transformer` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_typed, fn_prepared, fn_transformer = consistent_endings([fn_typed, fn_prepared, fn_transformer])\nfn_typed, fn_prepared, fn_transformer = potential_suffixes([fn_typed, fn_prepared, fn_transformer], fn_input)\nwarn_if_path_supplied([fn_typed, fn_prepared, fn_transformer], dir_experiment)\nreturn fn_typed, fn_prepared, fn_transformer\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(typed_dataset, prepared_dataset, metatransformer, fn_dataset, fn_metadata, dir_experiment, args)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>typed_dataset</code> <code>DataFrame</code> <p>The typed version of the input dataset.</p> required <code>prepared_dataset</code> <code>DataFrame</code> <p>The prepared version of the input dataset.</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its prepared state.</p> required <code>fn_dataset</code> <code>str</code> <p>The base dataset filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required Source code in <code>modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\ntyped_dataset: pd.DataFrame,\nprepared_dataset: pd.DataFrame,\nmetatransformer: MetaTransformer,\nfn_dataset: str,\nfn_metadata: str,\ndir_experiment: Path,\nargs: argparse.Namespace,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n    Args:\n        typed_dataset: The typed version of the input dataset.\n        prepared_dataset: The prepared version of the input dataset.\n        metatransformer: The metatransformer used to transform the data into its prepared state.\n        fn_dataset: The base dataset filename.\n        fn_metadata: The metadata filename.\n        dir_experiment: The experiment directory to write the outputs to.\n        args: The parsed command line arguments.\n    \"\"\"\nfn_typed, fn_prepared, fn_transformer = check_output_paths(\nfn_dataset, args.typed, args.prepared, args.metatransformer, dir_experiment\n)\noutput_metadata(dir_experiment / fn_metadata, metatransformer.get_assembled_metadata(), args.collapse_yaml)\ntyped_dataset.to_pickle(dir_experiment / fn_typed)\nprepared_dataset.to_pickle(dir_experiment / fn_prepared)\nprepared_dataset.to_csv(dir_experiment / (fn_prepared[:-3] + \"csv\"), index=False)\nwith open(dir_experiment / fn_transformer, \"wb\") as f:\npickle.dump(metatransformer, f)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata","title":"<code>metadata</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the <code>metadata</code> correspond to valid columns in the <code>data</code>. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the passed <code>data</code>.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns that are in metadata are not present in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict[str, Any]], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the `metadata` correspond to valid columns in the `data`.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n    Args:\n        metadata: A dictionary containing metadata for the columns in the passed `data`.\n        data: The DataFrame to check against the metadata.\n    Raises:\n        AssertionError: If any columns that *are* in metadata are *not* present in the `data`.\n    \"\"\"\nassert all([k in data.columns for k in metadata.keys()])\nmetadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # one entry for each column that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # one entry for each column that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\nc_index = 1\ncolumn_types = {}\nt_index = 1\ntransformers = {}\nfor cn, cd in metadata.items():\nif cd not in column_types.values():\ncolumn_types[c_index] = cd.copy()\nmetadata[cn] = column_types[c_index]\nc_index += 1\nelse:\ncix = get_key_by_value(column_types, cd)\nmetadata[cn] = column_types[cix]\nif cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\ntransformers[t_index] = cd[\"transformer\"].copy()\nmetadata[cn][\"transformer\"] = transformers[t_index]\nt_index += 1\nelif cd[\"transformer\"]:\ntix = get_key_by_value(transformers, cd[\"transformer\"])\nmetadata[cn][\"transformer\"] = transformers[tix]\nreturn {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame in question.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n    Args:\n        data: The DataFrame in question.\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\nreturn {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.get_sdtypes","title":"<code>get_sdtypes(metadata)</code>","text":"<p>Extracts the <code>sdtype</code> for each column from a valid assembled metadata dictionary and reformats them the correct format for use with SDMetrics.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to extract the <code>sdtype</code>s from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping column names to a dict containing <code>sdtype</code> value for that column.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def get_sdtypes(metadata: dict[str, dict[str, Any]]) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n    Extracts the `sdtype` for each column from a valid assembled metadata dictionary and reformats them the correct format for use with SDMetrics.\n    Args:\n        metadata: The metadata dictionary to extract the `sdtype`s from.\n    Returns:\n        A dictionary mapping column names to a dict containing `sdtype` value for that column.\n    \"\"\"\nreturn {\n\"columns\": {\ncn: {\n\"sdtype\": cd[\"sdtype\"],\n}\nfor cn, cd in metadata.items()\n}\n}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data`.\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n    Returns:\n        A metadata dictionary containing information about the columns in the `data`.\n    \"\"\"\nif in_path.exists():\nwith open(in_path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out expanded alias/anchor groups\nmetadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\ncheck_metadata_columns(metadata, data)\nelse:\nmetadata = create_empty_metadata(data)\nreturn metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, metadata, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be written.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\nout_path: pathlib.Path,\nmetadata: dict[str, dict[str, Any]],\ncollapse_yaml: bool,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        metadata: The metadata dictionary to be written.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\nif collapse_yaml:\nmetadata = collapse(metadata)\nwith open(out_path, \"w\") as yaml_file:\nyaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer","title":"<code>metatransformer</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code> MetaTransformer        </code>","text":"<p>A metatransformer object that can wrap a <code>BaseSingleTableSynthesizer</code> from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back to the original format of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>A dictionary mapping column names to their metadata.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> required <code>synthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use as the \"host\" for the MetaTransformer.</p> required <p>Once instantiated via <code>mt = MetaTransformer(&lt;parameters&gt;)</code>, the following attributes will be available:</p> <p>Attributes:</p> Name Type Description <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> <code>Synthesizer</code> <code>BaseSingleTableSynthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> host class.</p> <code>dtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).</p> <code>sdtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to the appropriate SDV-specific data type.</p> <code>transformers</code> <code>dict[str, BaseTransformer | None]</code> <p>A dictionary mapping each column to their assigned (if any) transformer.</p> <p>After preparing some data with the MetaTransformer, i.e. <code>prepared_data = mt.apply(data)</code>, the following attributes and methods will be available:</p> <p>Attributes:</p> Name Type Description <code>metatransformer</code> <code>self.Synthesizer</code> <p>An instanatiated <code>self.Synthesizer</code> object, ready to use on data.</p> <code>assembled_metadata</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the formatted and complete metadata for the MetaTransformer.</p> <code>onehots</code> <code>list[list[int]]</code> <p>The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).</p> <code>singles</code> <code>list[int]</code> <p>The indices of non-one-hotted columns.</p> <p>Methods:</p> <ul> <li><code>get_assembled_metadata()</code>: Returns the assembled metadata.</li> <li><code>get_sdtypes()</code>: Returns the sdtypes from the assembled metadata in the correct format for SDMetrics.</li> <li><code>get_onehots_and_singles()</code>: Returns the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</li> <li><code>inverse_apply(synthetic_data)</code>: Apply the inverse of the MetaTransformer to the given data.</li> </ul> <p>Note that <code>mt.apply</code> is a helper function that runs <code>mt.apply_dtypes</code>, <code>mt.instaniate</code>, <code>mt.assemble</code>, <code>mt.prepare</code> and finally <code>mt.count_onehots_and_singles</code> in sequence on a given raw dataset. Along the way it assigns the attributes listed above. This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    A metatransformer object that can wrap a [`BaseSingleTableSynthesizer`](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers)\n    from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming\n    the module's output back to the original format of the input data.\n    Args:\n        metadata: A dictionary mapping column names to their metadata.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        synthesizer: The `BaseSingleTableSynthesizer` class to use as the \"host\" for the MetaTransformer.\n    Once instantiated via `mt = MetaTransformer(&lt;parameters&gt;)`, the following attributes will be available:\n    Attributes:\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        Synthesizer: The `BaseSingleTableSynthesizer` host class.\n        dtypes: A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).\n        sdtypes: A dictionary mapping each column to the appropriate SDV-specific data type.\n        transformers: A dictionary mapping each column to their assigned (if any) transformer.\n    After preparing some data with the MetaTransformer, i.e. `prepared_data = mt.apply(data)`, the following attributes and methods will be available:\n    Attributes:\n        metatransformer (self.Synthesizer): An instanatiated `self.Synthesizer` object, ready to use on data.\n        assembled_metadata (dict[str, dict[str, Any]]): A dictionary containing the formatted and complete metadata for the MetaTransformer.\n        onehots (list[list[int]]): The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).\n        singles (list[int]): The indices of non-one-hotted columns.\n    **Methods:**\n    - `get_assembled_metadata()`: Returns the assembled metadata.\n    - `get_sdtypes()`: Returns the sdtypes from the assembled metadata in the correct format for SDMetrics.\n    - `get_onehots_and_singles()`: Returns the values of the MetaTransformer's `onehots` and `singles` attributes.\n    - `inverse_apply(synthetic_data)`: Apply the inverse of the MetaTransformer to the given data.\n    Note that `mt.apply` is a helper function that runs `mt.apply_dtypes`, `mt.instaniate`, `mt.assemble`, `mt.prepare` and finally\n    `mt.count_onehots_and_singles` in sequence on a given raw dataset. Along the way it assigns the attributes listed above. *This workflow is highly\n    encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.*\n    \"\"\"\ndef __init__(self, metadata, allow_null_transformers, synthesizer) -&gt; None:\nself.allow_null_transformers: bool = allow_null_transformers\nself.Synthesizer: BaseSingleTableSynthesizer = SDV_SYNTHESIZER_CHOICES[synthesizer]\nself.dtypes: dict[str, dict[str, Any]] = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\nself.sdtypes: dict[str, dict[str, Any]] = {\ncn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()\n}\nself.transformers: dict[str, BaseTransformer | None] = {cn: get_transformer(cd) for cn, cd in metadata.items()}\ndef apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n        Args:\n            data: The raw input DataFrame.\n        Returns:\n            The data with the dtypes applied.\n        \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\ndef _instantiate_ohe_component_transformers(\nself, transformers: dict[str, BaseTransformer | None]\n) -&gt; dict[str, BaseTransformer]:\n\"\"\"\n        Instantiates a OneHotEncoder for each resulting `*.component` column that arises from a ClusterBasedNormalizer.\n        Args:\n            transformers: A dictionary mapping column names to their assigned transformers.\n        Returns:\n            A dictionary mapping each `*.component` column to a OneHotEncoder.\n        \"\"\"\nreturn {\nf\"{cn}.component\": OneHotEncoder()\nfor cn, transformer in transformers.items()\nif transformer.get_name() == \"ClusterBasedNormalizer\"\n}\ndef instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n        Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete (and `self.allow_null_transformers` is `False`) in the case of missing transformer metadata.\n        \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\ndef _get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\nreturn self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\ndef assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in self.metatransformer.metadata.columns.items()\n}\ndef prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the data by processing it via the metatransformer.\n        Args:\n            data: The data to fit and apply the transformer to.\n        Returns:\n            The transformed data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nprepared_data = self.metatransformer.preprocess(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\ndef count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n        Also records the indices of non-one-hotted columns in a separate list.\n        Args:\n            data: The data to extract column indices from.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".value\").columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component.value\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelif cd[\"transformer\"].get(\"name\") != \"RegexGenerator\":\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\ndef apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n        Args:\n            data: The DataFrame to transform.\n        Returns:\n            The transformed data.\n        \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn typed_data, prepared_data\ndef get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Returns the assembled metadata for the transformer.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\ndef get_sdtypes(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n        Returns the sdtypes extracted from the assembled metadata for SDMetrics.\n        Returns:\n            A dictionary mapping column names to sdtypes.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn get_sdtypes(self.assembled_metadata)\ndef get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        Raises:\n            ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n        \"\"\"\nif not hasattr(self, \"onehots\") or not hasattr(self, \"singles\"):\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\ndef inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n        Args:\n            data: The transformed data.\n        Returns:\n            The original data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not hasattr(self, \"metatransformer\"):\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nreturn self.metatransformer._data_processor.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply(self, data)</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n    Args:\n        data: The DataFrame to transform.\n    Returns:\n        The transformed data.\n    \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn typed_data, prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(self, data)</code>","text":"<p>Applies dtypes from the metadata to <code>data</code> and infers missing dtypes by reading pandas defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the dtypes applied.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n    Args:\n        data: The raw input DataFrame.\n    Returns:\n        The data with the dtypes applied.\n    \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.assemble","title":"<code>assemble(self)</code>","text":"<p>Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in self.metatransformer.metadata.columns.items()\n}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.count_onehots_and_singles","title":"<code>count_onehots_and_singles(self, data)</code>","text":"<p>Uses the assembled metadata to identify and record the indices of one-hotted column groups. Also records the indices of non-one-hotted columns in a separate list.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to extract column indices from.</p> required <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n    Also records the indices of non-one-hotted columns in a separate list.\n    Args:\n        data: The data to extract column indices from.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".value\").columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component.value\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelif cd[\"transformer\"].get(\"name\") != \"RegexGenerator\":\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_assembled_metadata","title":"<code>get_assembled_metadata(self)</code>","text":"<p>Returns the assembled metadata for the transformer.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Returns the assembled metadata for the transformer.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_onehots_and_singles","title":"<code>get_onehots_and_singles(self)</code>","text":"<p>Get the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</p> <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>self.onehots</code> and <code>self.singles</code> have yet to be counted.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    Raises:\n        ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n    \"\"\"\nif not hasattr(self, \"onehots\") or not hasattr(self, \"singles\"):\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_sdtypes","title":"<code>get_sdtypes(self)</code>","text":"<p>Returns the sdtypes extracted from the assembled metadata for SDMetrics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping column names to sdtypes.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_sdtypes(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n    Returns the sdtypes extracted from the assembled metadata for SDMetrics.\n    Returns:\n        A dictionary mapping column names to sdtypes.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn get_sdtypes(self.assembled_metadata)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate","title":"<code>instantiate(self, data)</code>","text":"<p>Instantiates a <code>self.Synthesizer</code> object from the given metadata and data. Infers missing metadata (sdtypes and transformers).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Type Description <code>BaseSingleTableSynthesizer</code> <p>A fully instantiated <code>self.Synthesizer</code> object and a transformer for the <code>*.component</code> columns.</p> <p>Exceptions:</p> Type Description <code>UserWarning</code> <p>If the metadata is incomplete (and <code>self.allow_null_transformers</code> is <code>False</code>) in the case of missing transformer metadata.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n    Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n    Args:\n        data: The input DataFrame.\n    Returns:\n        A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n    Raises:\n        UserWarning: If the metadata is incomplete (and `self.allow_null_transformers` is `False`) in the case of missing transformer metadata.\n    \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(self, data)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The transformed data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n    Args:\n        data: The transformed data.\n    Returns:\n        The original data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not hasattr(self, \"metatransformer\"):\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nreturn self.metatransformer._data_processor.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.prepare","title":"<code>prepare(self, data)</code>","text":"<p>Prepares the data by processing it via the metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to fit and apply the transformer to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the data by processing it via the metatransformer.\n    Args:\n        data: The data to fit and apply the transformer to.\n    Returns:\n        The transformed data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nprepared_data = self.metatransformer.preprocess(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object constructed from data in the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>rdt.transformers.base.BaseTransformer | None</code> <p>An instantiated <code>BaseTransformer</code> if the dictionary contains valid transformer data, else None.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_transformer(d: dict) -&gt; BaseTransformer | None:\n\"\"\"\n    Return a callable transformer object constructed from data in the given dictionary.\n    Args:\n        d: A dictionary containing the transformer data.\n    Returns:\n        An instantiated `BaseTransformer` if the dictionary contains valid transformer data, else None.\n    \"\"\"\ntransformer_data = d.get(\"transformer\", None)\nif isinstance(transformer_data, dict) and \"name\" in transformer_data:\n# Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\ntransformer_data = transformer_data.copy()\ntransformer_name = transformer_data.pop(\"name\")\nreturn eval(transformer_name)(**transformer_data)\nelif isinstance(transformer_data, str):\nreturn eval(transformer_data)()\nelse:\nreturn None\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.make_transformer_dict","title":"<code>make_transformer_dict(transformer)</code>","text":"<p>Deconstruct a <code>transformer</code> into a dictionary of config.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>BaseTransformer</code> <p>A BaseTransformer object from RDT (SDV).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the transformer's name and arguments.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def make_transformer_dict(transformer: BaseTransformer) -&gt; dict[str, Any]:\n\"\"\"\n    Deconstruct a `transformer` into a dictionary of config.\n    Args:\n        transformer: A BaseTransformer object from RDT (SDV).\n    Returns:\n        A dictionary containing the transformer's name and arguments.\n    \"\"\"\nreturn {\n\"name\": type(transformer).__name__,\n**filter_dict(\ntransformer.__dict__,\n{\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n),\n}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.run","title":"<code>run</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforms the dataset and writes the output and transformer used to disk.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforms the dataset and writes the output and transformer used to disk.\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n    \"\"\"\nprint(\"Running dataloader module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\ndir_input, fn_dataset, fn_metadata = check_input_paths(args.dataset, args.metadata, args.data_dir)\ndataset = pd.read_csv(dir_input / fn_dataset, index_col=args.index_col)\nmetadata = load_metadata(dir_input / fn_metadata, dataset)\nmt = MetaTransformer(metadata, args.allow_null_transformers, args.synthesizer)\ntyped_dataset, prepared_dataset = mt.apply(dataset)\nwrite_data_outputs(typed_dataset, prepared_dataset, mt, fn_dataset, fn_metadata, dir_experiment, args)\nif \"model\" in args.modules_to_run:\nargs.module_handover.update(\n{\n\"fn_dataset\": fn_dataset,\n\"prepared_dataset\": prepared_dataset,\n\"metatransformer\": mt,\n}\n)\nif \"evaluation\" in args.modules_to_run:\nargs.module_handover.update({\"typed_dataset\": typed_dataset, \"sdtypes\": mt.get_sdtypes()})\nprint(\"\")\nreturn args\n</code></pre>"},{"location":"reference/modules/dataloader/io/","title":"io","text":""},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_input, fn_metadata, dir_data)</code>","text":"<p>Formats the input filenames and directory for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_input</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_input</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_input_paths(\nfn_input: str,\nfn_metadata: str,\ndir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input filenames and directory for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_input`.\n        dir_data: The directory that should contain both of the above.\n    Returns:\n        A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).\n    Warnings:\n        Raises a UserWarning when the path to `fn_input` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_input, fn_metadata = consistent_endings([(fn_input, \".csv\"), (fn_metadata, \".yaml\")])\ndir_data = Path(dir_data)\nfn_metadata = potential_suffix(fn_metadata, fn_input)\nwarn_if_path_supplied([fn_input, fn_metadata], dir_data)\ncheck_exists([fn_input], dir_data)\nreturn dir_data, fn_input, fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_input, fn_typed, fn_prepared, fn_transformer, dir_experiment)</code>","text":"<p>Formats the output filenames for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_typed</code> <code>str</code> <p>The typed input data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_prepared</code> <code>str</code> <p>The output data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_transformer</code> <code>str</code> <p>The transformer filename/suffix to append to <code>fn_input</code>.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the formatted output filenames.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_prepared</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_transformer</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_output_paths(\nfn_input: str,\nfn_typed: str,\nfn_prepared: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; tuple[str, str, str]:\n\"\"\"\n    Formats the output filenames for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_typed: The typed input data filename/suffix to append to `fn_input`.\n        fn_prepared: The output data filename/suffix to append to `fn_input`.\n        fn_transformer: The transformer filename/suffix to append to `fn_input`.\n        dir_experiment: The experiment directory to write the outputs to.\n    Returns:\n        A tuple containing the formatted output filenames.\n    Warnings:\n        Raises a UserWarning when the path to `fn_prepared` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_transformer` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_typed, fn_prepared, fn_transformer = consistent_endings([fn_typed, fn_prepared, fn_transformer])\nfn_typed, fn_prepared, fn_transformer = potential_suffixes([fn_typed, fn_prepared, fn_transformer], fn_input)\nwarn_if_path_supplied([fn_typed, fn_prepared, fn_transformer], dir_experiment)\nreturn fn_typed, fn_prepared, fn_transformer\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(typed_dataset, prepared_dataset, metatransformer, fn_dataset, fn_metadata, dir_experiment, args)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>typed_dataset</code> <code>DataFrame</code> <p>The typed version of the input dataset.</p> required <code>prepared_dataset</code> <code>DataFrame</code> <p>The prepared version of the input dataset.</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its prepared state.</p> required <code>fn_dataset</code> <code>str</code> <p>The base dataset filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <code>args</code> <code>Namespace</code> <p>The parsed command line arguments.</p> required Source code in <code>modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\ntyped_dataset: pd.DataFrame,\nprepared_dataset: pd.DataFrame,\nmetatransformer: MetaTransformer,\nfn_dataset: str,\nfn_metadata: str,\ndir_experiment: Path,\nargs: argparse.Namespace,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n    Args:\n        typed_dataset: The typed version of the input dataset.\n        prepared_dataset: The prepared version of the input dataset.\n        metatransformer: The metatransformer used to transform the data into its prepared state.\n        fn_dataset: The base dataset filename.\n        fn_metadata: The metadata filename.\n        dir_experiment: The experiment directory to write the outputs to.\n        args: The parsed command line arguments.\n    \"\"\"\nfn_typed, fn_prepared, fn_transformer = check_output_paths(\nfn_dataset, args.typed, args.prepared, args.metatransformer, dir_experiment\n)\noutput_metadata(dir_experiment / fn_metadata, metatransformer.get_assembled_metadata(), args.collapse_yaml)\ntyped_dataset.to_pickle(dir_experiment / fn_typed)\nprepared_dataset.to_pickle(dir_experiment / fn_prepared)\nprepared_dataset.to_csv(dir_experiment / (fn_prepared[:-3] + \"csv\"), index=False)\nwith open(dir_experiment / fn_transformer, \"wb\") as f:\npickle.dump(metatransformer, f)\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/","title":"metadata","text":""},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the <code>metadata</code> correspond to valid columns in the <code>data</code>. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the passed <code>data</code>.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns that are in metadata are not present in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict[str, Any]], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the `metadata` correspond to valid columns in the `data`.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n    Args:\n        metadata: A dictionary containing metadata for the columns in the passed `data`.\n        data: The DataFrame to check against the metadata.\n    Raises:\n        AssertionError: If any columns that *are* in metadata are *not* present in the `data`.\n    \"\"\"\nassert all([k in data.columns for k in metadata.keys()])\nmetadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # one entry for each column that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # one entry for each column that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\nc_index = 1\ncolumn_types = {}\nt_index = 1\ntransformers = {}\nfor cn, cd in metadata.items():\nif cd not in column_types.values():\ncolumn_types[c_index] = cd.copy()\nmetadata[cn] = column_types[c_index]\nc_index += 1\nelse:\ncix = get_key_by_value(column_types, cd)\nmetadata[cn] = column_types[cix]\nif cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\ntransformers[t_index] = cd[\"transformer\"].copy()\nmetadata[cn][\"transformer\"] = transformers[t_index]\nt_index += 1\nelif cd[\"transformer\"]:\ntix = get_key_by_value(transformers, cd[\"transformer\"])\nmetadata[cn][\"transformer\"] = transformers[tix]\nreturn {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame in question.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n    Args:\n        data: The DataFrame in question.\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\nreturn {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.get_sdtypes","title":"<code>get_sdtypes(metadata)</code>","text":"<p>Extracts the <code>sdtype</code> for each column from a valid assembled metadata dictionary and reformats them the correct format for use with SDMetrics.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to extract the <code>sdtype</code>s from.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping column names to a dict containing <code>sdtype</code> value for that column.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def get_sdtypes(metadata: dict[str, dict[str, Any]]) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n    Extracts the `sdtype` for each column from a valid assembled metadata dictionary and reformats them the correct format for use with SDMetrics.\n    Args:\n        metadata: The metadata dictionary to extract the `sdtype`s from.\n    Returns:\n        A dictionary mapping column names to a dict containing `sdtype` value for that column.\n    \"\"\"\nreturn {\n\"columns\": {\ncn: {\n\"sdtype\": cd[\"sdtype\"],\n}\nfor cn, cd in metadata.items()\n}\n}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data`.\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n    Returns:\n        A metadata dictionary containing information about the columns in the `data`.\n    \"\"\"\nif in_path.exists():\nwith open(in_path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out expanded alias/anchor groups\nmetadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\ncheck_metadata_columns(metadata, data)\nelse:\nmetadata = create_empty_metadata(data)\nreturn metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, metadata, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be written.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\nout_path: pathlib.Path,\nmetadata: dict[str, dict[str, Any]],\ncollapse_yaml: bool,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        metadata: The metadata dictionary to be written.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\nif collapse_yaml:\nmetadata = collapse(metadata)\nwith open(out_path, \"w\") as yaml_file:\nyaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/","title":"metatransformer","text":""},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code> MetaTransformer        </code>","text":"<p>A metatransformer object that can wrap a <code>BaseSingleTableSynthesizer</code> from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back to the original format of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>A dictionary mapping column names to their metadata.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> required <code>synthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use as the \"host\" for the MetaTransformer.</p> required <p>Once instantiated via <code>mt = MetaTransformer(&lt;parameters&gt;)</code>, the following attributes will be available:</p> <p>Attributes:</p> Name Type Description <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> <code>Synthesizer</code> <code>BaseSingleTableSynthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> host class.</p> <code>dtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).</p> <code>sdtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to the appropriate SDV-specific data type.</p> <code>transformers</code> <code>dict[str, BaseTransformer | None]</code> <p>A dictionary mapping each column to their assigned (if any) transformer.</p> <p>After preparing some data with the MetaTransformer, i.e. <code>prepared_data = mt.apply(data)</code>, the following attributes and methods will be available:</p> <p>Attributes:</p> Name Type Description <code>metatransformer</code> <code>self.Synthesizer</code> <p>An instanatiated <code>self.Synthesizer</code> object, ready to use on data.</p> <code>assembled_metadata</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the formatted and complete metadata for the MetaTransformer.</p> <code>onehots</code> <code>list[list[int]]</code> <p>The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).</p> <code>singles</code> <code>list[int]</code> <p>The indices of non-one-hotted columns.</p> <p>Methods:</p> <ul> <li><code>get_assembled_metadata()</code>: Returns the assembled metadata.</li> <li><code>get_sdtypes()</code>: Returns the sdtypes from the assembled metadata in the correct format for SDMetrics.</li> <li><code>get_onehots_and_singles()</code>: Returns the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</li> <li><code>inverse_apply(synthetic_data)</code>: Apply the inverse of the MetaTransformer to the given data.</li> </ul> <p>Note that <code>mt.apply</code> is a helper function that runs <code>mt.apply_dtypes</code>, <code>mt.instaniate</code>, <code>mt.assemble</code>, <code>mt.prepare</code> and finally <code>mt.count_onehots_and_singles</code> in sequence on a given raw dataset. Along the way it assigns the attributes listed above. This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    A metatransformer object that can wrap a [`BaseSingleTableSynthesizer`](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers)\n    from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming\n    the module's output back to the original format of the input data.\n    Args:\n        metadata: A dictionary mapping column names to their metadata.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        synthesizer: The `BaseSingleTableSynthesizer` class to use as the \"host\" for the MetaTransformer.\n    Once instantiated via `mt = MetaTransformer(&lt;parameters&gt;)`, the following attributes will be available:\n    Attributes:\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        Synthesizer: The `BaseSingleTableSynthesizer` host class.\n        dtypes: A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).\n        sdtypes: A dictionary mapping each column to the appropriate SDV-specific data type.\n        transformers: A dictionary mapping each column to their assigned (if any) transformer.\n    After preparing some data with the MetaTransformer, i.e. `prepared_data = mt.apply(data)`, the following attributes and methods will be available:\n    Attributes:\n        metatransformer (self.Synthesizer): An instanatiated `self.Synthesizer` object, ready to use on data.\n        assembled_metadata (dict[str, dict[str, Any]]): A dictionary containing the formatted and complete metadata for the MetaTransformer.\n        onehots (list[list[int]]): The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).\n        singles (list[int]): The indices of non-one-hotted columns.\n    **Methods:**\n    - `get_assembled_metadata()`: Returns the assembled metadata.\n    - `get_sdtypes()`: Returns the sdtypes from the assembled metadata in the correct format for SDMetrics.\n    - `get_onehots_and_singles()`: Returns the values of the MetaTransformer's `onehots` and `singles` attributes.\n    - `inverse_apply(synthetic_data)`: Apply the inverse of the MetaTransformer to the given data.\n    Note that `mt.apply` is a helper function that runs `mt.apply_dtypes`, `mt.instaniate`, `mt.assemble`, `mt.prepare` and finally\n    `mt.count_onehots_and_singles` in sequence on a given raw dataset. Along the way it assigns the attributes listed above. *This workflow is highly\n    encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.*\n    \"\"\"\ndef __init__(self, metadata, allow_null_transformers, synthesizer) -&gt; None:\nself.allow_null_transformers: bool = allow_null_transformers\nself.Synthesizer: BaseSingleTableSynthesizer = SDV_SYNTHESIZER_CHOICES[synthesizer]\nself.dtypes: dict[str, dict[str, Any]] = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\nself.sdtypes: dict[str, dict[str, Any]] = {\ncn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()\n}\nself.transformers: dict[str, BaseTransformer | None] = {cn: get_transformer(cd) for cn, cd in metadata.items()}\ndef apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n        Args:\n            data: The raw input DataFrame.\n        Returns:\n            The data with the dtypes applied.\n        \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\ndef _instantiate_ohe_component_transformers(\nself, transformers: dict[str, BaseTransformer | None]\n) -&gt; dict[str, BaseTransformer]:\n\"\"\"\n        Instantiates a OneHotEncoder for each resulting `*.component` column that arises from a ClusterBasedNormalizer.\n        Args:\n            transformers: A dictionary mapping column names to their assigned transformers.\n        Returns:\n            A dictionary mapping each `*.component` column to a OneHotEncoder.\n        \"\"\"\nreturn {\nf\"{cn}.component\": OneHotEncoder()\nfor cn, transformer in transformers.items()\nif transformer.get_name() == \"ClusterBasedNormalizer\"\n}\ndef instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n        Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete (and `self.allow_null_transformers` is `False`) in the case of missing transformer metadata.\n        \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\ndef _get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\nreturn self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\ndef assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in self.metatransformer.metadata.columns.items()\n}\ndef prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the data by processing it via the metatransformer.\n        Args:\n            data: The data to fit and apply the transformer to.\n        Returns:\n            The transformed data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nprepared_data = self.metatransformer.preprocess(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\ndef count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n        Also records the indices of non-one-hotted columns in a separate list.\n        Args:\n            data: The data to extract column indices from.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".value\").columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component.value\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelif cd[\"transformer\"].get(\"name\") != \"RegexGenerator\":\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\ndef apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n        Args:\n            data: The DataFrame to transform.\n        Returns:\n            The transformed data.\n        \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn typed_data, prepared_data\ndef get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Returns the assembled metadata for the transformer.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\ndef get_sdtypes(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n        Returns the sdtypes extracted from the assembled metadata for SDMetrics.\n        Returns:\n            A dictionary mapping column names to sdtypes.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn get_sdtypes(self.assembled_metadata)\ndef get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        Raises:\n            ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n        \"\"\"\nif not hasattr(self, \"onehots\") or not hasattr(self, \"singles\"):\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\ndef inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n        Args:\n            data: The transformed data.\n        Returns:\n            The original data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not hasattr(self, \"metatransformer\"):\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nreturn self.metatransformer._data_processor.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply(self, data)</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n    Args:\n        data: The DataFrame to transform.\n    Returns:\n        The transformed data.\n    \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn typed_data, prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(self, data)</code>","text":"<p>Applies dtypes from the metadata to <code>data</code> and infers missing dtypes by reading pandas defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the dtypes applied.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n    Args:\n        data: The raw input DataFrame.\n    Returns:\n        The data with the dtypes applied.\n    \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.assemble","title":"<code>assemble(self)</code>","text":"<p>Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Rearranges the dtype, sdtype and transformer metadata into a consistent format ready for output.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in self.metatransformer.metadata.columns.items()\n}\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.count_onehots_and_singles","title":"<code>count_onehots_and_singles(self, data)</code>","text":"<p>Uses the assembled metadata to identify and record the indices of one-hotted column groups. Also records the indices of non-one-hotted columns in a separate list.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to extract column indices from.</p> required <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n    Also records the indices of non-one-hotted columns in a separate list.\n    Args:\n        data: The data to extract column indices from.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".value\").columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component.value\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelif cd[\"transformer\"].get(\"name\") != \"RegexGenerator\":\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_assembled_metadata","title":"<code>get_assembled_metadata(self)</code>","text":"<p>Returns the assembled metadata for the transformer.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Returns the assembled metadata for the transformer.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_onehots_and_singles","title":"<code>get_onehots_and_singles(self)</code>","text":"<p>Get the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</p> <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>self.onehots</code> and <code>self.singles</code> have yet to be counted.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    Raises:\n        ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n    \"\"\"\nif not hasattr(self, \"onehots\") or not hasattr(self, \"singles\"):\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_sdtypes","title":"<code>get_sdtypes(self)</code>","text":"<p>Returns the sdtypes extracted from the assembled metadata for SDMetrics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping column names to sdtypes.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_sdtypes(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n    Returns the sdtypes extracted from the assembled metadata for SDMetrics.\n    Returns:\n        A dictionary mapping column names to sdtypes.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not hasattr(self, \"assembled_metadata\"):\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn get_sdtypes(self.assembled_metadata)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate","title":"<code>instantiate(self, data)</code>","text":"<p>Instantiates a <code>self.Synthesizer</code> object from the given metadata and data. Infers missing metadata (sdtypes and transformers).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Type Description <code>BaseSingleTableSynthesizer</code> <p>A fully instantiated <code>self.Synthesizer</code> object and a transformer for the <code>*.component</code> columns.</p> <p>Exceptions:</p> Type Description <code>UserWarning</code> <p>If the metadata is incomplete (and <code>self.allow_null_transformers</code> is <code>False</code>) in the case of missing transformer metadata.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n    Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n    Args:\n        data: The input DataFrame.\n    Returns:\n        A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n    Raises:\n        UserWarning: If the metadata is incomplete (and `self.allow_null_transformers` is `False`) in the case of missing transformer metadata.\n    \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(self, data)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The transformed data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n    Args:\n        data: The transformed data.\n    Returns:\n        The original data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not hasattr(self, \"metatransformer\"):\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nreturn self.metatransformer._data_processor.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.prepare","title":"<code>prepare(self, data)</code>","text":"<p>Prepares the data by processing it via the metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to fit and apply the transformer to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the data by processing it via the metatransformer.\n    Args:\n        data: The data to fit and apply the transformer to.\n    Returns:\n        The transformed data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nprepared_data = self.metatransformer.preprocess(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object constructed from data in the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>rdt.transformers.base.BaseTransformer | None</code> <p>An instantiated <code>BaseTransformer</code> if the dictionary contains valid transformer data, else None.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_transformer(d: dict) -&gt; BaseTransformer | None:\n\"\"\"\n    Return a callable transformer object constructed from data in the given dictionary.\n    Args:\n        d: A dictionary containing the transformer data.\n    Returns:\n        An instantiated `BaseTransformer` if the dictionary contains valid transformer data, else None.\n    \"\"\"\ntransformer_data = d.get(\"transformer\", None)\nif isinstance(transformer_data, dict) and \"name\" in transformer_data:\n# Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\ntransformer_data = transformer_data.copy()\ntransformer_name = transformer_data.pop(\"name\")\nreturn eval(transformer_name)(**transformer_data)\nelif isinstance(transformer_data, str):\nreturn eval(transformer_data)()\nelse:\nreturn None\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.make_transformer_dict","title":"<code>make_transformer_dict(transformer)</code>","text":"<p>Deconstruct a <code>transformer</code> into a dictionary of config.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>BaseTransformer</code> <p>A BaseTransformer object from RDT (SDV).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the transformer's name and arguments.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def make_transformer_dict(transformer: BaseTransformer) -&gt; dict[str, Any]:\n\"\"\"\n    Deconstruct a `transformer` into a dictionary of config.\n    Args:\n        transformer: A BaseTransformer object from RDT (SDV).\n    Returns:\n        A dictionary containing the transformer's name and arguments.\n    \"\"\"\nreturn {\n\"name\": type(transformer).__name__,\n**filter_dict(\ntransformer.__dict__,\n{\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n),\n}\n</code></pre>"},{"location":"reference/modules/dataloader/run/","title":"run","text":""},{"location":"reference/modules/dataloader/run/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforms the dataset and writes the output and transformer used to disk.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforms the dataset and writes the output and transformer used to disk.\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n    \"\"\"\nprint(\"Running dataloader module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\ndir_input, fn_dataset, fn_metadata = check_input_paths(args.dataset, args.metadata, args.data_dir)\ndataset = pd.read_csv(dir_input / fn_dataset, index_col=args.index_col)\nmetadata = load_metadata(dir_input / fn_metadata, dataset)\nmt = MetaTransformer(metadata, args.allow_null_transformers, args.synthesizer)\ntyped_dataset, prepared_dataset = mt.apply(dataset)\nwrite_data_outputs(typed_dataset, prepared_dataset, mt, fn_dataset, fn_metadata, dir_experiment, args)\nif \"model\" in args.modules_to_run:\nargs.module_handover.update(\n{\n\"fn_dataset\": fn_dataset,\n\"prepared_dataset\": prepared_dataset,\n\"metatransformer\": mt,\n}\n)\nif \"evaluation\" in args.modules_to_run:\nargs.module_handover.update({\"typed_dataset\": typed_dataset, \"sdtypes\": mt.get_sdtypes()})\nprint(\"\")\nreturn args\n</code></pre>"},{"location":"reference/modules/evaluation/","title":"evaluation","text":""},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report","title":"<code>full_report</code>","text":"<p>Single table quality report.</p>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport","title":"<code> FullReport        </code>","text":"<p>Single table full report.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>class FullReport:\n\"\"\"Single table full report.\"\"\"\ndef __init__(self, metrics, metric_args={}):\nself._overall_quality_score = None\nself._metric_results = {}\nself._metric_averages = {}\nself._property_breakdown = {}\nself._property_errors = {}\nself._metrics = metrics\nself._metric_args = metric_args\ndef _get_metric_scores(self, metric_name):\n\"\"\"Aggregate the scores and errors in a metric results mapping.\n        Args:\n            The metric results to aggregate.\n        Returns:\n            The average of the metric scores, and the number of errors.\n        \"\"\"\nmetric_results = self._metric_results.get(metric_name, {})\nif len(metric_results) == 0:\nreturn np.nan\nmetric_scores = []\nfor breakdown in metric_results.values():\nif isinstance(breakdown, dict):\nmetric_score = breakdown.get(\"score\", np.nan)\nif not np.isnan(metric_score):\nmetric_scores.append(metric_score)\nelse:\nreturn [metric_results.get(\"score\", np.nan)]\nreturn metric_scores\ndef _print_results(self):\n\"\"\"Print the quality report results.\"\"\"\nif pd.isna(self._overall_quality_score) &amp; any(self._property_errors.values()):\nprint(\"\\nOverall Score: Error computing report.\")\nelse:\nprint(f\"\\nOverall Score: {round(self._overall_quality_score * 100, 2)}%\")\nif len(self._property_breakdown) &gt; 0:\nprint(\"\\nProperties:\")\nfor prop, score in self._property_breakdown.items():\nif not pd.isna(score):\nprint(f\"{prop}: {round(score * 100, 2)}%\")\nelif self._property_errors[prop] &gt; 0:\nprint(f\"{prop}: Error computing property.\")\nelse:\nprint(f\"{prop}: NaN\")\nprint(\"\")\ndef generate(self, real_data, synthetic_data, metadata, verbose=True):\n\"\"\"Generate report.\n        Args:\n            real_data (pandas.DataFrame):\n                The real data.\n            synthetic_data (pandas.DataFrame):\n                The synthetic data.\n            metadata (dict):\n                The metadata, which contains each column's data type as well as relationships.\n            verbose (bool):\n                Whether or not to print report summary and progress.\n        \"\"\"\nprint(\"\")\nvalidate_single_table_inputs(real_data, synthetic_data, metadata)\nself._property_breakdown = {}\nfor prop, metrics in tqdm(\nself._metrics.items(), desc=\"Creating report\", position=0, disable=(not verbose), leave=True\n):\nnum_prop_errors = 0\nif \"NewRowSynthesis\" in SDV_METRIC_CHOICES[prop]:\nif \"NewRowSynthesis\" not in self._metric_args:\nself._metric_args[\"NewRowSynthesis\"] = {}\nself._metric_args[\"NewRowSynthesis\"][\"synthetic_sample_size\"] = min(\nmin(len(real_data), len(synthetic_data)),\nself._metric_args[\"NewRowSynthesis\"].get(\"synthetic_sample_size\", len(real_data)),\n)\nfor metric in tqdm(metrics, desc=prop + \" metrics\", position=1, disable=(not verbose), leave=False):\nmetric_name = metric.__name__\ntry:\nmetric_args = self._metric_args.get(metric_name, {})\nmetric_results = metric.compute_breakdown(real_data, synthetic_data, metadata, **metric_args)\nif \"score\" in metric_results:\nmetric_average = metric_results[\"score\"]\nnum_prop_errors += metric_results.get(\"error\", 0)\nelse:\nmetric_average, num_metric_errors = aggregate_metric_results(metric_results)\nnum_prop_errors += num_metric_errors\nexcept IncomputableMetricError:\n# Metric is not compatible with this dataset.\nmetric_results = {}\nmetric_average = np.nan\nnum_prop_errors += 1\nself._metric_averages[metric_name] = metric_average\nself._metric_results[metric_name] = metric_results\nif (\nprop == \"Column Similarity\"\nand \"ContingencySimilarity\" in self._metric_results\nand \"CorrelationSimilarity\" in self._metric_results\n):\nexisting_column_pairs = list(self._metric_results[\"ContingencySimilarity\"].keys())\nexisting_column_pairs.extend(list(self._metric_results[\"CorrelationSimilarity\"].keys()))\nadditional_results = discretize_and_apply_metric(\nreal_data, synthetic_data, metadata, ContingencySimilarity, existing_column_pairs\n)\nself._metric_results[\"ContingencySimilarity\"].update(additional_results)\nself._metric_averages[\"ContingencySimilarity\"], _ = aggregate_metric_results(\nself._metric_results[\"ContingencySimilarity\"]\n)\nself._property_breakdown[prop] = np.mean([s for m in metrics for s in self._get_metric_scores(m.__name__)])\nself._property_errors[prop] = num_prop_errors\nself._overall_quality_score = np.nanmean(list(self._property_breakdown.values()))\nif verbose:\nself._print_results()\ndef get_score(self):\n\"\"\"Return the overall quality score.\n        Returns:\n            float\n                The overall quality score.\n        \"\"\"\nreturn self._overall_quality_score\ndef get_properties(self):\n\"\"\"Return the property score breakdown.\n        Returns:\n            pandas.DataFrame\n                The property score breakdown.\n        \"\"\"\nreturn pd.DataFrame(\n{\n\"Property\": self._property_breakdown.keys(),\n\"Score\": self._property_breakdown.values(),\n}\n)\ndef get_visualization(self, property_name):\n\"\"\"Return a visualization for each score for the given property name.\n        Args:\n            property_name (str):\n                The name of the property to return score details for.\n        Returns:\n            plotly.graph_objects._figure.Figure\n                The visualization for the requested property.\n        \"\"\"\nscore_breakdowns = {\nmetric.__name__: self._metric_results[metric.__name__] for metric in self._metrics.get(property_name, [])\n}\nif property_name == \"Column Shape\":\nfig = get_column_shapes_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Column Similarity\":\nfig = get_column_pairs_plot(\nscore_breakdowns,\nself._property_breakdown[property_name],\n)\nfig.show()\nelif property_name == \"Coverage\":\nfig = get_column_coverage_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Boundary\":\nfig = get_column_boundaries_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Synthesis\":\nfig = get_synthesis_plot(score_breakdowns.get(\"NewRowSynthesis\", {}))\nfig.show()\nelif property_name == \"Detection\":\nprint(\"WARNING: Detection plots not currently implemented.\")\nelif property_name == \"Divergence\":\nprint(\"WARNING: Divergence plots not currently implemented.\")\nelse:\nraise ValueError(f\"Property name `{property_name}` is not recognized / supported.\")\ndef get_details(self, property_name):\n\"\"\"Return the details for each score for the given property name.\n        Args:\n            property_name (str):\n                The name of the property to return score details for.\n        Returns:\n            pandas.DataFrame\n                The score breakdown.\n        \"\"\"\ncolumns = []\nmetrics = []\nscores = []\nerrors = []\ndetails = pd.DataFrame()\nif property_name == \"Detection\":\nfor metric in self._metrics[property_name]:\nmetric_results = self._metric_results[metric.__name__]\nif \"score\" in metric_results and pd.isna(metric_results[\"score\"]):\ncontinue\nmetrics.append(metric.__name__)\nscores.append(metric_results.get(\"score\", np.nan))\nerrors.append(metric_results.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Shape\":\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nif \"score\" in score_breakdown and pd.isna(score_breakdown[\"score\"]):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Similarity\" or property_name == \"Divergence\":\nreal_scores = []\nsynthetic_scores = []\nfor metric in self._metrics[property_name]:\nfor column_pair, score_breakdown in self._metric_results[metric.__name__].items():\ncolumns.append(column_pair)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nif property_name == \"Column Similarity\":\nreal_scores.append(score_breakdown.get(\"real\", np.nan))\nsynthetic_scores.append(score_breakdown.get(\"synthetic\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column 1\": [col1 for col1, _ in columns],\n\"Column 2\": [col2 for _, col2 in columns],\n\"Metric\": metrics,\n\"Overall Score\": scores,\n\"Real Correlation\": real_scores,\n\"Synthetic Correlation\": synthetic_scores,\n}\n)\nelif property_name == \"Synthesis\":\nmetric_name = self._metrics[property_name][0].__name__\nmetric_result = self._metric_results[metric_name]\ndetails = pd.DataFrame(\n{\n\"Metric\": [metric_name],\n\"Overall Score\": [metric_result.get(\"score\", np.nan)],\n\"Num Matched Rows\": [metric_result.get(\"num_matched_rows\", np.nan)],\n\"Num New Rows\": [metric_result.get(\"num_new_rows\", np.nan)],\n}\n)\nerrors.append(metric_result.get(\"error\", np.nan))\nelse:\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nmetric_score = score_breakdown.get(\"score\", np.nan)\nmetric_error = score_breakdown.get(\"error\", np.nan)\nif pd.isna(metric_score) and pd.isna(metric_error):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(metric_score)\nerrors.append(metric_error)\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nif pd.Series(errors).notna().sum() &gt; 0:\ndetails[\"Error\"] = errors\nreturn details\ndef get_raw_result(self, metric_name):\n\"\"\"Return the raw result of the given metric name.\n        Args:\n            metric_name (str):\n                The name of the desired metric.\n        Returns:\n            dict\n                The raw results\n        \"\"\"\nmetrics = list(itertools.chain.from_iterable(self._metrics.values()))\nfor metric in metrics:\nif metric.__name__ == metric_name:\nreturn [\n{\n\"metric\": {\n\"method\": f\"{metric.__module__}.{metric.__name__}\",\n\"parameters\": {},\n},\n\"results\": {\nkey: result\nfor key, result in self._metric_results[metric_name].items()\nif not pd.isna(result.get(\"score\", np.nan))\n},\n},\n]\ndef save(self, filepath):\n\"\"\"Save this report instance to the given path using pickle.\n        Args:\n            filepath (str):\n                The path to the file where the report instance will be serialized.\n        \"\"\"\nwith open(filepath, \"wb\") as output:\npickle.dump(self, output)\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport.generate","title":"<code>generate(self, real_data, synthetic_data, metadata, verbose=True)</code>","text":"<p>Generate report.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>pandas.DataFrame</code> <p>The real data.</p> required <code>synthetic_data</code> <code>pandas.DataFrame</code> <p>The synthetic data.</p> required <code>metadata</code> <code>dict</code> <p>The metadata, which contains each column's data type as well as relationships.</p> required <code>verbose</code> <code>bool</code> <p>Whether or not to print report summary and progress.</p> <code>True</code> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def generate(self, real_data, synthetic_data, metadata, verbose=True):\n\"\"\"Generate report.\n    Args:\n        real_data (pandas.DataFrame):\n            The real data.\n        synthetic_data (pandas.DataFrame):\n            The synthetic data.\n        metadata (dict):\n            The metadata, which contains each column's data type as well as relationships.\n        verbose (bool):\n            Whether or not to print report summary and progress.\n    \"\"\"\nprint(\"\")\nvalidate_single_table_inputs(real_data, synthetic_data, metadata)\nself._property_breakdown = {}\nfor prop, metrics in tqdm(\nself._metrics.items(), desc=\"Creating report\", position=0, disable=(not verbose), leave=True\n):\nnum_prop_errors = 0\nif \"NewRowSynthesis\" in SDV_METRIC_CHOICES[prop]:\nif \"NewRowSynthesis\" not in self._metric_args:\nself._metric_args[\"NewRowSynthesis\"] = {}\nself._metric_args[\"NewRowSynthesis\"][\"synthetic_sample_size\"] = min(\nmin(len(real_data), len(synthetic_data)),\nself._metric_args[\"NewRowSynthesis\"].get(\"synthetic_sample_size\", len(real_data)),\n)\nfor metric in tqdm(metrics, desc=prop + \" metrics\", position=1, disable=(not verbose), leave=False):\nmetric_name = metric.__name__\ntry:\nmetric_args = self._metric_args.get(metric_name, {})\nmetric_results = metric.compute_breakdown(real_data, synthetic_data, metadata, **metric_args)\nif \"score\" in metric_results:\nmetric_average = metric_results[\"score\"]\nnum_prop_errors += metric_results.get(\"error\", 0)\nelse:\nmetric_average, num_metric_errors = aggregate_metric_results(metric_results)\nnum_prop_errors += num_metric_errors\nexcept IncomputableMetricError:\n# Metric is not compatible with this dataset.\nmetric_results = {}\nmetric_average = np.nan\nnum_prop_errors += 1\nself._metric_averages[metric_name] = metric_average\nself._metric_results[metric_name] = metric_results\nif (\nprop == \"Column Similarity\"\nand \"ContingencySimilarity\" in self._metric_results\nand \"CorrelationSimilarity\" in self._metric_results\n):\nexisting_column_pairs = list(self._metric_results[\"ContingencySimilarity\"].keys())\nexisting_column_pairs.extend(list(self._metric_results[\"CorrelationSimilarity\"].keys()))\nadditional_results = discretize_and_apply_metric(\nreal_data, synthetic_data, metadata, ContingencySimilarity, existing_column_pairs\n)\nself._metric_results[\"ContingencySimilarity\"].update(additional_results)\nself._metric_averages[\"ContingencySimilarity\"], _ = aggregate_metric_results(\nself._metric_results[\"ContingencySimilarity\"]\n)\nself._property_breakdown[prop] = np.mean([s for m in metrics for s in self._get_metric_scores(m.__name__)])\nself._property_errors[prop] = num_prop_errors\nself._overall_quality_score = np.nanmean(list(self._property_breakdown.values()))\nif verbose:\nself._print_results()\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport.get_details","title":"<code>get_details(self, property_name)</code>","text":"<p>Return the details for each score for the given property name.</p> <p>Parameters:</p> Name Type Description Default <code>property_name</code> <code>str</code> <p>The name of the property to return score details for.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame     The score breakdown.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_details(self, property_name):\n\"\"\"Return the details for each score for the given property name.\n    Args:\n        property_name (str):\n            The name of the property to return score details for.\n    Returns:\n        pandas.DataFrame\n            The score breakdown.\n    \"\"\"\ncolumns = []\nmetrics = []\nscores = []\nerrors = []\ndetails = pd.DataFrame()\nif property_name == \"Detection\":\nfor metric in self._metrics[property_name]:\nmetric_results = self._metric_results[metric.__name__]\nif \"score\" in metric_results and pd.isna(metric_results[\"score\"]):\ncontinue\nmetrics.append(metric.__name__)\nscores.append(metric_results.get(\"score\", np.nan))\nerrors.append(metric_results.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Shape\":\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nif \"score\" in score_breakdown and pd.isna(score_breakdown[\"score\"]):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Similarity\" or property_name == \"Divergence\":\nreal_scores = []\nsynthetic_scores = []\nfor metric in self._metrics[property_name]:\nfor column_pair, score_breakdown in self._metric_results[metric.__name__].items():\ncolumns.append(column_pair)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nif property_name == \"Column Similarity\":\nreal_scores.append(score_breakdown.get(\"real\", np.nan))\nsynthetic_scores.append(score_breakdown.get(\"synthetic\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column 1\": [col1 for col1, _ in columns],\n\"Column 2\": [col2 for _, col2 in columns],\n\"Metric\": metrics,\n\"Overall Score\": scores,\n\"Real Correlation\": real_scores,\n\"Synthetic Correlation\": synthetic_scores,\n}\n)\nelif property_name == \"Synthesis\":\nmetric_name = self._metrics[property_name][0].__name__\nmetric_result = self._metric_results[metric_name]\ndetails = pd.DataFrame(\n{\n\"Metric\": [metric_name],\n\"Overall Score\": [metric_result.get(\"score\", np.nan)],\n\"Num Matched Rows\": [metric_result.get(\"num_matched_rows\", np.nan)],\n\"Num New Rows\": [metric_result.get(\"num_new_rows\", np.nan)],\n}\n)\nerrors.append(metric_result.get(\"error\", np.nan))\nelse:\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nmetric_score = score_breakdown.get(\"score\", np.nan)\nmetric_error = score_breakdown.get(\"error\", np.nan)\nif pd.isna(metric_score) and pd.isna(metric_error):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(metric_score)\nerrors.append(metric_error)\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nif pd.Series(errors).notna().sum() &gt; 0:\ndetails[\"Error\"] = errors\nreturn details\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport.get_properties","title":"<code>get_properties(self)</code>","text":"<p>Return the property score breakdown.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame     The property score breakdown.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_properties(self):\n\"\"\"Return the property score breakdown.\n    Returns:\n        pandas.DataFrame\n            The property score breakdown.\n    \"\"\"\nreturn pd.DataFrame(\n{\n\"Property\": self._property_breakdown.keys(),\n\"Score\": self._property_breakdown.values(),\n}\n)\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport.get_raw_result","title":"<code>get_raw_result(self, metric_name)</code>","text":"<p>Return the raw result of the given metric name.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the desired metric.</p> required <p>Returns:</p> Type Description <p>dict     The raw results</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_raw_result(self, metric_name):\n\"\"\"Return the raw result of the given metric name.\n    Args:\n        metric_name (str):\n            The name of the desired metric.\n    Returns:\n        dict\n            The raw results\n    \"\"\"\nmetrics = list(itertools.chain.from_iterable(self._metrics.values()))\nfor metric in metrics:\nif metric.__name__ == metric_name:\nreturn [\n{\n\"metric\": {\n\"method\": f\"{metric.__module__}.{metric.__name__}\",\n\"parameters\": {},\n},\n\"results\": {\nkey: result\nfor key, result in self._metric_results[metric_name].items()\nif not pd.isna(result.get(\"score\", np.nan))\n},\n},\n]\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport.get_score","title":"<code>get_score(self)</code>","text":"<p>Return the overall quality score.</p> <p>Returns:</p> Type Description <p>float     The overall quality score.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_score(self):\n\"\"\"Return the overall quality score.\n    Returns:\n        float\n            The overall quality score.\n    \"\"\"\nreturn self._overall_quality_score\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport.get_visualization","title":"<code>get_visualization(self, property_name)</code>","text":"<p>Return a visualization for each score for the given property name.</p> <p>Parameters:</p> Name Type Description Default <code>property_name</code> <code>str</code> <p>The name of the property to return score details for.</p> required <p>Returns:</p> Type Description <p>plotly.graph_objects._figure.Figure     The visualization for the requested property.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_visualization(self, property_name):\n\"\"\"Return a visualization for each score for the given property name.\n    Args:\n        property_name (str):\n            The name of the property to return score details for.\n    Returns:\n        plotly.graph_objects._figure.Figure\n            The visualization for the requested property.\n    \"\"\"\nscore_breakdowns = {\nmetric.__name__: self._metric_results[metric.__name__] for metric in self._metrics.get(property_name, [])\n}\nif property_name == \"Column Shape\":\nfig = get_column_shapes_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Column Similarity\":\nfig = get_column_pairs_plot(\nscore_breakdowns,\nself._property_breakdown[property_name],\n)\nfig.show()\nelif property_name == \"Coverage\":\nfig = get_column_coverage_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Boundary\":\nfig = get_column_boundaries_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Synthesis\":\nfig = get_synthesis_plot(score_breakdowns.get(\"NewRowSynthesis\", {}))\nfig.show()\nelif property_name == \"Detection\":\nprint(\"WARNING: Detection plots not currently implemented.\")\nelif property_name == \"Divergence\":\nprint(\"WARNING: Divergence plots not currently implemented.\")\nelse:\nraise ValueError(f\"Property name `{property_name}` is not recognized / supported.\")\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.full_report.FullReport.save","title":"<code>save(self, filepath)</code>","text":"<p>Save this report instance to the given path using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file where the report instance will be serialized.</p> required Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def save(self, filepath):\n\"\"\"Save this report instance to the given path using pickle.\n    Args:\n        filepath (str):\n            The path to the file where the report instance will be serialized.\n    \"\"\"\nwith open(filepath, \"wb\") as output:\npickle.dump(self, output)\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.io","title":"<code>io</code>","text":""},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_synthetic, fn_metadata, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed data file.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>fn_metadata</code> <code>str</code> <p>The name of the metadata file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/evaluation/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_typed: str, fn_synthetic: str, fn_metadata: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed data file.\n        fn_synthetic: The name of the metatransformer file.\n        fn_metadata: The name of the metadata file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_typed, fn_synthetic, fn_metadata = consistent_endings(\n[fn_dataset, fn_typed, fn_synthetic, (fn_metadata, \".yaml\")]\n)\nfn_typed, fn_synthetic, fn_metadata = potential_suffixes([fn_typed, fn_synthetic, fn_metadata], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_typed, fn_synthetic, fn_metadata], dir_experiment)\ncheck_exists([fn_typed, fn_synthetic, fn_metadata], dir_experiment)\nreturn fn_dataset, fn_typed, fn_synthetic, fn_metadata\n</code></pre>"},{"location":"reference/modules/evaluation/#nhssynth.modules.evaluation.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/evaluation/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"typed_dataset\", \"synthetic\", \"sdtypes\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"typed_dataset\"],\nargs.module_handover[\"synthetic\"],\nargs.module_handover[\"sdtypes\"],\n)\nelse:\nfn_dataset, fn_typed, fn_synthetic, fn_metadata = check_input_paths(\nargs.dataset, args.typed, args.synthetic, args.metadata, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f)\nwith open(dir_experiment / fn_synthetic, \"rb\") as f:\nsynthetic_data = pickle.load(f)\nsdtypes = get_sdtypes(load_metadata(dir_experiment / fn_metadata, real_data))\nreturn fn_dataset, real_data, synthetic_data, sdtypes\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/","title":"full_report","text":"<p>Single table quality report.</p>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport","title":"<code> FullReport        </code>","text":"<p>Single table full report.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>class FullReport:\n\"\"\"Single table full report.\"\"\"\ndef __init__(self, metrics, metric_args={}):\nself._overall_quality_score = None\nself._metric_results = {}\nself._metric_averages = {}\nself._property_breakdown = {}\nself._property_errors = {}\nself._metrics = metrics\nself._metric_args = metric_args\ndef _get_metric_scores(self, metric_name):\n\"\"\"Aggregate the scores and errors in a metric results mapping.\n        Args:\n            The metric results to aggregate.\n        Returns:\n            The average of the metric scores, and the number of errors.\n        \"\"\"\nmetric_results = self._metric_results.get(metric_name, {})\nif len(metric_results) == 0:\nreturn np.nan\nmetric_scores = []\nfor breakdown in metric_results.values():\nif isinstance(breakdown, dict):\nmetric_score = breakdown.get(\"score\", np.nan)\nif not np.isnan(metric_score):\nmetric_scores.append(metric_score)\nelse:\nreturn [metric_results.get(\"score\", np.nan)]\nreturn metric_scores\ndef _print_results(self):\n\"\"\"Print the quality report results.\"\"\"\nif pd.isna(self._overall_quality_score) &amp; any(self._property_errors.values()):\nprint(\"\\nOverall Score: Error computing report.\")\nelse:\nprint(f\"\\nOverall Score: {round(self._overall_quality_score * 100, 2)}%\")\nif len(self._property_breakdown) &gt; 0:\nprint(\"\\nProperties:\")\nfor prop, score in self._property_breakdown.items():\nif not pd.isna(score):\nprint(f\"{prop}: {round(score * 100, 2)}%\")\nelif self._property_errors[prop] &gt; 0:\nprint(f\"{prop}: Error computing property.\")\nelse:\nprint(f\"{prop}: NaN\")\nprint(\"\")\ndef generate(self, real_data, synthetic_data, metadata, verbose=True):\n\"\"\"Generate report.\n        Args:\n            real_data (pandas.DataFrame):\n                The real data.\n            synthetic_data (pandas.DataFrame):\n                The synthetic data.\n            metadata (dict):\n                The metadata, which contains each column's data type as well as relationships.\n            verbose (bool):\n                Whether or not to print report summary and progress.\n        \"\"\"\nprint(\"\")\nvalidate_single_table_inputs(real_data, synthetic_data, metadata)\nself._property_breakdown = {}\nfor prop, metrics in tqdm(\nself._metrics.items(), desc=\"Creating report\", position=0, disable=(not verbose), leave=True\n):\nnum_prop_errors = 0\nif \"NewRowSynthesis\" in SDV_METRIC_CHOICES[prop]:\nif \"NewRowSynthesis\" not in self._metric_args:\nself._metric_args[\"NewRowSynthesis\"] = {}\nself._metric_args[\"NewRowSynthesis\"][\"synthetic_sample_size\"] = min(\nmin(len(real_data), len(synthetic_data)),\nself._metric_args[\"NewRowSynthesis\"].get(\"synthetic_sample_size\", len(real_data)),\n)\nfor metric in tqdm(metrics, desc=prop + \" metrics\", position=1, disable=(not verbose), leave=False):\nmetric_name = metric.__name__\ntry:\nmetric_args = self._metric_args.get(metric_name, {})\nmetric_results = metric.compute_breakdown(real_data, synthetic_data, metadata, **metric_args)\nif \"score\" in metric_results:\nmetric_average = metric_results[\"score\"]\nnum_prop_errors += metric_results.get(\"error\", 0)\nelse:\nmetric_average, num_metric_errors = aggregate_metric_results(metric_results)\nnum_prop_errors += num_metric_errors\nexcept IncomputableMetricError:\n# Metric is not compatible with this dataset.\nmetric_results = {}\nmetric_average = np.nan\nnum_prop_errors += 1\nself._metric_averages[metric_name] = metric_average\nself._metric_results[metric_name] = metric_results\nif (\nprop == \"Column Similarity\"\nand \"ContingencySimilarity\" in self._metric_results\nand \"CorrelationSimilarity\" in self._metric_results\n):\nexisting_column_pairs = list(self._metric_results[\"ContingencySimilarity\"].keys())\nexisting_column_pairs.extend(list(self._metric_results[\"CorrelationSimilarity\"].keys()))\nadditional_results = discretize_and_apply_metric(\nreal_data, synthetic_data, metadata, ContingencySimilarity, existing_column_pairs\n)\nself._metric_results[\"ContingencySimilarity\"].update(additional_results)\nself._metric_averages[\"ContingencySimilarity\"], _ = aggregate_metric_results(\nself._metric_results[\"ContingencySimilarity\"]\n)\nself._property_breakdown[prop] = np.mean([s for m in metrics for s in self._get_metric_scores(m.__name__)])\nself._property_errors[prop] = num_prop_errors\nself._overall_quality_score = np.nanmean(list(self._property_breakdown.values()))\nif verbose:\nself._print_results()\ndef get_score(self):\n\"\"\"Return the overall quality score.\n        Returns:\n            float\n                The overall quality score.\n        \"\"\"\nreturn self._overall_quality_score\ndef get_properties(self):\n\"\"\"Return the property score breakdown.\n        Returns:\n            pandas.DataFrame\n                The property score breakdown.\n        \"\"\"\nreturn pd.DataFrame(\n{\n\"Property\": self._property_breakdown.keys(),\n\"Score\": self._property_breakdown.values(),\n}\n)\ndef get_visualization(self, property_name):\n\"\"\"Return a visualization for each score for the given property name.\n        Args:\n            property_name (str):\n                The name of the property to return score details for.\n        Returns:\n            plotly.graph_objects._figure.Figure\n                The visualization for the requested property.\n        \"\"\"\nscore_breakdowns = {\nmetric.__name__: self._metric_results[metric.__name__] for metric in self._metrics.get(property_name, [])\n}\nif property_name == \"Column Shape\":\nfig = get_column_shapes_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Column Similarity\":\nfig = get_column_pairs_plot(\nscore_breakdowns,\nself._property_breakdown[property_name],\n)\nfig.show()\nelif property_name == \"Coverage\":\nfig = get_column_coverage_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Boundary\":\nfig = get_column_boundaries_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Synthesis\":\nfig = get_synthesis_plot(score_breakdowns.get(\"NewRowSynthesis\", {}))\nfig.show()\nelif property_name == \"Detection\":\nprint(\"WARNING: Detection plots not currently implemented.\")\nelif property_name == \"Divergence\":\nprint(\"WARNING: Divergence plots not currently implemented.\")\nelse:\nraise ValueError(f\"Property name `{property_name}` is not recognized / supported.\")\ndef get_details(self, property_name):\n\"\"\"Return the details for each score for the given property name.\n        Args:\n            property_name (str):\n                The name of the property to return score details for.\n        Returns:\n            pandas.DataFrame\n                The score breakdown.\n        \"\"\"\ncolumns = []\nmetrics = []\nscores = []\nerrors = []\ndetails = pd.DataFrame()\nif property_name == \"Detection\":\nfor metric in self._metrics[property_name]:\nmetric_results = self._metric_results[metric.__name__]\nif \"score\" in metric_results and pd.isna(metric_results[\"score\"]):\ncontinue\nmetrics.append(metric.__name__)\nscores.append(metric_results.get(\"score\", np.nan))\nerrors.append(metric_results.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Shape\":\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nif \"score\" in score_breakdown and pd.isna(score_breakdown[\"score\"]):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Similarity\" or property_name == \"Divergence\":\nreal_scores = []\nsynthetic_scores = []\nfor metric in self._metrics[property_name]:\nfor column_pair, score_breakdown in self._metric_results[metric.__name__].items():\ncolumns.append(column_pair)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nif property_name == \"Column Similarity\":\nreal_scores.append(score_breakdown.get(\"real\", np.nan))\nsynthetic_scores.append(score_breakdown.get(\"synthetic\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column 1\": [col1 for col1, _ in columns],\n\"Column 2\": [col2 for _, col2 in columns],\n\"Metric\": metrics,\n\"Overall Score\": scores,\n\"Real Correlation\": real_scores,\n\"Synthetic Correlation\": synthetic_scores,\n}\n)\nelif property_name == \"Synthesis\":\nmetric_name = self._metrics[property_name][0].__name__\nmetric_result = self._metric_results[metric_name]\ndetails = pd.DataFrame(\n{\n\"Metric\": [metric_name],\n\"Overall Score\": [metric_result.get(\"score\", np.nan)],\n\"Num Matched Rows\": [metric_result.get(\"num_matched_rows\", np.nan)],\n\"Num New Rows\": [metric_result.get(\"num_new_rows\", np.nan)],\n}\n)\nerrors.append(metric_result.get(\"error\", np.nan))\nelse:\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nmetric_score = score_breakdown.get(\"score\", np.nan)\nmetric_error = score_breakdown.get(\"error\", np.nan)\nif pd.isna(metric_score) and pd.isna(metric_error):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(metric_score)\nerrors.append(metric_error)\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nif pd.Series(errors).notna().sum() &gt; 0:\ndetails[\"Error\"] = errors\nreturn details\ndef get_raw_result(self, metric_name):\n\"\"\"Return the raw result of the given metric name.\n        Args:\n            metric_name (str):\n                The name of the desired metric.\n        Returns:\n            dict\n                The raw results\n        \"\"\"\nmetrics = list(itertools.chain.from_iterable(self._metrics.values()))\nfor metric in metrics:\nif metric.__name__ == metric_name:\nreturn [\n{\n\"metric\": {\n\"method\": f\"{metric.__module__}.{metric.__name__}\",\n\"parameters\": {},\n},\n\"results\": {\nkey: result\nfor key, result in self._metric_results[metric_name].items()\nif not pd.isna(result.get(\"score\", np.nan))\n},\n},\n]\ndef save(self, filepath):\n\"\"\"Save this report instance to the given path using pickle.\n        Args:\n            filepath (str):\n                The path to the file where the report instance will be serialized.\n        \"\"\"\nwith open(filepath, \"wb\") as output:\npickle.dump(self, output)\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport.generate","title":"<code>generate(self, real_data, synthetic_data, metadata, verbose=True)</code>","text":"<p>Generate report.</p> <p>Parameters:</p> Name Type Description Default <code>real_data</code> <code>pandas.DataFrame</code> <p>The real data.</p> required <code>synthetic_data</code> <code>pandas.DataFrame</code> <p>The synthetic data.</p> required <code>metadata</code> <code>dict</code> <p>The metadata, which contains each column's data type as well as relationships.</p> required <code>verbose</code> <code>bool</code> <p>Whether or not to print report summary and progress.</p> <code>True</code> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def generate(self, real_data, synthetic_data, metadata, verbose=True):\n\"\"\"Generate report.\n    Args:\n        real_data (pandas.DataFrame):\n            The real data.\n        synthetic_data (pandas.DataFrame):\n            The synthetic data.\n        metadata (dict):\n            The metadata, which contains each column's data type as well as relationships.\n        verbose (bool):\n            Whether or not to print report summary and progress.\n    \"\"\"\nprint(\"\")\nvalidate_single_table_inputs(real_data, synthetic_data, metadata)\nself._property_breakdown = {}\nfor prop, metrics in tqdm(\nself._metrics.items(), desc=\"Creating report\", position=0, disable=(not verbose), leave=True\n):\nnum_prop_errors = 0\nif \"NewRowSynthesis\" in SDV_METRIC_CHOICES[prop]:\nif \"NewRowSynthesis\" not in self._metric_args:\nself._metric_args[\"NewRowSynthesis\"] = {}\nself._metric_args[\"NewRowSynthesis\"][\"synthetic_sample_size\"] = min(\nmin(len(real_data), len(synthetic_data)),\nself._metric_args[\"NewRowSynthesis\"].get(\"synthetic_sample_size\", len(real_data)),\n)\nfor metric in tqdm(metrics, desc=prop + \" metrics\", position=1, disable=(not verbose), leave=False):\nmetric_name = metric.__name__\ntry:\nmetric_args = self._metric_args.get(metric_name, {})\nmetric_results = metric.compute_breakdown(real_data, synthetic_data, metadata, **metric_args)\nif \"score\" in metric_results:\nmetric_average = metric_results[\"score\"]\nnum_prop_errors += metric_results.get(\"error\", 0)\nelse:\nmetric_average, num_metric_errors = aggregate_metric_results(metric_results)\nnum_prop_errors += num_metric_errors\nexcept IncomputableMetricError:\n# Metric is not compatible with this dataset.\nmetric_results = {}\nmetric_average = np.nan\nnum_prop_errors += 1\nself._metric_averages[metric_name] = metric_average\nself._metric_results[metric_name] = metric_results\nif (\nprop == \"Column Similarity\"\nand \"ContingencySimilarity\" in self._metric_results\nand \"CorrelationSimilarity\" in self._metric_results\n):\nexisting_column_pairs = list(self._metric_results[\"ContingencySimilarity\"].keys())\nexisting_column_pairs.extend(list(self._metric_results[\"CorrelationSimilarity\"].keys()))\nadditional_results = discretize_and_apply_metric(\nreal_data, synthetic_data, metadata, ContingencySimilarity, existing_column_pairs\n)\nself._metric_results[\"ContingencySimilarity\"].update(additional_results)\nself._metric_averages[\"ContingencySimilarity\"], _ = aggregate_metric_results(\nself._metric_results[\"ContingencySimilarity\"]\n)\nself._property_breakdown[prop] = np.mean([s for m in metrics for s in self._get_metric_scores(m.__name__)])\nself._property_errors[prop] = num_prop_errors\nself._overall_quality_score = np.nanmean(list(self._property_breakdown.values()))\nif verbose:\nself._print_results()\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport.get_details","title":"<code>get_details(self, property_name)</code>","text":"<p>Return the details for each score for the given property name.</p> <p>Parameters:</p> Name Type Description Default <code>property_name</code> <code>str</code> <p>The name of the property to return score details for.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame     The score breakdown.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_details(self, property_name):\n\"\"\"Return the details for each score for the given property name.\n    Args:\n        property_name (str):\n            The name of the property to return score details for.\n    Returns:\n        pandas.DataFrame\n            The score breakdown.\n    \"\"\"\ncolumns = []\nmetrics = []\nscores = []\nerrors = []\ndetails = pd.DataFrame()\nif property_name == \"Detection\":\nfor metric in self._metrics[property_name]:\nmetric_results = self._metric_results[metric.__name__]\nif \"score\" in metric_results and pd.isna(metric_results[\"score\"]):\ncontinue\nmetrics.append(metric.__name__)\nscores.append(metric_results.get(\"score\", np.nan))\nerrors.append(metric_results.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Shape\":\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nif \"score\" in score_breakdown and pd.isna(score_breakdown[\"score\"]):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nelif property_name == \"Column Similarity\" or property_name == \"Divergence\":\nreal_scores = []\nsynthetic_scores = []\nfor metric in self._metrics[property_name]:\nfor column_pair, score_breakdown in self._metric_results[metric.__name__].items():\ncolumns.append(column_pair)\nmetrics.append(metric.__name__)\nscores.append(score_breakdown.get(\"score\", np.nan))\nif property_name == \"Column Similarity\":\nreal_scores.append(score_breakdown.get(\"real\", np.nan))\nsynthetic_scores.append(score_breakdown.get(\"synthetic\", np.nan))\nerrors.append(score_breakdown.get(\"error\", np.nan))\ndetails = pd.DataFrame(\n{\n\"Column 1\": [col1 for col1, _ in columns],\n\"Column 2\": [col2 for _, col2 in columns],\n\"Metric\": metrics,\n\"Overall Score\": scores,\n\"Real Correlation\": real_scores,\n\"Synthetic Correlation\": synthetic_scores,\n}\n)\nelif property_name == \"Synthesis\":\nmetric_name = self._metrics[property_name][0].__name__\nmetric_result = self._metric_results[metric_name]\ndetails = pd.DataFrame(\n{\n\"Metric\": [metric_name],\n\"Overall Score\": [metric_result.get(\"score\", np.nan)],\n\"Num Matched Rows\": [metric_result.get(\"num_matched_rows\", np.nan)],\n\"Num New Rows\": [metric_result.get(\"num_new_rows\", np.nan)],\n}\n)\nerrors.append(metric_result.get(\"error\", np.nan))\nelse:\nfor metric in self._metrics[property_name]:\nfor column, score_breakdown in self._metric_results[metric.__name__].items():\nmetric_score = score_breakdown.get(\"score\", np.nan)\nmetric_error = score_breakdown.get(\"error\", np.nan)\nif pd.isna(metric_score) and pd.isna(metric_error):\ncontinue\ncolumns.append(column)\nmetrics.append(metric.__name__)\nscores.append(metric_score)\nerrors.append(metric_error)\ndetails = pd.DataFrame(\n{\n\"Column\": columns,\n\"Metric\": metrics,\n\"Overall Score\": scores,\n}\n)\nif pd.Series(errors).notna().sum() &gt; 0:\ndetails[\"Error\"] = errors\nreturn details\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport.get_properties","title":"<code>get_properties(self)</code>","text":"<p>Return the property score breakdown.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame     The property score breakdown.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_properties(self):\n\"\"\"Return the property score breakdown.\n    Returns:\n        pandas.DataFrame\n            The property score breakdown.\n    \"\"\"\nreturn pd.DataFrame(\n{\n\"Property\": self._property_breakdown.keys(),\n\"Score\": self._property_breakdown.values(),\n}\n)\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport.get_raw_result","title":"<code>get_raw_result(self, metric_name)</code>","text":"<p>Return the raw result of the given metric name.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the desired metric.</p> required <p>Returns:</p> Type Description <p>dict     The raw results</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_raw_result(self, metric_name):\n\"\"\"Return the raw result of the given metric name.\n    Args:\n        metric_name (str):\n            The name of the desired metric.\n    Returns:\n        dict\n            The raw results\n    \"\"\"\nmetrics = list(itertools.chain.from_iterable(self._metrics.values()))\nfor metric in metrics:\nif metric.__name__ == metric_name:\nreturn [\n{\n\"metric\": {\n\"method\": f\"{metric.__module__}.{metric.__name__}\",\n\"parameters\": {},\n},\n\"results\": {\nkey: result\nfor key, result in self._metric_results[metric_name].items()\nif not pd.isna(result.get(\"score\", np.nan))\n},\n},\n]\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport.get_score","title":"<code>get_score(self)</code>","text":"<p>Return the overall quality score.</p> <p>Returns:</p> Type Description <p>float     The overall quality score.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_score(self):\n\"\"\"Return the overall quality score.\n    Returns:\n        float\n            The overall quality score.\n    \"\"\"\nreturn self._overall_quality_score\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport.get_visualization","title":"<code>get_visualization(self, property_name)</code>","text":"<p>Return a visualization for each score for the given property name.</p> <p>Parameters:</p> Name Type Description Default <code>property_name</code> <code>str</code> <p>The name of the property to return score details for.</p> required <p>Returns:</p> Type Description <p>plotly.graph_objects._figure.Figure     The visualization for the requested property.</p> Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def get_visualization(self, property_name):\n\"\"\"Return a visualization for each score for the given property name.\n    Args:\n        property_name (str):\n            The name of the property to return score details for.\n    Returns:\n        plotly.graph_objects._figure.Figure\n            The visualization for the requested property.\n    \"\"\"\nscore_breakdowns = {\nmetric.__name__: self._metric_results[metric.__name__] for metric in self._metrics.get(property_name, [])\n}\nif property_name == \"Column Shape\":\nfig = get_column_shapes_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Column Similarity\":\nfig = get_column_pairs_plot(\nscore_breakdowns,\nself._property_breakdown[property_name],\n)\nfig.show()\nelif property_name == \"Coverage\":\nfig = get_column_coverage_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Boundary\":\nfig = get_column_boundaries_plot(score_breakdowns, self._property_breakdown[property_name])\nfig.show()\nelif property_name == \"Synthesis\":\nfig = get_synthesis_plot(score_breakdowns.get(\"NewRowSynthesis\", {}))\nfig.show()\nelif property_name == \"Detection\":\nprint(\"WARNING: Detection plots not currently implemented.\")\nelif property_name == \"Divergence\":\nprint(\"WARNING: Divergence plots not currently implemented.\")\nelse:\nraise ValueError(f\"Property name `{property_name}` is not recognized / supported.\")\n</code></pre>"},{"location":"reference/modules/evaluation/full_report/#nhssynth.modules.evaluation.full_report.FullReport.save","title":"<code>save(self, filepath)</code>","text":"<p>Save this report instance to the given path using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file where the report instance will be serialized.</p> required Source code in <code>modules/evaluation/full_report.py</code> <pre><code>def save(self, filepath):\n\"\"\"Save this report instance to the given path using pickle.\n    Args:\n        filepath (str):\n            The path to the file where the report instance will be serialized.\n    \"\"\"\nwith open(filepath, \"wb\") as output:\npickle.dump(self, output)\n</code></pre>"},{"location":"reference/modules/evaluation/io/","title":"io","text":""},{"location":"reference/modules/evaluation/io/#nhssynth.modules.evaluation.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_synthetic, fn_metadata, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed data file.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>fn_metadata</code> <code>str</code> <p>The name of the metadata file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/evaluation/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_typed: str, fn_synthetic: str, fn_metadata: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed data file.\n        fn_synthetic: The name of the metatransformer file.\n        fn_metadata: The name of the metadata file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_typed, fn_synthetic, fn_metadata = consistent_endings(\n[fn_dataset, fn_typed, fn_synthetic, (fn_metadata, \".yaml\")]\n)\nfn_typed, fn_synthetic, fn_metadata = potential_suffixes([fn_typed, fn_synthetic, fn_metadata], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_typed, fn_synthetic, fn_metadata], dir_experiment)\ncheck_exists([fn_typed, fn_synthetic, fn_metadata], dir_experiment)\nreturn fn_dataset, fn_typed, fn_synthetic, fn_metadata\n</code></pre>"},{"location":"reference/modules/evaluation/io/#nhssynth.modules.evaluation.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/evaluation/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"typed_dataset\", \"synthetic\", \"sdtypes\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"typed_dataset\"],\nargs.module_handover[\"synthetic\"],\nargs.module_handover[\"sdtypes\"],\n)\nelse:\nfn_dataset, fn_typed, fn_synthetic, fn_metadata = check_input_paths(\nargs.dataset, args.typed, args.synthetic, args.metadata, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f)\nwith open(dir_experiment / fn_synthetic, \"rb\") as f:\nsynthetic_data = pickle.load(f)\nsdtypes = get_sdtypes(load_metadata(dir_experiment / fn_metadata, real_data))\nreturn fn_dataset, real_data, synthetic_data, sdtypes\n</code></pre>"},{"location":"reference/modules/evaluation/metrics/","title":"metrics","text":""},{"location":"reference/modules/evaluation/run/","title":"run","text":""},{"location":"reference/modules/model/","title":"model","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE","title":"<code>DPVAE</code>","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\ndef __init__(\nself,\nlatent_dim,\nonehots=[[]],\nsingles=[],\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = len(singles) + sum([len(x) for x in onehots])\nself.singles = singles\nself.onehots = onehots\nself.net = nn.Sequential(\nnn.Linear(latent_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nlatent_dim,\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = 2 * latent_dim\nself.latent_dim = latent_dim\nself.net = nn.Sequential(\nnn.Linear(input_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\ndef __init__(self, num_singles):\nsuper().__init__()\nself.output_logsigma_fn = nn.Linear(num_singles, num_singles, bias=True)\ntorch.nn.init.zeros_(self.output_logsigma_fn.weight)\ntorch.nn.init.zeros_(self.output_logsigma_fn.bias)\nself.output_logsigma_fn.weight.requires_grad = False\ndef forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder.to(encoder.device)\nself.decoder = decoder.to(decoder.device)\nself.device = encoder.device\nself.onehots = self.decoder.onehots\nself.singles = self.decoder.singles\nself.noiser = Noiser(len(self.singles)).to(decoder.device)\ndef reconstruct(self, X):\nmu_z, logsigma_z = self.encoder(X)\nx_recon = self.decoder(mu_z)\nreturn x_recon\ndef generate(self, N):\nz_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\nx_gen = self.decoder(z_samples)\nx_gen_ = torch.ones_like(x_gen, device=self.device)\nfor cat_idxs in self.onehots:\nx_gen_[:, cat_idxs] = torch.distributions.one_hot_categorical.OneHotCategorical(\nlogits=x_gen[:, cat_idxs]\n).sample()\nx_gen_[:, self.singles] = x_gen[:, self.singles] + torch.exp(\nself.noiser(x_gen[:, self.singles])\n) * torch.randn_like(x_gen[:, self.singles])\nif torch.cuda.is_available():\nx_gen_ = x_gen_.cpu()\nreturn x_gen_.detach()\ndef loss(self, X):\nmu_z, logsigma_z = self.encoder(X)\np = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\nq = Normal(mu_z, torch.exp(logsigma_z))\nkld = torch.sum(torch.distributions.kl_divergence(q, p))\ns = torch.randn_like(mu_z)\nz_samples = mu_z + s * torch.exp(logsigma_z)\nx_recon = self.decoder(z_samples)\ncategoric_loglik = 0\nif len(self.onehots):\nfor cat_idxs in self.onehots:\ncategoric_loglik += -torch.nn.functional.cross_entropy(\nx_recon[:, cat_idxs],\ntorch.max(X[:, cat_idxs], 1)[1],\n).sum()\ngauss_loglik = 0\nif len(self.singles):\ngauss_loglik = (\nNormal(\nloc=x_recon[:, self.singles],\nscale=torch.exp(self.noiser(x_recon[:, self.singles])),\n)\n.log_prob(X[:, self.singles])\n.sum()\n)\nreconstruction_loss = -(categoric_loglik + gauss_loglik)\nelbo = kld + reconstruction_loss\nreturn {\n\"ELBO\": elbo,\n\"ReconstructionLoss\": reconstruction_loss,\n\"KLD\": kld,\n\"CategoricalLoss\": categoric_loglik,\n\"NumericalLoss\": gauss_loglik,\n}\ndef train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\ntracked_metrics: list[str] = [\"ELBO\"],\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nprint(\"\")\nself.start_time = time.time()\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\nelif \"Privacy\" in tracked_metrics:\ntracked_metrics.remove(\"Privacy\")\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nmetrics = {metric: [] for metric in tracked_metrics}\nstats_bars = {\nmetric: tqdm(total=0, desc=\"\", position=i, bar_format=\"{desc}\", leave=True)\nfor i, metric in enumerate(tracked_metrics)\n}\nmax_length = max(len(s) for s in tracked_metrics) + 1\nepoch_bar = tqdm(range(num_epochs), desc=\"Epochs\", position=len(stats_bars), leave=False)\nfor epoch in epoch_bar:\nfor key in metrics.keys():\nif key != \"Privacy\":\nmetrics[key].append(0.0)\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=len(stats_bars) + 1, leave=False):\nself.optimizer.zero_grad()\nlosses = self.loss(Y_subset.to(self.encoder.device))\nlosses[\"ELBO\"].backward()\nself.optimizer.step()\nfor key in metrics.keys():\nif key in losses:\nif losses[key]:\nmetrics[key][-1] += losses[key].item()\nelse:\nmetrics[key][-1] += 0.0\nfor key, stats_bar in stats_bars.items():\nif key == \"Privacy\" and privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar.set_description_str(\nf\"{(key + ':').ljust(max_length)}  \\u03B5 = {epsilon_e[0]:.2f}\\tbest \\u03B1 = {epsilon_e[1]:.2f}\"\n)\nmetrics[\"Privacy\"].append(epsilon_e)\nelse:\nstats_bar.set_description_str(f\"{(key + ':').ljust(max_length)}  {metrics[key][-1]:.2f}\")\nif epoch == 0:\nmin_elbo = metrics[\"ELBO\"][-1]\nif metrics[\"ELBO\"][-1] &lt; (min_elbo - delta):\nmin_elbo = metrics[\"ELBO\"][-1]\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nfor stats_bar in stats_bars.values():\nstats_bar.close()\ntqdm.write(f\"Completed {num_epochs} epochs in {time.time() - self.start_time:.2f} seconds.\")\nreturn (num_epochs, metrics)\ndef get_privacy_spent(self, delta):\nif hasattr(self, \"privacy_engine\"):\nreturn self.privacy_engine.get_privacy_spent(delta)\nelse:\nprint(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n)\ndef save(self, filename):\ntorch.save(self.state_dict(), filename)\ndef load(self, filename):\nself.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, num_epochs, tracked_metrics=['ELBO'], privacy_engine=None, patience=5, delta=10)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\ntracked_metrics: list[str] = [\"ELBO\"],\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nprint(\"\")\nself.start_time = time.time()\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\nelif \"Privacy\" in tracked_metrics:\ntracked_metrics.remove(\"Privacy\")\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nmetrics = {metric: [] for metric in tracked_metrics}\nstats_bars = {\nmetric: tqdm(total=0, desc=\"\", position=i, bar_format=\"{desc}\", leave=True)\nfor i, metric in enumerate(tracked_metrics)\n}\nmax_length = max(len(s) for s in tracked_metrics) + 1\nepoch_bar = tqdm(range(num_epochs), desc=\"Epochs\", position=len(stats_bars), leave=False)\nfor epoch in epoch_bar:\nfor key in metrics.keys():\nif key != \"Privacy\":\nmetrics[key].append(0.0)\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=len(stats_bars) + 1, leave=False):\nself.optimizer.zero_grad()\nlosses = self.loss(Y_subset.to(self.encoder.device))\nlosses[\"ELBO\"].backward()\nself.optimizer.step()\nfor key in metrics.keys():\nif key in losses:\nif losses[key]:\nmetrics[key][-1] += losses[key].item()\nelse:\nmetrics[key][-1] += 0.0\nfor key, stats_bar in stats_bars.items():\nif key == \"Privacy\" and privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar.set_description_str(\nf\"{(key + ':').ljust(max_length)}  \\u03B5 = {epsilon_e[0]:.2f}\\tbest \\u03B1 = {epsilon_e[1]:.2f}\"\n)\nmetrics[\"Privacy\"].append(epsilon_e)\nelse:\nstats_bar.set_description_str(f\"{(key + ':').ljust(max_length)}  {metrics[key][-1]:.2f}\")\nif epoch == 0:\nmin_elbo = metrics[\"ELBO\"][-1]\nif metrics[\"ELBO\"][-1] &lt; (min_elbo - delta):\nmin_elbo = metrics[\"ELBO\"][-1]\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nfor stats_bar in stats_bars.values():\nstats_bar.close()\ntqdm.write(f\"Completed {num_epochs} epochs in {time.time() - self.start_time:.2f} seconds.\")\nreturn (num_epochs, metrics)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.io","title":"<code>io</code>","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_prepared, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_prepared</code> <code>str</code> <p>The name of the prepared data file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_prepared: str, fn_metatransformer: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_prepared: The name of the prepared data file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_prepared, fn_metatransformer = consistent_endings([fn_dataset, fn_prepared, fn_metatransformer])\nfn_prepared, fn_metatransformer = potential_suffixes([fn_prepared, fn_metatransformer], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_prepared, fn_metatransformer], dir_experiment)\ncheck_exists([fn_prepared, fn_metatransformer], dir_experiment)\nreturn fn_dataset, fn_prepared, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.io.check_output_paths","title":"<code>check_output_paths(fn_dataset, fn_synthetic, fn_model, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>Path</code> <p>The base name of the dataset.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the synthetic data file.</p> required <code>fn_model</code> <code>str</code> <p>The name of the model file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The path to output the model.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_output_paths(fn_dataset: Path, fn_synthetic: str, fn_model: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_synthetic: The name of the synthetic data file.\n        fn_model: The name of the model file.\n        dir_experiment: The path to the experiment output directory.\n    Returns:\n        The path to output the model.\n    \"\"\"\nfn_synthetic, fn_model = consistent_endings([fn_synthetic, (fn_model, \".pt\")])\nfn_synthetic, fn_model = potential_suffixes([fn_synthetic, fn_model], fn_dataset)\nwarn_if_path_supplied([fn_synthetic, fn_model], dir_experiment)\nreturn fn_synthetic, fn_model\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/model/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"prepared_dataset\", \"metatransformer\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"prepared_dataset\"],\nargs.module_handover[\"metatransformer\"],\n)\nelse:\nfn_dataset, fn_prepared, fn_metatransformer = check_input_paths(\nargs.dataset, args.prepared, args.metatransformer, dir_experiment\n)\nwith open(dir_experiment / fn_prepared, \"rb\") as f:\ndata = pickle.load(f)\nwith open(dir_experiment / fn_metatransformer, \"rb\") as f:\nmt = pickle.load(f)\nreturn fn_dataset, data, mt\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.run","title":"<code>run</code>","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.run.run","title":"<code>run(args)</code>","text":"<p>Run the model architecture module.</p> Source code in <code>modules/model/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"Run the model architecture module.\"\"\"\nprint(\"Running model architecture module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\nfn_dataset, prepared_dataset, mt = load_required_data(args, dir_experiment)\nonehots, singles = mt.get_onehots_and_singles()\nnrows, ncols = prepared_dataset.shape\n# Should the data also all be turned into floats?\ntorch_data = TensorDataset(torch.Tensor(prepared_dataset.to_numpy()))\nsample_rate = args.batch_size / nrows\nmodel = VAE(\nEncoder(input_dim=ncols, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim, use_gpu=args.use_gpu),\nDecoder(args.latent_dim, onehots=onehots, singles=singles, use_gpu=args.use_gpu),\n)\nmodel.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\ndata_loader = DataLoader(\ntorch_data,\nbatch_sampler=UniformWithReplacementSampler(num_samples=nrows, sample_rate=sample_rate),\npin_memory=True,\n# batch_size=args.batch_size,\n)\nif not args.non_private_training:\nprivacy_engine = PrivacyEngine(\n# secure_rng=args.secure_rng,\nmodule=model,\nsample_rate=sample_rate,\nalphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\ntarget_epsilon=args.target_epsilon,\ntarget_delta=args.target_delta,\nepochs=args.num_epochs,\nmax_grad_norm=args.max_grad_norm,\n)\n# model.privacy_engine = PrivacyEngine(secure_mode=args.secure_rng)\n# model, optimizer, data_loader = model.privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=data_loader,\n#     epochs=args.num_epochs,\n#     target_epsilon=args.target_epsilon,\n#     target_delta=args.target_delta,\n#     max_grad_norm=args.max_grad_norm,\n# )\n# print(model)\n# print(f\"Using sigma={optimizer.noise_multiplier} and C={args.max_grad_norm}\")\nnum_epochs, results = model.train(\ndata_loader, args.num_epochs, args.tracked_metrics, privacy_engine=privacy_engine\n)\nelse:\nnum_epochs, results = model.train(data_loader, args.num_epochs, args.tracked_metrics)\nsynthetic = pd.DataFrame(model.generate(nrows), columns=prepared_dataset.columns)\nsynthetic = mt.inverse_apply(synthetic)\nfn_output, fn_model = check_output_paths(fn_dataset, args.synthetic, args.model_file, dir_experiment)\nsynthetic.to_pickle(dir_experiment / fn_output)\nsynthetic.to_csv(dir_experiment / (fn_output[:-3] + \"csv\"), index=False)\nmodel.save(dir_experiment / fn_model)\nif \"evaluation\" in args.modules_to_run:\nargs.module_handover.update({\"fn_dataset\": fn_dataset, \"synthetic\": synthetic})\nif \"plotting\" in args.modules_to_run:\nargs.module_handover.update({\"results\": results, \"num_epochs\": num_epochs})\nprint(\"\")\nreturn args\n</code></pre>"},{"location":"reference/modules/model/DPVAE/","title":"DPVAE","text":""},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\ndef __init__(\nself,\nlatent_dim,\nonehots=[[]],\nsingles=[],\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = len(singles) + sum([len(x) for x in onehots])\nself.singles = singles\nself.onehots = onehots\nself.net = nn.Sequential(\nnn.Linear(latent_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nlatent_dim,\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = 2 * latent_dim\nself.latent_dim = latent_dim\nself.net = nn.Sequential(\nnn.Linear(input_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\ndef __init__(self, num_singles):\nsuper().__init__()\nself.output_logsigma_fn = nn.Linear(num_singles, num_singles, bias=True)\ntorch.nn.init.zeros_(self.output_logsigma_fn.weight)\ntorch.nn.init.zeros_(self.output_logsigma_fn.bias)\nself.output_logsigma_fn.weight.requires_grad = False\ndef forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder.to(encoder.device)\nself.decoder = decoder.to(decoder.device)\nself.device = encoder.device\nself.onehots = self.decoder.onehots\nself.singles = self.decoder.singles\nself.noiser = Noiser(len(self.singles)).to(decoder.device)\ndef reconstruct(self, X):\nmu_z, logsigma_z = self.encoder(X)\nx_recon = self.decoder(mu_z)\nreturn x_recon\ndef generate(self, N):\nz_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\nx_gen = self.decoder(z_samples)\nx_gen_ = torch.ones_like(x_gen, device=self.device)\nfor cat_idxs in self.onehots:\nx_gen_[:, cat_idxs] = torch.distributions.one_hot_categorical.OneHotCategorical(\nlogits=x_gen[:, cat_idxs]\n).sample()\nx_gen_[:, self.singles] = x_gen[:, self.singles] + torch.exp(\nself.noiser(x_gen[:, self.singles])\n) * torch.randn_like(x_gen[:, self.singles])\nif torch.cuda.is_available():\nx_gen_ = x_gen_.cpu()\nreturn x_gen_.detach()\ndef loss(self, X):\nmu_z, logsigma_z = self.encoder(X)\np = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\nq = Normal(mu_z, torch.exp(logsigma_z))\nkld = torch.sum(torch.distributions.kl_divergence(q, p))\ns = torch.randn_like(mu_z)\nz_samples = mu_z + s * torch.exp(logsigma_z)\nx_recon = self.decoder(z_samples)\ncategoric_loglik = 0\nif len(self.onehots):\nfor cat_idxs in self.onehots:\ncategoric_loglik += -torch.nn.functional.cross_entropy(\nx_recon[:, cat_idxs],\ntorch.max(X[:, cat_idxs], 1)[1],\n).sum()\ngauss_loglik = 0\nif len(self.singles):\ngauss_loglik = (\nNormal(\nloc=x_recon[:, self.singles],\nscale=torch.exp(self.noiser(x_recon[:, self.singles])),\n)\n.log_prob(X[:, self.singles])\n.sum()\n)\nreconstruction_loss = -(categoric_loglik + gauss_loglik)\nelbo = kld + reconstruction_loss\nreturn {\n\"ELBO\": elbo,\n\"ReconstructionLoss\": reconstruction_loss,\n\"KLD\": kld,\n\"CategoricalLoss\": categoric_loglik,\n\"NumericalLoss\": gauss_loglik,\n}\ndef train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\ntracked_metrics: list[str] = [\"ELBO\"],\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nprint(\"\")\nself.start_time = time.time()\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\nelif \"Privacy\" in tracked_metrics:\ntracked_metrics.remove(\"Privacy\")\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nmetrics = {metric: [] for metric in tracked_metrics}\nstats_bars = {\nmetric: tqdm(total=0, desc=\"\", position=i, bar_format=\"{desc}\", leave=True)\nfor i, metric in enumerate(tracked_metrics)\n}\nmax_length = max(len(s) for s in tracked_metrics) + 1\nepoch_bar = tqdm(range(num_epochs), desc=\"Epochs\", position=len(stats_bars), leave=False)\nfor epoch in epoch_bar:\nfor key in metrics.keys():\nif key != \"Privacy\":\nmetrics[key].append(0.0)\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=len(stats_bars) + 1, leave=False):\nself.optimizer.zero_grad()\nlosses = self.loss(Y_subset.to(self.encoder.device))\nlosses[\"ELBO\"].backward()\nself.optimizer.step()\nfor key in metrics.keys():\nif key in losses:\nif losses[key]:\nmetrics[key][-1] += losses[key].item()\nelse:\nmetrics[key][-1] += 0.0\nfor key, stats_bar in stats_bars.items():\nif key == \"Privacy\" and privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar.set_description_str(\nf\"{(key + ':').ljust(max_length)}  \\u03B5 = {epsilon_e[0]:.2f}\\tbest \\u03B1 = {epsilon_e[1]:.2f}\"\n)\nmetrics[\"Privacy\"].append(epsilon_e)\nelse:\nstats_bar.set_description_str(f\"{(key + ':').ljust(max_length)}  {metrics[key][-1]:.2f}\")\nif epoch == 0:\nmin_elbo = metrics[\"ELBO\"][-1]\nif metrics[\"ELBO\"][-1] &lt; (min_elbo - delta):\nmin_elbo = metrics[\"ELBO\"][-1]\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nfor stats_bar in stats_bars.values():\nstats_bar.close()\ntqdm.write(f\"Completed {num_epochs} epochs in {time.time() - self.start_time:.2f} seconds.\")\nreturn (num_epochs, metrics)\ndef get_privacy_spent(self, delta):\nif hasattr(self, \"privacy_engine\"):\nreturn self.privacy_engine.get_privacy_spent(delta)\nelse:\nprint(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n)\ndef save(self, filename):\ntorch.save(self.state_dict(), filename)\ndef load(self, filename):\nself.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, num_epochs, tracked_metrics=['ELBO'], privacy_engine=None, patience=5, delta=10)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\ntracked_metrics: list[str] = [\"ELBO\"],\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nprint(\"\")\nself.start_time = time.time()\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\nelif \"Privacy\" in tracked_metrics:\ntracked_metrics.remove(\"Privacy\")\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nmetrics = {metric: [] for metric in tracked_metrics}\nstats_bars = {\nmetric: tqdm(total=0, desc=\"\", position=i, bar_format=\"{desc}\", leave=True)\nfor i, metric in enumerate(tracked_metrics)\n}\nmax_length = max(len(s) for s in tracked_metrics) + 1\nepoch_bar = tqdm(range(num_epochs), desc=\"Epochs\", position=len(stats_bars), leave=False)\nfor epoch in epoch_bar:\nfor key in metrics.keys():\nif key != \"Privacy\":\nmetrics[key].append(0.0)\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=len(stats_bars) + 1, leave=False):\nself.optimizer.zero_grad()\nlosses = self.loss(Y_subset.to(self.encoder.device))\nlosses[\"ELBO\"].backward()\nself.optimizer.step()\nfor key in metrics.keys():\nif key in losses:\nif losses[key]:\nmetrics[key][-1] += losses[key].item()\nelse:\nmetrics[key][-1] += 0.0\nfor key, stats_bar in stats_bars.items():\nif key == \"Privacy\" and privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar.set_description_str(\nf\"{(key + ':').ljust(max_length)}  \\u03B5 = {epsilon_e[0]:.2f}\\tbest \\u03B1 = {epsilon_e[1]:.2f}\"\n)\nmetrics[\"Privacy\"].append(epsilon_e)\nelse:\nstats_bar.set_description_str(f\"{(key + ':').ljust(max_length)}  {metrics[key][-1]:.2f}\")\nif epoch == 0:\nmin_elbo = metrics[\"ELBO\"][-1]\nif metrics[\"ELBO\"][-1] &lt; (min_elbo - delta):\nmin_elbo = metrics[\"ELBO\"][-1]\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nfor stats_bar in stats_bars.values():\nstats_bar.close()\ntqdm.write(f\"Completed {num_epochs} epochs in {time.time() - self.start_time:.2f} seconds.\")\nreturn (num_epochs, metrics)\n</code></pre>"},{"location":"reference/modules/model/io/","title":"io","text":""},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_prepared, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_prepared</code> <code>str</code> <p>The name of the prepared data file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_prepared: str, fn_metatransformer: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_prepared: The name of the prepared data file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_prepared, fn_metatransformer = consistent_endings([fn_dataset, fn_prepared, fn_metatransformer])\nfn_prepared, fn_metatransformer = potential_suffixes([fn_prepared, fn_metatransformer], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_prepared, fn_metatransformer], dir_experiment)\ncheck_exists([fn_prepared, fn_metatransformer], dir_experiment)\nreturn fn_dataset, fn_prepared, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.check_output_paths","title":"<code>check_output_paths(fn_dataset, fn_synthetic, fn_model, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>Path</code> <p>The base name of the dataset.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the synthetic data file.</p> required <code>fn_model</code> <code>str</code> <p>The name of the model file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The path to output the model.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_output_paths(fn_dataset: Path, fn_synthetic: str, fn_model: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_synthetic: The name of the synthetic data file.\n        fn_model: The name of the model file.\n        dir_experiment: The path to the experiment output directory.\n    Returns:\n        The path to output the model.\n    \"\"\"\nfn_synthetic, fn_model = consistent_endings([fn_synthetic, (fn_model, \".pt\")])\nfn_synthetic, fn_model = potential_suffixes([fn_synthetic, fn_model], fn_dataset)\nwarn_if_path_supplied([fn_synthetic, fn_model], dir_experiment)\nreturn fn_synthetic, fn_model\n</code></pre>"},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/model/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"prepared_dataset\", \"metatransformer\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"prepared_dataset\"],\nargs.module_handover[\"metatransformer\"],\n)\nelse:\nfn_dataset, fn_prepared, fn_metatransformer = check_input_paths(\nargs.dataset, args.prepared, args.metatransformer, dir_experiment\n)\nwith open(dir_experiment / fn_prepared, \"rb\") as f:\ndata = pickle.load(f)\nwith open(dir_experiment / fn_metatransformer, \"rb\") as f:\nmt = pickle.load(f)\nreturn fn_dataset, data, mt\n</code></pre>"},{"location":"reference/modules/model/run/","title":"run","text":""},{"location":"reference/modules/model/run/#nhssynth.modules.model.run.run","title":"<code>run(args)</code>","text":"<p>Run the model architecture module.</p> Source code in <code>modules/model/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"Run the model architecture module.\"\"\"\nprint(\"Running model architecture module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\nfn_dataset, prepared_dataset, mt = load_required_data(args, dir_experiment)\nonehots, singles = mt.get_onehots_and_singles()\nnrows, ncols = prepared_dataset.shape\n# Should the data also all be turned into floats?\ntorch_data = TensorDataset(torch.Tensor(prepared_dataset.to_numpy()))\nsample_rate = args.batch_size / nrows\nmodel = VAE(\nEncoder(input_dim=ncols, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim, use_gpu=args.use_gpu),\nDecoder(args.latent_dim, onehots=onehots, singles=singles, use_gpu=args.use_gpu),\n)\nmodel.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\ndata_loader = DataLoader(\ntorch_data,\nbatch_sampler=UniformWithReplacementSampler(num_samples=nrows, sample_rate=sample_rate),\npin_memory=True,\n# batch_size=args.batch_size,\n)\nif not args.non_private_training:\nprivacy_engine = PrivacyEngine(\n# secure_rng=args.secure_rng,\nmodule=model,\nsample_rate=sample_rate,\nalphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\ntarget_epsilon=args.target_epsilon,\ntarget_delta=args.target_delta,\nepochs=args.num_epochs,\nmax_grad_norm=args.max_grad_norm,\n)\n# model.privacy_engine = PrivacyEngine(secure_mode=args.secure_rng)\n# model, optimizer, data_loader = model.privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=data_loader,\n#     epochs=args.num_epochs,\n#     target_epsilon=args.target_epsilon,\n#     target_delta=args.target_delta,\n#     max_grad_norm=args.max_grad_norm,\n# )\n# print(model)\n# print(f\"Using sigma={optimizer.noise_multiplier} and C={args.max_grad_norm}\")\nnum_epochs, results = model.train(\ndata_loader, args.num_epochs, args.tracked_metrics, privacy_engine=privacy_engine\n)\nelse:\nnum_epochs, results = model.train(data_loader, args.num_epochs, args.tracked_metrics)\nsynthetic = pd.DataFrame(model.generate(nrows), columns=prepared_dataset.columns)\nsynthetic = mt.inverse_apply(synthetic)\nfn_output, fn_model = check_output_paths(fn_dataset, args.synthetic, args.model_file, dir_experiment)\nsynthetic.to_pickle(dir_experiment / fn_output)\nsynthetic.to_csv(dir_experiment / (fn_output[:-3] + \"csv\"), index=False)\nmodel.save(dir_experiment / fn_model)\nif \"evaluation\" in args.modules_to_run:\nargs.module_handover.update({\"fn_dataset\": fn_dataset, \"synthetic\": synthetic})\nif \"plotting\" in args.modules_to_run:\nargs.module_handover.update({\"results\": results, \"num_epochs\": num_epochs})\nprint(\"\")\nreturn args\n</code></pre>"},{"location":"reference/modules/plotting/","title":"plotting","text":""},{"location":"reference/modules/plotting/#nhssynth.modules.plotting.io","title":"<code>io</code>","text":""},{"location":"reference/modules/plotting/#nhssynth.modules.plotting.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_synthetic, fn_report, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed data file.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>fn_report</code> <code>str</code> <p>The name of the report file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/plotting/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_typed: str, fn_synthetic: str, fn_report: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed data file.\n        fn_synthetic: The name of the metatransformer file.\n        fn_report: The name of the report file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_typed, fn_synthetic, fn_report = consistent_endings([fn_dataset, fn_typed, fn_synthetic, fn_report])\nfn_typed, fn_synthetic, fn_report = potential_suffixes([fn_typed, fn_synthetic, fn_report], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_typed, fn_synthetic, fn_report], dir_experiment)\ncheck_exists([fn_typed, fn_synthetic], dir_experiment)\nreturn fn_dataset, fn_typed, fn_synthetic, fn_report\n</code></pre>"},{"location":"reference/modules/plotting/#nhssynth.modules.plotting.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/plotting/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"typed_dataset\", \"synthetic\", \"report\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"typed_dataset\"],\nargs.module_handover[\"synthetic\"],\nargs.module_handover[\"report\"],\n)\nelse:\nfn_dataset, fn_typed, fn_synthetic, fn_report = check_input_paths(\nargs.dataset, args.typed, args.synthetic, args.report, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f)\nwith open(dir_experiment / fn_synthetic, \"rb\") as f:\nsynthetic_data = pickle.load(f)\nif (dir_experiment / fn_report).exists():\nwith open(dir_experiment / fn_report, \"rb\") as f:\nreport = pickle.load(f)\nelse:\nreport = None\nreturn fn_dataset, real_data, synthetic_data, report\n</code></pre>"},{"location":"reference/modules/plotting/#nhssynth.modules.plotting.plots","title":"<code>plots</code>","text":""},{"location":"reference/modules/plotting/#nhssynth.modules.plotting.plots.factorize_all_categoricals","title":"<code>factorize_all_categoricals(df)</code>","text":"<p>Factorize all categorical columns in a dataframe.</p> Source code in <code>modules/plotting/plots.py</code> <pre><code>def factorize_all_categoricals(\ndf: pd.DataFrame,\n) -&gt; pd.DataFrame:\n\"\"\"Factorize all categorical columns in a dataframe.\"\"\"\nfor col in df.columns:\nif df[col].dtype == \"object\":\ndf[col] = pd.factorize(df[col])[0]\nelif df[col].dtype == \"datetime64[ns]\":\ndf[col] = pd.to_numeric(df[col])\nmin_val = df[col].min()\nmax_val = df[col].max()\ndf[col] = (df[col] - min_val) / (max_val - min_val)\nreturn df\n</code></pre>"},{"location":"reference/modules/plotting/io/","title":"io","text":""},{"location":"reference/modules/plotting/io/#nhssynth.modules.plotting.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_synthetic, fn_report, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed data file.</p> required <code>fn_synthetic</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>fn_report</code> <code>str</code> <p>The name of the report file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/plotting/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_typed: str, fn_synthetic: str, fn_report: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed data file.\n        fn_synthetic: The name of the metatransformer file.\n        fn_report: The name of the report file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_typed, fn_synthetic, fn_report = consistent_endings([fn_dataset, fn_typed, fn_synthetic, fn_report])\nfn_typed, fn_synthetic, fn_report = potential_suffixes([fn_typed, fn_synthetic, fn_report], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_typed, fn_synthetic, fn_report], dir_experiment)\ncheck_exists([fn_typed, fn_synthetic], dir_experiment)\nreturn fn_dataset, fn_typed, fn_synthetic, fn_report\n</code></pre>"},{"location":"reference/modules/plotting/io/#nhssynth.modules.plotting.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/plotting/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"fn_dataset\", \"typed_dataset\", \"synthetic\", \"report\"]):\nreturn (\nargs.module_handover[\"fn_dataset\"],\nargs.module_handover[\"typed_dataset\"],\nargs.module_handover[\"synthetic\"],\nargs.module_handover[\"report\"],\n)\nelse:\nfn_dataset, fn_typed, fn_synthetic, fn_report = check_input_paths(\nargs.dataset, args.typed, args.synthetic, args.report, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f)\nwith open(dir_experiment / fn_synthetic, \"rb\") as f:\nsynthetic_data = pickle.load(f)\nif (dir_experiment / fn_report).exists():\nwith open(dir_experiment / fn_report, \"rb\") as f:\nreport = pickle.load(f)\nelse:\nreport = None\nreturn fn_dataset, real_data, synthetic_data, report\n</code></pre>"},{"location":"reference/modules/plotting/plots/","title":"plots","text":""},{"location":"reference/modules/plotting/plots/#nhssynth.modules.plotting.plots.factorize_all_categoricals","title":"<code>factorize_all_categoricals(df)</code>","text":"<p>Factorize all categorical columns in a dataframe.</p> Source code in <code>modules/plotting/plots.py</code> <pre><code>def factorize_all_categoricals(\ndf: pd.DataFrame,\n) -&gt; pd.DataFrame:\n\"\"\"Factorize all categorical columns in a dataframe.\"\"\"\nfor col in df.columns:\nif df[col].dtype == \"object\":\ndf[col] = pd.factorize(df[col])[0]\nelif df[col].dtype == \"datetime64[ns]\":\ndf[col] = pd.to_numeric(df[col])\nmin_val = df[col].min()\nmax_val = df[col].max()\ndf[col] = (df[col] - min_val) / (max_val - min_val)\nreturn df\n</code></pre>"},{"location":"reference/modules/plotting/run/","title":"run","text":""},{"location":"reference/modules/structure/","title":"structure","text":""},{"location":"reference/modules/structure/run/","title":"run","text":""}]}
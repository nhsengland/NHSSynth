{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NHS Synth","text":"<p>Under construction, see the Code Reference or Model Card.</p>"},{"location":"model_card/","title":"Model Card: Variational AutoEncoder with Differential Privacy","text":""},{"location":"model_card/#model-details","title":"Model Details","text":"<p>The implementation of the Variational AutoEncoder (VAE) with Differential Privacy within this repository is based on work done by Dominic Danks during an NHSX Analytics Unit PhD internship (last commit to the original SynthVAE repository: commit 88a4bdf). This model card describes an updated and extended version of the model, by Harrison Wilde. Further information about the previous version created by Dom and its model implementation can be found in Section 5.4 of the associated report.</p>"},{"location":"model_card/#model-use","title":"Model Use","text":""},{"location":"model_card/#intended-use","title":"Intended Use","text":"<p>This model is intended for use in experimenting with differential privacy and VAEs.</p>"},{"location":"model_card/#training-data","title":"Training Data","text":"<p>Experiments in this repository are run against the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) dataset accessed via the pycox python library. We also performed further analysis on a single table that we extracted from MIMIC-III.</p>"},{"location":"model_card/#performance-and-limitations","title":"Performance and Limitations","text":"<p>A from-scratch VAE implementation was compared against various models available within the SDV framework using a variety of quality and privacy metrics on the SUPPORT dataset. The VAE was found to be competitive with all of these models across the various metrics. Differential Privacy (DP) was introduced via DP-SGD and the performance of the VAE for different levels of privacy was evaluated. It was found that as the level of Differential Privacy introduced by DP-SGD was increased, it became easier to distinguish between synthetic and real data.</p> <p>Proper evaluation of quality and privacy of synthetic data is challenging. In this work, we utilised metrics from the SDV library due to their natural integration with the rest of the codebase. A valuable extension of this work would be to apply a variety of external metrics, including more advanced adversarial attacks to more thoroughly evaluate the privacy of the considered methods, including as the level of DP is varied. It would also be of interest to apply DP-SGD and/or PATE to all of the considered methods and evaluate whether the performance drop as a function of implemented privacy is similar or different across the models.</p> <p>Currently the SynthVAE model only works for data which is 'clean'. I.e data that has no missingness or NaNs within its input. It can handle continuous, categorical and datetime variables. Special types such as nominal data cannot be handled properly however the model may still run. Column names have to be specified in the code for the variable group they belong to.</p> <p>Hyperparameter tuning of the model can result in errors if certain parameter values are selected. Most commonly, changing learning rate in our example results in errors during training. An extensive test to evaluate plausible ranges has not been performed as of yet. If you get errors during tuning then consider your hyperparameter values and adjust accordingly.</p>"},{"location":"model_card/#acknowledgements","title":"Acknowledgements","text":"<p>This documentation is inspired by Model Cards for Model Reporting (Mitchell et al.) and Lessons from Archives (Jo &amp; Gebru).</p>"},{"location":"modules/","title":"Modules","text":"<p>This folder contains all of the modules contained in this package. They can be used together or independently - through importing them into your existing codebase or using the CLI to select which / all modules to run.</p>"},{"location":"modules/#importing-a-module-from-this-package","title":"Importing a module from this package","text":"<p>After installing the package, you can simply do: <pre><code>from nhssynth.modules import &lt;module&gt;\n</code></pre> and you will be able to use it in your code!</p>"},{"location":"modules/#creating-a-new-module-and-folding-it-into-the-cli","title":"Creating a new module and folding it into the CLI","text":"<p>The following instructions specify how to extend this package with a new module:</p> <ol> <li>Create a folder for your module within the package, i.e. <code>src/nhssynth/modules/mymodule</code></li> <li> <p>Include within it a main executor function that accepts arguments from the CLI, i.e.</p> <pre><code>def myexecutor(args):\n...\n</code></pre> <p>In <code>mymodule/executor.py</code> and export it by adding <code>from .executor import myexecutor</code> to <code>mymodule/__init__.py</code>.</p> </li> <li> <p>In the <code>cli</code> folder, add a corresponding function to <code>arguments.py</code> and populate with arguments you want to expose in the CLI:</p> <pre><code>def add_mymodule_args(parser: argparse.ArgumentParser, group_title: str, overrides=False):\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(...)\ngroup.add_argument(...)\n...\n</code></pre> </li> <li> <p>Next, in <code>module_setup.py</code> make the following adjustments the following code:</p> <pre><code>from nhssynth.modules import ..., mymodule, ...\n</code></pre> <pre><code>MODULE_MAP = {\n...\n\"mymodule\": ModuleConfig(\nfunc=mymodule.myexecutor,\nadd_args=add_mymodule_args,\ndescription=\"...\",\nhelp=\"...\",\ncommon_parsers=[...]\n),\n...\n}\n</code></pre> <p>Where <code>common_parsers</code> is a subset of <code>COMMON_PARSERS</code> defined in <code>common_arguments.py</code>. Note that the \"dataset\" and \"core\" parsers are added automatically, so you don't need to specify them. These parsers can be used to add arguments to your module that are common to multiple modules, e.g. the <code>dataloader</code> and <code>evaluation</code> modules both use <code>--typed</code> to specify the path of the typed input dataset.</p> </li> <li> <p>You can (optionally) also edit the following block if you want your module to be included in a full pipeline run:</p> <pre><code>PIPELINE = [..., mymodule, ...]  # NOTE this determines the order of a pipeline run\n</code></pre> </li> <li> <p>Congrats, your module is implemented!</p> </li> </ol>"},{"location":"secure_mode/","title":"Opacus' Secure Mode","text":"<p>Part of the process for achieving a differential privacy guarantee under Opacus involves generating noise according to a Gaussian distribution with mean 0 in Opacus' <code>_generate_noise()</code> function.</p> <p>Enabling <code>secure_mode</code> when using the NHSSynth package ensures that the generated noise is also secure against floating point representation attacks, such as the ones in https://arxiv.org/abs/2107.10138 and https://arxiv.org/abs/2112.05307.</p> <p>This attack first appeared in https://arxiv.org/abs/2112.05307; the fix via the <code>csprng</code> package is based on https://arxiv.org/abs/2107.10138 and involves calling the Gaussian noise function $2n$ times, where $n=2$ (see section 5.1 in https://arxiv.org/abs/2107.10138).</p> <p>The reason for choosing $n=2$ is that $n$ can be any number greater than $1$. The bigger $n$ is, though, the more computation needs to be done to generate the Gaussian samples. The choice of $n=2$ is justified via the knowledge that the attack has a complexity of $2^{p(2n-1)}$. In PyTorch, $p=53$ and so the complexity is $2^159$, which is deemed sufficiently hard for an attacker to break.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli<ul> <li>common_arguments</li> <li>config</li> <li>model_arguments</li> <li>module_arguments</li> <li>module_setup</li> <li>run</li> </ul> </li> <li>common<ul> <li>common</li> <li>constants</li> <li>debugging</li> <li>dicts</li> <li>io</li> <li>strings</li> </ul> </li> <li>modules<ul> <li>dashboard<ul> <li>Upload</li> <li>io</li> <li>pages<ul> <li>1_Tables</li> <li>2_Plots</li> <li>3_Experiment_Configurations</li> </ul> </li> <li>run</li> <li>utils</li> </ul> </li> <li>dataloader<ul> <li>constraints</li> <li>io</li> <li>metadata</li> <li>metatransformer</li> <li>missingness</li> <li>run</li> <li>transformers<ul> <li>base</li> <li>categorical</li> <li>continuous</li> <li>datetime</li> </ul> </li> </ul> </li> <li>evaluation<ul> <li>aequitas</li> <li>io</li> <li>metrics</li> <li>run</li> <li>tasks</li> <li>utils</li> </ul> </li> <li>model<ul> <li>common<ul> <li>dp</li> <li>model</li> </ul> </li> <li>io</li> <li>models<ul> <li>dpvae</li> <li>vae</li> </ul> </li> <li>run</li> <li>utils</li> </ul> </li> <li>plotting<ul> <li>io</li> <li>plots</li> <li>run</li> </ul> </li> <li>structure<ul> <li>run</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/cli/","title":"cli","text":""},{"location":"reference/cli/common_arguments/","title":"common_arguments","text":"<p>Functions to define the CLI's \"common\" arguments, i.e. those that can be applied to either:  - All module argument lists, e.g. --dataset, --seed, etc.  - A subset of module(s) argument lists, e.g. --synthetic, --typed, etc.</p>"},{"location":"reference/cli/common_arguments/#nhssynth.cli.common_arguments.get_core_parser","title":"<code>get_core_parser(overrides=False)</code>","text":"<p>Create the core common parser group applied to all modules (and the <code>pipeline</code> and <code>config</code> options). Note that we leverage common titling of the argument group to ensure arguments appear together even if declared separately.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <p>whether the arguments declared within are required or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The parser with the group containing the core arguments attached.</p> Source code in <code>src/nhssynth/cli/common_arguments.py</code> <pre><code>def get_core_parser(overrides=False) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Create the core common parser group applied to all modules (and the `pipeline` and `config` options).\n    Note that we leverage common titling of the argument group to ensure arguments appear together even if declared separately.\n    Args:\n        overrides: whether the arguments declared within are required or not.\n    Returns:\n        The parser with the group containing the core arguments attached.\n    \"\"\"\n\"\"\"\"\"\"\ncore = argparse.ArgumentParser(add_help=False)\ncore_grp = core.add_argument_group(title=\"options\")\ncore_grp.add_argument(\n\"-d\",\n\"--dataset\",\nrequired=(not overrides),\ntype=str,\nhelp=\"the name of the dataset to experiment with, should be present in `&lt;DATA_DIR&gt;`\",\n)\ncore_grp.add_argument(\n\"-e\",\n\"--experiment-name\",\ntype=str,\ndefault=TIME,\nhelp=f\"name the experiment run to affect logging, config, and default-behaviour i/o\",\n)\ncore_grp.add_argument(\n\"--save-config\",\naction=\"store_true\",\nhelp=\"save the config provided via the cli, this is a recommended option for reproducibility\",\n)\nreturn core\n</code></pre>"},{"location":"reference/cli/common_arguments/#nhssynth.cli.common_arguments.get_seed_parser","title":"<code>get_seed_parser(overrides=False)</code>","text":"<p>Create the common parser group for the seed. NB This is separate to the rest of the core arguments as it does not apply to the dashboard module.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <p>whether the arguments declared within are required or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The parser with the group containing the seed argument attached.</p> Source code in <code>src/nhssynth/cli/common_arguments.py</code> <pre><code>def get_seed_parser(overrides=False) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Create the common parser group for the seed.\n    NB This is separate to the rest of the core arguments as it does not apply to the dashboard module.\n    Args:\n        overrides: whether the arguments declared within are required or not.\n    Returns:\n        The parser with the group containing the seed argument attached.\n    \"\"\"\nparser = argparse.ArgumentParser(add_help=False)\nparser_grp = parser.add_argument_group(title=\"options\")\nparser_grp.add_argument(\n\"-s\",\n\"--seed\",\ntype=int,\nhelp=\"specify a seed for reproducibility, this is a recommended option for reproducibility\",\n)\nreturn parser\n</code></pre>"},{"location":"reference/cli/common_arguments/#nhssynth.cli.common_arguments.suffix_parser_generator","title":"<code>suffix_parser_generator(name, help, required=False)</code>","text":"<p>Generator function for creating parsers following a common template. These parsers are all suffixes to the --dataset / -d / DATASET argument, see <code>COMMON_TITLE</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name / label of the argument to add to the CLI options.</p> required <code>help</code> <code>str</code> <p>the help message when the CLI is run with --help / -h.</p> required <code>required</code> <code>bool</code> <p>whether the argument must be provided or not.</p> <code>False</code> Source code in <code>src/nhssynth/cli/common_arguments.py</code> <pre><code>def suffix_parser_generator(name: str, help: str, required: bool = False) -&gt; argparse.ArgumentParser:\n\"\"\"Generator function for creating parsers following a common template.\n    These parsers are all suffixes to the --dataset / -d / DATASET argument, see `COMMON_TITLE`.\n    Args:\n        name: the name / label of the argument to add to the CLI options.\n        help: the help message when the CLI is run with --help / -h.\n        required: whether the argument must be provided or not.\n    \"\"\"\ndef get_parser(overrides: bool = False) -&gt; argparse.ArgumentParser:\nparser = argparse.ArgumentParser(add_help=False)\nparser_grp = parser.add_argument_group(title=COMMON_TITLE)\nparser_grp.add_argument(\nf\"--{name.replace('_', '-')}\",\nrequired=required and not overrides,\ntype=str,\ndefault=f\"_{name}\",\nhelp=help,\n)\nreturn parser\nreturn get_parser\n</code></pre>"},{"location":"reference/cli/config/","title":"config","text":"<p>Read, write and process config files, including handling of module-specific / common config overrides.</p>"},{"location":"reference/cli/config/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a nested-via-module configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict[str, ArgumentParser]</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def assemble_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a nested-via-module configuration dictionary from parsed command-line arguments to be output as a YAML record.\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\nargs_dict = vars(args)\n# Filter out the keys that are not relevant to the config file\nargs_dict = filter_dict(\nargs_dict, {\"func\", \"experiment_name\", \"save_config\", \"save_config_path\", \"module_handover\"}\n)\nfor k in args_dict.copy().keys():\n# Remove empty metric lists from the config\nif \"_metrics\" in k and not args_dict[k]:\nargs_dict.pop(k)\nmodules_to_run = args_dict.pop(\"modules_to_run\")\nif len(modules_to_run) == 1:\nrun_type = modules_to_run[0]\nelif modules_to_run == PIPELINE:\nrun_type = \"pipeline\"\nelse:\nraise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n# Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\nmodule_args = {\nmodule_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\nfor module_name in modules_to_run\n}\n# Use the flat namespace to populate a nested (by module) dictionary of config args and values\nout_dict = {}\nfor module_name in modules_to_run:\nfor k in args_dict.copy().keys():\n# We want to keep dataset, experiment_name, seed and save_config at the top-level as they are core args\nif k in module_args[module_name] and k not in {\n\"version\",\n\"dataset\",\n\"experiment_name\",\n\"seed\",\n\"save_config\",\n}:\nif module_name not in out_dict:\nout_dict[module_name] = {}\nv = args_dict.pop(k)\nif v is not None:\nout_dict[module_name][k] = v\n# Assemble the final dictionary in YAML-compliant form\nreturn {**({\"run_type\": run_type} if run_type else {}), **args_dict, **out_dict}\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>ArgumentParser</code> <p>The top-level parser (contains common arguments).</p> required <code>module_parsers</code> <code>dict[str, ArgumentParser]</code> <p>The dict of module-level parsers mapped to their names.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], list[str]]</code> <p>A tuple containing two elements: - A dictionary containing all arguments and their default values. - A list of key-value-pairs of the required arguments and their associated module.</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def get_default_and_required_args(\ntop_parser: argparse.ArgumentParser,\nmodule_parsers: dict[str, argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n    Args:\n        top_parser: The top-level parser (contains common arguments).\n        module_parsers: The dict of module-level parsers mapped to their names.\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of key-value-pairs of the required arguments and their associated module.\n    \"\"\"\nall_actions = {\"top-level\": top_parser._actions} | {m: p._actions for m, p in module_parsers.items()}\ndefaults = {}\nrequired_args = []\nfor module, actions in all_actions.items():\nfor action in actions:\nif action.dest not in [\"help\", \"==SUPPRESS==\"]:\ndefaults[action.dest] = action.default\nif action.required:\nrequired_args.append({\"arg\": action.dest, \"module\": module})\nreturn defaults, required_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_modules_to_run","title":"<code>get_modules_to_run(executor)</code>","text":"<p>Get the list of modules to run from the passed executor function.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Callable</code> <p>The executor function to run.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of module names to run.</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def get_modules_to_run(executor: Callable) -&gt; list[str]:\n\"\"\"\n    Get the list of modules to run from the passed executor function.\n    Args:\n        executor: The executor function to run.\n    Returns:\n        A list of module names to run.\n    \"\"\"\nif executor == run_pipeline:\nreturn PIPELINE\nelse:\nreturn [get_key_by_value({mn: mc.func for mn, mc in MODULE_MAP.items()}, executor)]\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config <code>argparse.Namespace</code> object for the inferred modules to run and execute, given a file.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default configuration set of arguments from the <code>parser</code> and <code>all_subparsers</code> for the determined <code>modules_to_run</code></li> <li>Overwrite these with the specified (sub)set of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting configuration <code>Namespace</code> object</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>ArgumentParser</code> <p>top-level <code>ArgumentParser</code> object containing common arguments</p> required <code>all_subparsers</code> <code>dict[str, ArgumentParser]</code> <p>dictionary of <code>ArgumentParser</code> objects, one for each module</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>A Namespace object containing the assembled configuration settings</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file / overrides</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def read_config(\nargs: argparse.Namespace,\nparser: argparse.ArgumentParser,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config `argparse.Namespace` object for the inferred modules to run and execute, given a file.\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default configuration set of arguments from the `parser` and `all_subparsers` for the determined `modules_to_run`\n    4. Overwrite these with the specified (sub)set of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting configuration `Namespace` object\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level `ArgumentParser` object containing common arguments\n        all_subparsers: dictionary of `ArgumentParser` objects, one for each module\n    Returns:\n        A Namespace object containing the assembled configuration settings\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file / overrides\n    \"\"\"\n# Open the passed yaml file and load into a dictionary\nwith open(f\"config/{args.input_config}.yaml\") as stream:\nconfig_dict = yaml.safe_load(stream)\nvalid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\nversion = config_dict.pop(\"version\", None)\nif version and version != version(\"nhssynth\"):\nwarnings.warn(\nf\"This config file's specified version ({version}) does not match the currently installed version of nhssynth ({version('nhssynth')}), results may differ.\"\n)\nelif not version:\nversion = ver(\"nhssynth\")\nrun_type = config_dict.pop(\"run_type\", None)\nif run_type == \"pipeline\":\nmodules_to_run = PIPELINE\nelse:\nmodules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\nif not args.custom_pipeline:\nmodules_to_run = sorted(modules_to_run, key=lambda x: PIPELINE.index(x))\nif not modules_to_run:\nwarnings.warn(\n\"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n)\nmodules_to_run = PIPELINE\n# Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\nargs_dict, required_args = get_default_and_required_args(\nparser, filter_dict(all_subparsers, modules_to_run, include=True)\n)\n# Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\nnon_default_passed_args_dict = {\nk: v\nfor k, v in vars(args).items()\nif k in [\"input_config\", \"custom_pipeline\"] or (k in args_dict and k != \"func\" and v != args_dict[k])\n}\n# Overwrite the default arguments with the ones from the yaml file\nargs_dict.update(flatten_dict(config_dict))\n# Overwrite the result of the above with any non-default CLI args\nargs_dict.update(non_default_passed_args_dict)\n# Create a new Namespace using the assembled dictionary\nnew_args = argparse.Namespace(**args_dict)\nassert getattr(\nnew_args, \"dataset\"\n), \"No dataset specified in the passed config file, provide one with the `--dataset` argument or add it to the config file\"\nassert all(\ngetattr(new_args, req_arg[\"arg\"]) for req_arg in required_args\n), f\"Required arguments are missing from the passed config file: {[ra['module'] + ':' + ra['arg'] for ra in required_args if not getattr(new_args, ra['arg'])]}\"\n# Run the appropriate execution function(s)\nif not new_args.seed:\nwarnings.warn(\"No seed has been specified, meaning the results of this run may not be reproducible.\")\nnew_args.version = version\nnew_args.modules_to_run = modules_to_run\nnew_args.module_handover = {}\nfor module in new_args.modules_to_run:\nMODULE_MAP[module](new_args)\nreturn new_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict[str, ArgumentParser]</code> <p>A dictionary containing all subparsers for the config args.</p> required Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def write_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n    \"\"\"\nexperiment_name = args.experiment_name\nargs_dict = assemble_config(args, all_subparsers)\nwith open(f\"experiments/{experiment_name}/config_{experiment_name}.yaml\", \"w\") as yaml_file:\nyaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/model_arguments/","title":"model_arguments","text":"<p>Define arguments for each of the model classes.</p>"},{"location":"reference/cli/model_arguments/#nhssynth.cli.model_arguments.add_model_specific_args","title":"<code>add_model_specific_args(group, name, overrides=False)</code>","text":"<p>Adds arguments to an existing group according to <code>name</code>.</p> Source code in <code>src/nhssynth/cli/model_arguments.py</code> <pre><code>def add_model_specific_args(group: argparse._ArgumentGroup, name: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing group according to `name`.\"\"\"\nif name == \"VAE\":\nadd_vae_args(group, overrides)\n</code></pre>"},{"location":"reference/cli/model_arguments/#nhssynth.cli.model_arguments.add_vae_args","title":"<code>add_vae_args(group, overrides=False)</code>","text":"<p>Adds arguments to an existing group for the VAE model.</p> Source code in <code>src/nhssynth/cli/model_arguments.py</code> <pre><code>def add_vae_args(group: argparse._ArgumentGroup, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing group for the VAE model.\"\"\"\ngroup.add_argument(\n\"--encoder-latent-dim\",\ntype=int,\nnargs=\"+\",\nhelp=\"the latent dimension of the encoder\",\n)\ngroup.add_argument(\n\"--encoder-hidden-dim\",\ntype=int,\nnargs=\"+\",\nhelp=\"the hidden dimension of the encoder\",\n)\ngroup.add_argument(\n\"--encoder-activation\",\ntype=str,\nnargs=\"+\",\nchoices=list(ACTIVATION_FUNCTIONS.keys()),\nhelp=\"the activation function of the encoder\",\n)\ngroup.add_argument(\n\"--encoder-learning-rate\",\ntype=float,\nnargs=\"+\",\nhelp=\"the learning rate for the encoder\",\n)\ngroup.add_argument(\n\"--decoder-latent-dim\",\ntype=int,\nnargs=\"+\",\nhelp=\"the latent dimension of the decoder\",\n)\ngroup.add_argument(\n\"--decoder-hidden-dim\",\ntype=int,\nnargs=\"+\",\nhelp=\"the hidden dimension of the decoder\",\n)\ngroup.add_argument(\n\"--decoder-activation\",\ntype=str,\nnargs=\"+\",\nchoices=list(ACTIVATION_FUNCTIONS.keys()),\nhelp=\"the activation function of the decoder\",\n)\ngroup.add_argument(\n\"--decoder-learning-rate\",\ntype=float,\nnargs=\"+\",\nhelp=\"the learning rate for the decoder\",\n)\ngroup.add_argument(\n\"--shared-optimizer\",\naction=\"store_true\",\nhelp=\"whether to use a shared optimizer for the encoder and decoder\",\n)\n</code></pre>"},{"location":"reference/cli/module_arguments/","title":"module_arguments","text":"<p>Define arguments for each of the modules' CLI sub-parsers.</p>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.AllChoicesDefault","title":"<code>AllChoicesDefault</code>","text":"<p>             Bases: <code>Action</code></p> <p>Customised argparse action for defaulting to the full list of choices if only the argument's flag is supplied: (i.e. user passes <code>--metrics</code> with no follow up list of metric groups =&gt; all metric groups will be executed).</p> Notes <p>1) If no <code>option_string</code> is supplied: set to default value (<code>self.default</code>) 2) If <code>option_string</code> is supplied:     a) If <code>values</code> are supplied, set to list of values     b) If no <code>values</code> are supplied, set to <code>self.const</code>, if <code>self.const</code> is not set, set to <code>self.default</code></p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>class AllChoicesDefault(argparse.Action):\n\"\"\"\n    Customised argparse action for defaulting to the full list of choices if only the argument's flag is supplied:\n    (i.e. user passes `--metrics` with no follow up list of metric groups =&gt; all metric groups will be executed).\n    Notes:\n        1) If no `option_string` is supplied: set to default value (`self.default`)\n        2) If `option_string` is supplied:\n            a) If `values` are supplied, set to list of values\n            b) If no `values` are supplied, set to `self.const`, if `self.const` is not set, set to `self.default`\n    \"\"\"\ndef __call__(self, parser, namespace, values=None, option_string=None):\nif values:\nsetattr(namespace, self.dest, values)\nelif option_string:\nsetattr(namespace, self.dest, self.const if self.const else self.default)\nelse:\nsetattr(namespace, self.dest, self.default)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--data-dir\",\ntype=str,\ndefault=\"./data\",\nhelp=\"the directory containing the chosen dataset\",\n)\ngroup.add_argument(\n\"--index-col\",\ndefault=None,\nchoices=[None, 0],\nhelp=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n)\ngroup.add_argument(\n\"--constraint-graph\",\ntype=str,\ndefault=\"_constraint_graph\",\nhelp=\"the name of the html file to write the constraint graph to, defaults to `&lt;DATASET&gt;_constraint_graph`\",\n)\ngroup.add_argument(\n\"--collapse-yaml\",\naction=\"store_true\",\nhelp=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n)\ngroup.add_argument(\n\"--missingness\",\ntype=str,\ndefault=\"augment\",\nchoices=MISSINGNESS_STRATEGIES,\nhelp=\"how to handle missing values in the dataset\",\n)\ngroup.add_argument(\n\"--impute\",\ntype=str,\ndefault=None,\nhelp=\"the imputation strategy to use, ONLY USED if &lt;MISSINGNESS&gt; is set to 'impute', choose from: 'mean', 'median', 'mode', or any specific value (e.g. '0')\",\n)\ngroup.add_argument(\n\"--write-csv\",\naction=\"store_true\",\nhelp=\"write the transformed real data to a csv file\",\n)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--downstream-tasks\",\n\"--tasks\",\naction=\"store_true\",\nhelp=\"run the downstream tasks evaluation\",\n)\ngroup.add_argument(\n\"--tasks-dir\",\ntype=str,\ndefault=\"./tasks\",\nhelp=\"the directory containing the downstream tasks to run, this directory must contain a folder called &lt;DATASET&gt; containing the tasks to run\",\n)\ngroup.add_argument(\n\"--aequitas\",\naction=\"store_true\",\nhelp=\"run the aequitas fairness evaluation (note this runs for each of the downstream tasks)\",\n)\ngroup.add_argument(\n\"--aequitas-attributes\",\ntype=str,\nnargs=\"+\",\ndefault=None,\nhelp=\"the attributes to use for the aequitas fairness evaluation, defaults to all attributes\",\n)\ngroup.add_argument(\n\"--key-numerical-fields\",\ntype=str,\nnargs=\"+\",\ndefault=None,\nhelp=\"the numerical key field attributes to use for SDV privacy evaluations\",\n)\ngroup.add_argument(\n\"--sensitive-numerical-fields\",\ntype=str,\nnargs=\"+\",\ndefault=None,\nhelp=\"the numerical sensitive field attributes to use for SDV privacy evaluations\",\n)\ngroup.add_argument(\n\"--key-categorical-fields\",\ntype=str,\nnargs=\"+\",\ndefault=None,\nhelp=\"the categorical key field attributes to use for SDV privacy evaluations\",\n)\ngroup.add_argument(\n\"--sensitive-categorical-fields\",\ntype=str,\nnargs=\"+\",\ndefault=None,\nhelp=\"the categorical sensitive field attributes to use for SDV privacy evaluations\",\n)\nfor name in METRIC_CHOICES:\ngenerate_evaluation_arg(group, name)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--architecture\",\ntype=str,\nnargs=\"+\",\ndefault=[\"VAE\"],\nchoices=MODELS,\nhelp=\"the model architecture(s) to train\",\n)\ngroup.add_argument(\n\"--repeats\",\ntype=int,\ndefault=1,\nhelp=\"how many times to repeat the training process per model architecture (&lt;SEED&gt; is incremented each time)\",\n)\ngroup.add_argument(\n\"--batch-size\",\ntype=int,\nnargs=\"+\",\ndefault=32,\nhelp=\"the batch size for the model\",\n)\ngroup.add_argument(\n\"--num-epochs\",\ntype=int,\nnargs=\"+\",\ndefault=100,\nhelp=\"number of epochs to train for\",\n)\ngroup.add_argument(\n\"--patience\",\ntype=int,\nnargs=\"+\",\ndefault=5,\nhelp=\"how many epochs the model is allowed to train for without improvement\",\n)\ngroup.add_argument(\n\"--displayed-metrics\",\ntype=str,\nnargs=\"+\",\ndefault=TRACKED_METRICS,\nhelp=\"metrics to display during training of the model\",\nchoices=TRACKED_METRICS,\n)\ngroup.add_argument(\n\"--use-gpu\",\naction=\"store_true\",\nhelp=\"use the GPU for training\",\n)\ngroup.add_argument(\n\"--num-samples\",\ntype=int,\ndefault=None,\nhelp=\"the number of samples to generate from the model, defaults to the size of the original dataset\",\n)\nprivacy_group = parser.add_argument_group(title=\"model privacy options\")\nprivacy_group.add_argument(\n\"--target-epsilon\",\ntype=float,\nnargs=\"+\",\ndefault=1.0,\nhelp=\"the target epsilon for differential privacy\",\n)\nprivacy_group.add_argument(\n\"--target-delta\",\ntype=float,\nnargs=\"+\",\nhelp=\"the target delta for differential privacy, defaults to `1 / len(dataset)` if not specified\",\n)\nprivacy_group.add_argument(\n\"--max-grad-norm\",\ntype=float,\nnargs=\"+\",\ndefault=5.0,\nhelp=\"the clipping threshold for gradients (only relevant under differential privacy)\",\n)\nprivacy_group.add_argument(\n\"--secure-mode\",\naction=\"store_true\",\nhelp=\"Enable secure RNG via the `csprng` package to make privacy guarantees more robust, comes at a cost of performance and reproducibility\",\n)\nfor model_name in MODELS.keys():\nmodel_group = parser.add_argument_group(title=f\"{model_name}-specific options\")\nadd_model_specific_args(model_group, model_name, overrides=overrides)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, group_title, overrides=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, group_title: str, overrides: bool = False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\ngroup = parser.add_argument_group(title=group_title)\ngroup.add_argument(\n\"--plot-quality\",\naction=\"store_true\",\nhelp=\"plot the SDV quality report\",\n)\ngroup.add_argument(\n\"--plot-diagnostic\",\naction=\"store_true\",\nhelp=\"plot the SDV diagnostic report\",\n)\ngroup.add_argument(\n\"--plot-sdv-report\",\naction=\"store_true\",\nhelp=\"plot the SDV report\",\n)\ngroup.add_argument(\n\"--plot-tsne\",\naction=\"store_true\",\nhelp=\"plot the t-SNE embeddings of the real and synthetic data\",\n)\n</code></pre>"},{"location":"reference/cli/module_setup/","title":"module_setup","text":"<p>Specify all CLI-accessible modules and their configurations, the pipeline to run by default, and define special functions for the <code>config</code> and <code>pipeline</code> CLI option trees.</p>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig","title":"<code>ModuleConfig</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Attributes:</p> Name Type Description <code>func</code> <p>A callable that executes the module's functionality.</p> <code>add_args</code> <p>A callable that populates the module's sub-parser arguments.</p> <code>description</code> <p>A description of the module's functionality.</p> <code>help</code> <p>A help message for the module's command-line interface.</p> <code>common_parsers</code> <p>A list of common parsers to add to the module's sub-parser, appending the 'dataset' and 'core' parsers to those passed.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>class ModuleConfig:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n    Attributes:\n        func: A callable that executes the module's functionality.\n        add_args: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n        common_parsers: A list of common parsers to add to the module's sub-parser, appending the 'dataset' and 'core' parsers to those passed.\n    \"\"\"\ndef __init__(\nself,\nfunc: Callable[..., argparse.Namespace],\nadd_args: Callable[..., None],\ndescription: str,\nhelp: str,\ncommon_parsers: Optional[list[str]] = None,\nno_seed: bool = False,\n) -&gt; None:\nself.func = func\nself.add_args = add_args\nself.description = description\nself.help = help\nself.common_parsers = [\"core\", \"seed\"] if not no_seed else [\"core\"]\nif common_parsers:\nassert set(common_parsers) &lt;= COMMON_PARSERS.keys(), \"Invalid common parser(s) specified.\"\n# merge the below two assert statements\nassert (\n\"core\" not in common_parsers and \"seed\" not in common_parsers\n), \"The 'seed' and 'core' parser groups are automatically added to all modules, remove the from `ModuleConfig`s.\"\nself.common_parsers += common_parsers\ndef __call__(self, args: argparse.Namespace) -&gt; argparse.Namespace:\nreturn self.func(args)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\nparser.add_argument(\n\"-c\",\n\"--input-config\",\nrequired=True,\nhelp=\"specify the config file name\",\n)\nparser.add_argument(\n\"-cp\",\n\"--custom-pipeline\",\naction=\"store_true\",\nhelp=\"infer a custom pipeline running order of modules from the config\",\n)\nfor module_name in PIPELINE:\nMODULE_MAP[module_name].add_args(parser, f\"{module_name} option overrides\", overrides=True)\nfor module_name in VALID_MODULES - set(PIPELINE):\nMODULE_MAP[module_name].add_args(parser, f\"{module_name} options overrides\", overrides=True)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to <code>parser</code> for each module in the pipeline.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to `parser` for each module in the pipeline.\"\"\"\nfor module_name in PIPELINE:\nMODULE_MAP[module_name].add_args(parser, f\"{module_name} options\")\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, module_config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>module_config</code> <code>ModuleConfig</code> <p>A <code>ModuleConfig</code> object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def add_subparser(\nsubparsers: argparse._SubParsersAction,\nname: str,\nmodule_config: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        module_config: A [`ModuleConfig`][nhssynth.cli.module_setup.ModuleConfig] object containing information about the subparser, including a function to execute and a function to add arguments.\n    Returns:\n        The newly created subparser.\n    \"\"\"\nparent_parsers = get_parent_parsers(name, module_config.common_parsers)\nparser = subparsers.add_parser(\nname=name,\ndescription=module_config.description,\nhelp=module_config.help,\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nparents=parent_parsers,\n)\nif name not in {\"pipeline\", \"config\"}:\nmodule_config.add_args(parser, f\"{name} options\")\nelse:\nmodule_config.add_args(parser)\nparser.set_defaults(func=module_config.func)\nreturn parser\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.get_parent_parsers","title":"<code>get_parent_parsers(name, module_parsers)</code>","text":"<p>Get a list of parent parsers for a given module, based on the module's <code>common_parsers</code> attribute.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def get_parent_parsers(name: str, module_parsers: list[str]) -&gt; list[argparse.ArgumentParser]:\n\"\"\"Get a list of parent parsers for a given module, based on the module's `common_parsers` attribute.\"\"\"\nif name in {\"pipeline\", \"config\"}:\nreturn [p(name == \"config\") for p in COMMON_PARSERS.values()]\nelif name == \"dashboard\":\nreturn [COMMON_PARSERS[pn](True) for pn in module_parsers]\nelse:\nreturn [COMMON_PARSERS[pn]() for pn in module_parsers]\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\nprint(\"Running full pipeline...\")\nargs.modules_to_run = PIPELINE\nfor module_name in PIPELINE:\nargs = MODULE_MAP[module_name](args)\n</code></pre>"},{"location":"reference/cli/run/","title":"run","text":""},{"location":"reference/common/","title":"common","text":""},{"location":"reference/common/common/","title":"common","text":"<p>Common functions for all modules.</p>"},{"location":"reference/common/common/#nhssynth.common.common.set_seed","title":"<code>set_seed(seed=None)</code>","text":"<p>(Potentially) set the seed for numpy, torch and random. If no seed is provided, nothing happens.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>Optional[int]</code> <p>The seed to set.</p> <code>None</code> Source code in <code>src/nhssynth/common/common.py</code> <pre><code>def set_seed(seed: Optional[int] = None) -&gt; None:\n\"\"\"\n    (Potentially) set the seed for numpy, torch and random. If no seed is provided, nothing happens.\n    Args:\n        seed: The seed to set.\n    \"\"\"\nif seed:\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)\n</code></pre>"},{"location":"reference/common/constants/","title":"constants","text":"<p>Define all of the common constants used throughout the project.</p>"},{"location":"reference/common/debugging/","title":"debugging","text":"<p>Debugging utilities.</p>"},{"location":"reference/common/dicts/","title":"dicts","text":"<p>Common functions for working with dictionaries.</p>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.filter_dict","title":"<code>filter_dict(d, filter_keys, include=False)</code>","text":"<p>Given a dictionary, return a new dictionary either including or excluding keys in a given <code>filter</code> set.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to filter.</p> required <code>filter_keys</code> <code>Union[set, list]</code> <p>A list or set of keys to either include or exclude.</p> required <code>include</code> <code>bool</code> <p>Determine whether to return a dictionary including or excluding keys in <code>filter</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A filtered dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n{'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n{'a': 1, 'b': 2}\n</code></pre> Source code in <code>src/nhssynth/common/dicts.py</code> <pre><code>def filter_dict(d: dict, filter_keys: Union[set, list], include: bool = False) -&gt; dict:\n\"\"\"\n    Given a dictionary, return a new dictionary either including or excluding keys in a given `filter` set.\n    Args:\n        d: A dictionary to filter.\n        filter_keys: A list or set of keys to either include or exclude.\n        include: Determine whether to return a dictionary including or excluding keys in `filter`.\n    Returns:\n        A filtered dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n        {'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n        {'a': 1, 'b': 2}\n    \"\"\"\nif include:\nfiltered_keys = set(filter_keys) &amp; set(d.keys())\nelse:\nfiltered_keys = set(d.keys()) - set(filter_keys)\nreturn {k: v for k, v in d.items() if k in filtered_keys}\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, Any]</code> <p>A dictionary with potentially nested keys.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A flattened dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If duplicate keys are found in the flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>src/nhssynth/common/dicts.py</code> <pre><code>def flatten_dict(d: dict[str, Any]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n    Args:\n        d: A dictionary with potentially nested keys.\n    Returns:\n        A flattened dictionary.\n    Raises:\n        ValueError: If duplicate keys are found in the flattened dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\nitems = []\nfor k, v in d.items():\nif isinstance(v, dict):\nitems.extend(flatten_dict(v).items())\nelse:\nitems.append((k, v))\nif len(set([p[0] for p in items])) != len(items):\nraise ValueError(f\"Duplicate keys found in flattened dictionary\")\nreturn dict(items)\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <code>Any</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Union[Any, None]</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>src/nhssynth/common/dicts.py</code> <pre><code>def get_key_by_value(d: dict, value: Any) -&gt; Union[Any, None]:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n    \"\"\"\nfor key, val in d.items():\nif val == value:\nreturn key\nreturn None\n</code></pre>"},{"location":"reference/common/io/","title":"io","text":"<p>Common building-block functions for handling module input and output.</p>"},{"location":"reference/common/io/#nhssynth.common.io.check_exists","title":"<code>check_exists(fns, dir)</code>","text":"<p>Checks if the files in <code>fns</code> exist in <code>dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list[str]</code> <p>The list of files to check.</p> required <code>dir</code> <code>Path</code> <p>The directory the files should exist in.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any of the files in <code>fns</code> do not exist in <code>dir</code>.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def check_exists(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Checks if the files in `fns` exist in `dir`.\n    Args:\n        fns: The list of files to check.\n        dir: The directory the files should exist in.\n    Raises:\n        FileNotFoundError: If any of the files in `fns` do not exist in `dir`.\n    \"\"\"\nfor fn in fns:\nif not (dir / fn).exists():\nraise FileNotFoundError(f\"File {fn} does not exist at {dir}.\")\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.consistent_ending","title":"<code>consistent_ending(fn, ending='.pkl', suffix='')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, removes any existing ending and appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".pkl\".</p> <code>'.pkl'</code> <code>suffix</code> <code>str</code> <p>A suffix to append to the filename before the ending.</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending and potentially an inserted suffix.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def consistent_ending(fn: str, ending: str = \".pkl\", suffix: str = \"\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, removes any existing ending and appends `ending`.\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".pkl\".\n        suffix: A suffix to append to the filename before the ending.\n    Returns:\n        The filename with the correct ending and potentially an inserted suffix.\n    \"\"\"\npath_fn = Path(fn)\nreturn str(path_fn.parent / path_fn.stem) + (\"_\" if suffix else \"\") + suffix + ending\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.consistent_endings","title":"<code>consistent_endings(args)</code>","text":"<p>Wrapper around <code>consistent_ending</code> to apply it to a list of filenames.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list[Union[str, tuple[str, str], tuple[str, str, str]]]</code> <p>The list of filenames to check. Can take the form of a single filename, a pair of a filename and an ending, or a triple of a filename, an ending and a suffix.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The list of filenames with the correct endings.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def consistent_endings(args: list[Union[str, tuple[str, str], tuple[str, str, str]]]) -&gt; list[str]:\n\"\"\"\n    Wrapper around `consistent_ending` to apply it to a list of filenames.\n    Args:\n        args: The list of filenames to check. Can take the form of a single filename, a pair of a filename and an ending, or a triple of a filename, an ending and a suffix.\n    Returns:\n        The list of filenames with the correct endings.\n    \"\"\"\nreturn list(consistent_ending(arg) if isinstance(arg, str) else consistent_ending(*arg) for arg in args)\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.experiment_io","title":"<code>experiment_io(experiment_name, dir_experiments='experiments')</code>","text":"<p>Create an experiment's directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <code>dir_experiments</code> <code>str</code> <p>The name of the directory containing all experiments.</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the experiment directory.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def experiment_io(experiment_name: str, dir_experiments: str = \"experiments\") -&gt; str:\n\"\"\"\n    Create an experiment's directory and return the path.\n    Args:\n        experiment_name: The name of the experiment.\n        dir_experiments: The name of the directory containing all experiments.\n    Returns:\n        The path to the experiment directory.\n    \"\"\"\ndir_experiment = Path(dir_experiments) / experiment_name\ndir_experiment.mkdir(parents=True, exist_ok=True)\nreturn dir_experiment\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.potential_suffix","title":"<code>potential_suffix(fn, fn_base)</code>","text":"<p>Checks if <code>fn</code> is a suffix (starts with an underscore) to append to <code>fn_base</code>, or a filename in its own right.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename / potential suffix to append to <code>fn_base</code>.</p> required <code>fn_base</code> <code>str</code> <p>The name of the file the suffix would attach to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The appropriately processed <code>fn</code></p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def potential_suffix(fn: str, fn_base: str) -&gt; str:\n\"\"\"\n    Checks if `fn` is a suffix (starts with an underscore) to append to `fn_base`, or a filename in its own right.\n    Args:\n        fn: The filename / potential suffix to append to `fn_base`.\n        fn_base: The name of the file the suffix would attach to.\n    Returns:\n        The appropriately processed `fn`\n    \"\"\"\nfn_base = Path(fn_base).stem\nif fn[0] == \"_\":\nreturn fn_base + fn\nelse:\nreturn fn\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.potential_suffixes","title":"<code>potential_suffixes(fns, fn_base)</code>","text":"<p>Wrapper around <code>potential_suffix</code> to apply it to a list of filenames.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list[str]</code> <p>The list of filenames / potential suffixes to append to <code>fn_base</code>.</p> required <code>fn_base</code> <code>str</code> <p>The name of the file the suffixes would attach to.</p> required Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def potential_suffixes(fns: list[str], fn_base: str) -&gt; list[str]:\n\"\"\"\n    Wrapper around `potential_suffix` to apply it to a list of filenames.\n    Args:\n        fns: The list of filenames / potential suffixes to append to `fn_base`.\n        fn_base: The name of the file the suffixes would attach to.\n    \"\"\"\nreturn list(potential_suffix(fn, fn_base) for fn in fns)\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.warn_if_path_supplied","title":"<code>warn_if_path_supplied(fns, dir)</code>","text":"<p>Warns if the files in <code>fns</code> include directory separators.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list[str]</code> <p>The list of files to check.</p> required <code>dir</code> <code>Path</code> <p>The directory the files should exist in.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>when the path to any of the files in <code>fns</code> includes directory separators, as this may lead to unintended consequences if the user doesn't realise default directories are pre-specified.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def warn_if_path_supplied(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Warns if the files in `fns` include directory separators.\n    Args:\n        fns: The list of files to check.\n        dir: The directory the files should exist in.\n    Warnings:\n        UserWarning: when the path to any of the files in `fns` includes directory separators, as this may lead to unintended consequences if the user doesn't realise default directories are pre-specified.\n    \"\"\"\nfor fn in fns:\nif \"/\" in fn:\nwarnings.warn(\nf\"Using the path supplied appended to {dir}, i.e. attempting to read data from {dir / fn}\",\nUserWarning,\n)\n</code></pre>"},{"location":"reference/common/strings/","title":"strings","text":"<p>String manipulation functions.</p>"},{"location":"reference/common/strings/#nhssynth.common.strings.add_spaces_before_caps","title":"<code>add_spaces_before_caps(string)</code>","text":"<p>Adds spaces before capital letters in a string if there is a lower-case letter following it.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string to add spaces to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string with spaces added before capital letters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; add_spaces_before_caps(\"HelloWorld\")\n'Hello World'\n&gt;&gt;&gt; add_spaces_before_caps(\"HelloWorldAGAIN\")\n'Hello World AGAIN'\n</code></pre> Source code in <code>src/nhssynth/common/strings.py</code> <pre><code>def add_spaces_before_caps(string: str) -&gt; str:\n\"\"\"\n    Adds spaces before capital letters in a string if there is a lower-case letter following it.\n    Args:\n        string: The string to add spaces to.\n    Returns:\n        The string with spaces added before capital letters.\n    Examples:\n        &gt;&gt;&gt; add_spaces_before_caps(\"HelloWorld\")\n        'Hello World'\n        &gt;&gt;&gt; add_spaces_before_caps(\"HelloWorldAGAIN\")\n        'Hello World AGAIN'\n    \"\"\"\nreturn \" \".join(re.findall(r\"[a-z]?[A-Z][a-z]+|[A-Z]+(?=[A-Z][a-z]|\\b)\", string))\n</code></pre>"},{"location":"reference/common/strings/#nhssynth.common.strings.format_timedelta","title":"<code>format_timedelta(start, finish)</code>","text":"<p>Calculate and prettily format the difference between two calls to <code>time.time()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>The start time.</p> required <code>finish</code> <code>float</code> <p>The finish time.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string containing the time difference in a human-readable format.</p> Source code in <code>src/nhssynth/common/strings.py</code> <pre><code>def format_timedelta(start: float, finish: float) -&gt; str:\n\"\"\"\n    Calculate and prettily format the difference between two calls to `time.time()`.\n    Args:\n        start: The start time.\n        finish: The finish time.\n    Returns:\n        A string containing the time difference in a human-readable format.\n    \"\"\"\ntotal = datetime.timedelta(seconds=finish - start)\nhours, remainder = divmod(total.seconds, 3600)\nminutes, seconds = divmod(remainder, 60)\nif total.days &gt; 0:\ndelta_str = f\"{total.days}d {hours}h {minutes}m {seconds}s\"\nelif hours &gt; 0:\ndelta_str = f\"{hours}h {minutes}m {seconds}s\"\nelif minutes &gt; 0:\ndelta_str = f\"{minutes}m {seconds}s\"\nelse:\ndelta_str = f\"{seconds}s\"\nreturn delta_str\n</code></pre>"},{"location":"reference/modules/","title":"modules","text":""},{"location":"reference/modules/dashboard/","title":"dashboard","text":""},{"location":"reference/modules/dashboard/Upload/","title":"Upload","text":""},{"location":"reference/modules/dashboard/Upload/#nhssynth.modules.dashboard.Upload.get_component","title":"<code>get_component(args, name, component_type, text)</code>","text":"<p>Generate an upload field and its functionality for a given component of the evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the component as it should be recorded in the session state and as it exists in the args.</p> required <code>component_type</code> <code>Any</code> <p>The type of the component (to ensure that only the expected object can be uploaded)</p> required <code>text</code> <code>str</code> <p>The human-readable text to display to the user as part of the element.</p> required Source code in <code>src/nhssynth/modules/dashboard/Upload.py</code> <pre><code>def get_component(args: argparse.Namespace, name: str, component_type: Any, text: str) -&gt; None:\n\"\"\"\n    Generate an upload field and its functionality for a given component of the evaluations.\n    Args:\n        name: The name of the component as it should be recorded in the session state and as it exists in the args.\n        component_type: The type of the component (to ensure that only the expected object can be uploaded)\n        text: The human-readable text to display to the user as part of the element.\n    \"\"\"\nuploaded = st.file_uploader(f\"Upload a pickle file containing a {text}\", type=\"pkl\")\nif getattr(args, name):\nwith open(os.getcwd() + \"/\" + getattr(args, name), \"rb\") as f:\nloaded = pickle.load(f)\nif uploaded is not None:\nloaded = pickle.load(uploaded)\nif loaded is not None:\nassert isinstance(loaded, component_type), f\"Uploaded file does not contain a {text}!\"\nst.session_state[name] = loaded.contents\nst.success(f\"Loaded {text}!\")\n</code></pre>"},{"location":"reference/modules/dashboard/Upload/#nhssynth.modules.dashboard.Upload.parse_args","title":"<code>parse_args()</code>","text":"<p>These arguments allow a user to automatically load the required data for the dashboard from disk.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>The parsed arguments.</p> Source code in <code>src/nhssynth/modules/dashboard/Upload.py</code> <pre><code>def parse_args() -&gt; argparse.Namespace:\n\"\"\"\n    These arguments allow a user to automatically load the required data for the dashboard from disk.\n    Returns:\n        The parsed arguments.\n    \"\"\"\nparser = argparse.ArgumentParser(description=\"NHSSynth Evaluation Dashboard\")\nparser.add_argument(\"--evaluations\", type=str, help=\"Path to a set of evaluations.\")\nparser.add_argument(\"--experiments\", type=str, help=\"Path to a set of experiments.\")\nparser.add_argument(\"--synthetic-datasets\", type=str, help=\"Path to a set of synthetic datasets.\")\nparser.add_argument(\"--typed\", type=str, help=\"Path to a typed real dataset.\")\nreturn parser.parse_args()\n</code></pre>"},{"location":"reference/modules/dashboard/io/","title":"io","text":""},{"location":"reference/modules/dashboard/io/#nhssynth.modules.dashboard.io.check_input_paths","title":"<code>check_input_paths(dir_experiment, fn_dataset, fn_typed, fn_experiments, fn_synthetic_datasets, fn_evaluations)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>dir_experiment</code> <code>str</code> <p>The path to the experiment directory.</p> required <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_experiments</code> <code>str</code> <p>The filename of the collection of experiments.</p> required <code>fn_synthetic_datasets</code> <code>str</code> <p>The filename of the collection of synthetic datasets.</p> required <code>fn_evaluations</code> <code>str</code> <p>The filename of the collection of evaluations.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The paths</p> Source code in <code>src/nhssynth/modules/dashboard/io.py</code> <pre><code>def check_input_paths(\ndir_experiment: str,\nfn_dataset: str,\nfn_typed: str,\nfn_experiments: str,\nfn_synthetic_datasets: str,\nfn_evaluations: str,\n) -&gt; str:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        dir_experiment: The path to the experiment directory.\n        fn_dataset: The base name of the dataset.\n        fn_experiments: The filename of the collection of experiments.\n        fn_synthetic_datasets: The filename of the collection of synthetic datasets.\n        fn_evaluations: The filename of the collection of evaluations.\n    Returns:\n        The paths\n    \"\"\"\nfn_dataset = Path(fn_dataset).stem\nfn_typed, fn_experiments, fn_synthetic_datasets, fn_evaluations = consistent_endings(\n[fn_typed, fn_experiments, fn_synthetic_datasets, fn_evaluations]\n)\nfn_typed, fn_experiments, fn_synthetic_datasets, fn_evaluations = potential_suffixes(\n[fn_typed, fn_experiments, fn_synthetic_datasets, fn_evaluations], fn_dataset\n)\nwarn_if_path_supplied([fn_typed, fn_experiments, fn_synthetic_datasets, fn_evaluations], dir_experiment)\ncheck_exists([fn_typed, fn_experiments, fn_synthetic_datasets, fn_evaluations], dir_experiment)\nreturn (\ndir_experiment / fn_typed,\ndir_experiment / fn_experiments,\ndir_experiment / fn_synthetic_datasets,\ndir_experiment / fn_evaluations,\n)\n</code></pre>"},{"location":"reference/modules/dashboard/run/","title":"run","text":""},{"location":"reference/modules/dashboard/utils/","title":"utils","text":""},{"location":"reference/modules/dashboard/utils/#nhssynth.modules.dashboard.utils.hide_streamlit_content","title":"<code>hide_streamlit_content()</code>","text":"<p>Hide the footer message and deploy button in Streamlit.</p> Source code in <code>src/nhssynth/modules/dashboard/utils.py</code> <pre><code>def hide_streamlit_content() -&gt; None:\n\"\"\"\n    Hide the footer message and deploy button in Streamlit.\n    \"\"\"\nhide_streamlit_style = \"\"\"\n    &lt;style&gt;\n    footer {visibility: hidden;}\n    .stDeployButton {visibility: hidden;}\n    &lt;/style&gt;\n    \"\"\"\nst.markdown(hide_streamlit_style, unsafe_allow_html=True)\n</code></pre>"},{"location":"reference/modules/dashboard/utils/#nhssynth.modules.dashboard.utils.id_selector","title":"<code>id_selector(df)</code>","text":"<p>Select an ID from the dataframe to then operate on.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to select an ID from.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The dataset subset to only the row corresponding to the ID.</p> Source code in <code>src/nhssynth/modules/dashboard/utils.py</code> <pre><code>def id_selector(df: pd.DataFrame) -&gt; pd.Series:\n\"\"\"\n    Select an ID from the dataframe to then operate on.\n    Args:\n        df: The dataframe to select an ID from.\n    Returns:\n        The dataset subset to only the row corresponding to the ID.\n    \"\"\"\narchitecture = st.sidebar.selectbox(\n\"Select architecture to display\", df.index.get_level_values(\"architecture\").unique()\n)\n# Different architectures may have different numbers of repeats and configs\nrepeats = df.loc[architecture].index.get_level_values(\"repeat\").astype(int).unique()\nconfigs = df.loc[architecture].index.get_level_values(\"config\").astype(int).unique()\nif len(repeats) &gt; 1:\nrepeat = st.sidebar.selectbox(\"Select repeat to display\", repeats)\nelse:\nrepeat = repeats[0]\nif len(configs) &gt; 1:\nconfig = st.sidebar.selectbox(\"Select configuration to display\", configs)\nelse:\nconfig = configs[0]\nreturn df.loc[(architecture, repeat, config)]\n</code></pre>"},{"location":"reference/modules/dashboard/utils/#nhssynth.modules.dashboard.utils.subset_selector","title":"<code>subset_selector(df)</code>","text":"<p>Select a subset of the dataframe to then operate on.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to select a subset of.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The subset of the dataframe.</p> Source code in <code>src/nhssynth/modules/dashboard/utils.py</code> <pre><code>def subset_selector(df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Select a subset of the dataframe to then operate on.\n    Args:\n        df: The dataframe to select a subset of.\n    Returns:\n        The subset of the dataframe.\n    \"\"\"\narchitectures = df.index.get_level_values(\"architecture\").unique().tolist()\nrepeats = df.index.get_level_values(\"repeat\").astype(int).unique().tolist()\nconfigs = df.index.get_level_values(\"config\").astype(int).unique().tolist()\nselected_architectures = st.sidebar.multiselect(\n\"Select architectures to display\", architectures, default=architectures\n)\nselected_repeats = st.sidebar.multiselect(\"Select repeats to display\", repeats, default=repeats[0])\nselected_configs = st.sidebar.multiselect(\"Select configurations to display\", configs, default=configs)\nreturn df.loc[(selected_architectures, selected_repeats, selected_configs)]\n</code></pre>"},{"location":"reference/modules/dashboard/pages/","title":"pages","text":""},{"location":"reference/modules/dashboard/pages/1_Tables/","title":"1_Tables","text":""},{"location":"reference/modules/dashboard/pages/2_Plots/","title":"2_Plots","text":""},{"location":"reference/modules/dashboard/pages/2_Plots/#nhssynth.modules.dashboard.pages.2_Plots.prepare_for_dimensionality","title":"<code>prepare_for_dimensionality(df)</code>","text":"<p>Factorize all categorical columns in a dataframe.</p> Source code in <code>src/nhssynth/modules/dashboard/pages/2_Plots.py</code> <pre><code>def prepare_for_dimensionality(df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Factorize all categorical columns in a dataframe.\"\"\"\nfor col in df.columns:\nif df[col].dtype == \"object\":\ndf[col] = pd.factorize(df[col])[0]\nelif df[col].dtype == \"datetime64[ns]\":\ndf[col] = pd.to_numeric(df[col])\nmin_val = df[col].min()\nmax_val = df[col].max()\ndf[col] = (df[col] - min_val) / (max_val - min_val)\nreturn df\n</code></pre>"},{"location":"reference/modules/dashboard/pages/3_Experiment_Configurations/","title":"3_Experiment_Configurations","text":""},{"location":"reference/modules/dataloader/","title":"dataloader","text":""},{"location":"reference/modules/dataloader/constraints/","title":"constraints","text":""},{"location":"reference/modules/dataloader/io/","title":"io","text":""},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_input, fn_metadata, dir_data)</code>","text":"<p>Formats the input filenames and directory for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_input</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple[Path, str, str]</code> <p>A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>When the path to <code>fn_input</code> includes directory separators, as this is not supported and may not work as intended.</p> <code>UserWarning</code> <p>When the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>src/nhssynth/modules/dataloader/io.py</code> <pre><code>def check_input_paths(\nfn_input: str,\nfn_metadata: str,\ndir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input filenames and directory for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_input`.\n        dir_data: The directory that should contain both of the above.\n    Returns:\n        A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).\n    Warnings:\n        UserWarning: When the path to `fn_input` includes directory separators, as this is not supported and may not work as intended.\n        UserWarning: When the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_input, fn_metadata = consistent_endings([(fn_input, \".csv\"), (fn_metadata, \".yaml\")])\ndir_data = Path(dir_data)\nfn_metadata = potential_suffix(fn_metadata, fn_input)\nwarn_if_path_supplied([fn_input, fn_metadata], dir_data)\ncheck_exists([fn_input], dir_data)\nreturn dir_data, fn_input, fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_dataset, fn_typed, fn_transformed, fn_metatransformer, fn_constraint_graph, fn_sdv_metadata, dir_experiment)</code>","text":"<p>Formats the output filenames for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The input data filename.</p> required <code>fn_typed</code> <code>str</code> <p>The typed input data filename/suffix to append to <code>fn_dataset</code>.</p> required <code>fn_transformed</code> <code>str</code> <p>The transformed output data filename/suffix to append to <code>fn_dataset</code>.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The metatransformer filename/suffix to append to <code>fn_dataset</code>.</p> required <code>fn_constraint_graph</code> <code>str</code> <p>The constraint graph filename/suffix to append to <code>fn_dataset</code>.</p> required <code>fn_sdv_metadata</code> <code>str</code> <p>The SDV metadata filename/suffix to append to <code>fn_dataset</code>.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <p>Returns:</p> Type Description <code>tuple[str, str, str]</code> <p>A tuple containing the formatted output filenames.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>When any of the filenames include directory separators, as this is not supported and may not work as intended.</p> Source code in <code>src/nhssynth/modules/dataloader/io.py</code> <pre><code>def check_output_paths(\nfn_dataset: str,\nfn_typed: str,\nfn_transformed: str,\nfn_metatransformer: str,\nfn_constraint_graph: str,\nfn_sdv_metadata: str,\ndir_experiment: Path,\n) -&gt; tuple[str, str, str]:\n\"\"\"\n    Formats the output filenames for an experiment.\n    Args:\n        fn_dataset: The input data filename.\n        fn_typed: The typed input data filename/suffix to append to `fn_dataset`.\n        fn_transformed: The transformed output data filename/suffix to append to `fn_dataset`.\n        fn_metatransformer: The metatransformer filename/suffix to append to `fn_dataset`.\n        fn_constraint_graph: The constraint graph filename/suffix to append to `fn_dataset`.\n        fn_sdv_metadata: The SDV metadata filename/suffix to append to `fn_dataset`.\n        dir_experiment: The experiment directory to write the outputs to.\n    Returns:\n        A tuple containing the formatted output filenames.\n    Warnings:\n        UserWarning: When any of the filenames include directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_dataset = Path(fn_dataset).stem\nfn_typed, fn_transformed, fn_metatransformer, fn_constraint_graph, fn_sdv_metadata = consistent_endings(\n[fn_typed, fn_transformed, fn_metatransformer, (fn_constraint_graph, \".html\"), fn_sdv_metadata]\n)\nfn_typed, fn_transformed, fn_metatransformer, fn_constraint_graph, fn_sdv_metadata = potential_suffixes(\n[fn_typed, fn_transformed, fn_metatransformer, fn_constraint_graph, fn_sdv_metadata], fn_dataset\n)\nwarn_if_path_supplied(\n[fn_typed, fn_transformed, fn_metatransformer, fn_constraint_graph, fn_sdv_metadata], dir_experiment\n)\nreturn fn_dataset, fn_typed, fn_transformed, fn_metatransformer, fn_constraint_graph, fn_sdv_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(metatransformer, fn_dataset, fn_metadata, dir_experiment, args)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its model-ready state.</p> required <code>fn_dataset</code> <code>str</code> <p>The base dataset filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <code>args</code> <code>Namespace</code> <p>The full set of parsed command line arguments.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The filename of the dataset used.</p> Source code in <code>src/nhssynth/modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\nmetatransformer: MetaTransformer,\nfn_dataset: str,\nfn_metadata: str,\ndir_experiment: Path,\nargs: argparse.Namespace,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n    Args:\n        metatransformer: The metatransformer used to transform the data into its model-ready state.\n        fn_dataset: The base dataset filename.\n        fn_metadata: The metadata filename.\n        dir_experiment: The experiment directory to write the outputs to.\n        args: The full set of parsed command line arguments.\n    Returns:\n        The filename of the dataset used.\n    \"\"\"\nfn_dataset, fn_typed, fn_transformed, fn_metatransformer, fn_constraint_graph, fn_sdv_metadata = check_output_paths(\nfn_dataset,\nargs.typed,\nargs.transformed,\nargs.metatransformer,\nargs.constraint_graph,\nargs.sdv_metadata,\ndir_experiment,\n)\nmetatransformer.save_metadata(dir_experiment / fn_metadata, args.collapse_yaml)\nmetatransformer.save_constraint_graphs(dir_experiment / fn_constraint_graph)\nwith open(dir_experiment / fn_typed, \"wb\") as f:\npickle.dump(TypedDataset(metatransformer.get_typed_dataset()), f)\ntransformed_dataset = metatransformer.get_transformed_dataset()\ntransformed_dataset.to_pickle(dir_experiment / fn_transformed)\nif args.write_csv:\nchunks = np.array_split(transformed_dataset.index, 100)\nfor chunk, subset in enumerate(tqdm(chunks, desc=\"Writing transformed dataset to CSV\", unit=\"chunk\")):\nif chunk == 0:\ntransformed_dataset.loc[subset].to_csv(\ndir_experiment / (fn_transformed[:-3] + \"csv\"), mode=\"w\", index=False\n)\nelse:\ntransformed_dataset.loc[subset].to_csv(\ndir_experiment / (fn_transformed[:-3] + \"csv\"), mode=\"a\", index=False, header=False\n)\nwith open(dir_experiment / fn_metatransformer, \"wb\") as f:\npickle.dump(metatransformer, f)\nwith open(dir_experiment / fn_sdv_metadata, \"wb\") as f:\npickle.dump(metatransformer.get_sdv_metadata(), f)\nreturn fn_dataset\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/","title":"metadata","text":""},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.MetaData","title":"<code>MetaData</code>","text":"Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>class MetaData:\nclass ColumnMetaData:\ndef __init__(self, name: str, data: pd.Series, raw: dict) -&gt; None:\nself.name = name\nself.dtype: np.dtype = self._validate_dtype(data, raw.get(\"dtype\"))\nself.categorical: bool = self._validate_categorical(data, raw.get(\"categorical\"))\nself.missingness_strategy: GenericMissingnessStrategy = self._validate_missingness_strategy(\nraw.get(\"missingness\")\n)\nself.transformer: ColumnTransformer = self._validate_transformer(raw.get(\"transformer\"))\ndef _validate_dtype(self, data: pd.Series, dtype_raw: Optional[Union[dict, str]] = None) -&gt; np.dtype:\nif isinstance(dtype_raw, dict):\ndtype_name = dtype_raw.pop(\"name\", None)\nelif isinstance(dtype_raw, str):\ndtype_name = dtype_raw\nelse:\ndtype_name = self._infer_dtype(data)\ntry:\ndtype = np.dtype(dtype_name)\nexcept TypeError:\nwarnings.warn(\nf\"Invalid dtype specification '{dtype_name}' for column '{self.name}', ignoring dtype for this column\"\n)\ndtype = self._infer_dtype(data)\nif dtype.kind == \"M\":\nself._setup_datetime_config(data, dtype_raw)\nelif dtype.kind in [\"f\", \"i\", \"u\"]:\nself.rounding_scheme = self._validate_rounding_scheme(data, dtype, dtype_raw)\nreturn dtype\ndef _infer_dtype(self, data: pd.Series) -&gt; np.dtype:\nreturn data.dtype.name\ndef _infer_datetime_format(self, data: pd.Series) -&gt; str:\nreturn _guess_datetime_format_for_array(data[data.notna()].astype(str).to_numpy())\ndef _setup_datetime_config(self, data: pd.Series, datetime_config: dict) -&gt; dict:\n\"\"\"\n            Add keys to `datetime_config` corresponding to args from the `pd.to_datetime` function\n            (see [the docs](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html))\n            \"\"\"\nif not isinstance(datetime_config, dict):\ndatetime_config = {}\nelse:\ndatetime_config = filter_dict(datetime_config, {\"format\", \"floor\"}, include=True)\nif \"format\" not in datetime_config:\ndatetime_config[\"format\"] = self._infer_datetime_format(data)\nself.datetime_config = datetime_config\ndef _validate_rounding_scheme(self, data: pd.Series, dtype: np.dtype, dtype_dict: dict) -&gt; int:\nif dtype_dict and \"rounding_scheme\" in dtype_dict:\nreturn dtype_dict[\"rounding_scheme\"]\nelse:\nif dtype.kind != \"f\":\nreturn 1.0\nroundable_data = data[data.notna()]\nfor i in range(np.finfo(dtype).precision):\nif (roundable_data.round(i) == roundable_data).all():\nreturn 10**-i\nreturn None\ndef _validate_categorical(self, data: pd.Series, categorical: Optional[bool] = None) -&gt; bool:\nif categorical is None:\nreturn self._infer_categorical(data)\nelif not isinstance(categorical, bool):\nwarnings.warn(\nf\"Invalid categorical '{categorical}' for column '{self.name}', ignoring categorical for this column\"\n)\nreturn self._infer_categorical(data)\nelse:\nself.boolean = data.nunique() &lt;= 2\nreturn categorical\ndef _infer_categorical(self, data: pd.Series) -&gt; bool:\nself.boolean = data.nunique() &lt;= 2\nreturn data.nunique() &lt;= 10 or self.dtype.kind == \"O\"\ndef _validate_missingness_strategy(self, missingness_strategy: Optional[Union[dict, str]]) -&gt; tuple[str, dict]:\nif not missingness_strategy:\nreturn None\nif isinstance(missingness_strategy, dict):\nimpute = missingness_strategy.get(\"impute\", None)\nstrategy = \"impute\" if impute else missingness_strategy.get(\"strategy\", None)\nelse:\nstrategy = missingness_strategy\nif (\nstrategy not in MISSINGNESS_STRATEGIES\nor (strategy == \"impute\" and impute == \"mean\" and self.dtype.kind != \"f\")\nor (strategy == \"impute\" and not impute)\n):\nwarnings.warn(\nf\"Invalid missingness strategy '{missingness_strategy}' for column '{self.name}', ignoring missingness strategy for this column\"\n)\nreturn None\nreturn (\nMISSINGNESS_STRATEGIES[strategy](impute) if strategy == \"impute\" else MISSINGNESS_STRATEGIES[strategy]()\n)\ndef _validate_transformer(self, transformer: Optional[Union[dict, str]] = {}) -&gt; tuple[str, dict]:\n# if transformer is neither a dict nor a str statement below will raise a TypeError\nif isinstance(transformer, dict):\nself.transformer_name = transformer.get(\"name\")\nself.transformer_config = filter_dict(transformer, \"name\")\nelif isinstance(transformer, str):\nself.transformer_name = transformer\nself.transformer_config = {}\nelse:\nif transformer is not None:\nwarnings.warn(\nf\"Invalid transformer config '{transformer}' for column '{self.name}', ignoring transformer for this column\"\n)\nself.transformer_name = None\nself.transformer_config = {}\nif not self.transformer_name:\nreturn self._infer_transformer()\nelse:\ntry:\nreturn eval(self.transformer_name)(**self.transformer_config)\nexcept:\nwarnings.warn(\nf\"Invalid transformer '{self.transformer_name}' or config '{self.transformer_config}' for column '{self.name}', ignoring transformer for this column\"\n)\nreturn self._infer_transformer()\ndef _infer_transformer(self) -&gt; ColumnTransformer:\nif self.categorical:\ntransformer = OHECategoricalTransformer(**self.transformer_config)\nelse:\ntransformer = ClusterContinuousTransformer(**self.transformer_config)\nif self.dtype.kind == \"M\":\ntransformer = DatetimeTransformer(transformer)\nreturn transformer\ndef __init__(self, data: pd.DataFrame, metadata: Optional[dict] = {}):\nself.columns: pd.Index = data.columns\nself.raw_metadata: dict = metadata\nif set(self.raw_metadata[\"columns\"].keys()) - set(self.columns):\nraise ValueError(\"Metadata contains keys that do not appear amongst the columns.\")\nself.dropped_columns = [cn for cn in self.columns if self.raw_metadata[\"columns\"].get(cn, None) == \"drop\"]\nself.columns = self.columns.drop(self.dropped_columns)\nself._metadata = {\ncn: self.ColumnMetaData(cn, data[cn], self.raw_metadata[\"columns\"].get(cn, {})) for cn in self.columns\n}\nself.constraints = ConstraintGraph(self.raw_metadata.get(\"constraints\", []), self.columns, self._metadata)\ndef __getitem__(self, key: str) -&gt; dict[str, Any]:\nreturn self._metadata[key]\ndef __iter__(self) -&gt; Iterator:\nreturn iter(self._metadata.values())\ndef __repr__(self) -&gt; None:\nreturn yaml.dump(self._metadata, default_flow_style=False, sort_keys=False)\n@classmethod\ndef from_path(cls, data: pd.DataFrame, path_str: str):\n\"\"\"\n        Instantiate a MetaData object from a YAML file via a specified path.\n        Args:\n            data: The data to be used to infer / validate the metadata.\n            path_str: The path to the metadata YAML file.\n        Returns:\n            The metadata object.\n        \"\"\"\npath = pathlib.Path(path_str)\nif path.exists():\nwith open(path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out the expanded alias/anchor group as it is not needed\nmetadata = filter_dict(metadata, {\"column_types\"})\nelse:\nwarnings.warn(f\"No metadata found at {path}...\")\nmetadata = {\"columns\": {}}\nreturn cls(data, metadata)\ndef _collapse(self, metadata: dict) -&gt; dict:\n\"\"\"\n        Given a metadata dictionary, rewrite to collapse duplicate column types in order to leverage YAML anchors and shrink the file.\n        Args:\n            metadata: The metadata dictionary to be rewritten.\n        Returns:\n            A rewritten metadata dictionary with collapsed column types and transformers.\n                The returned dictionary has the following structure:\n                {\n                    \"column_types\": dict,\n                    **metadata  # one entry for each column in \"columns\" that now reference the dicts above\n                }\n                - \"column_types\" is a dictionary mapping column type indices to column type configurations.\n                - \"**metadata\" contains the original metadata dictionary, with column types rewritten to use the indices and \"column_types\".\n        \"\"\"\nc_index = 1\ncolumn_types = {}\ncolumn_type_counts = {}\nfor cn, cd in metadata[\"columns\"].items():\nif cd not in column_types.values():\ncolumn_types[c_index] = cd if isinstance(cd, str) else cd.copy()\ncolumn_type_counts[c_index] = 1\nc_index += 1\nelse:\ncix = get_key_by_value(column_types, cd)\ncolumn_type_counts[cix] += 1\nfor cn, cd in metadata[\"columns\"].items():\ncix = get_key_by_value(column_types, cd)\nif column_type_counts[cix] &gt; 1:\nmetadata[\"columns\"][cn] = column_types[cix]\nelse:\ncolumn_types.pop(cix)\nreturn {\"column_types\": {i + 1: x for i, x in enumerate(column_types.values())}, **metadata}\ndef _assemble(self, collapse_yaml: bool) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Rearrange the metadata into a dictionary that can be written to a YAML file.\n        Args:\n            collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n        Returns:\n            A dictionary containing the assembled metadata.\n        \"\"\"\nassembled_metadata = {\n\"columns\": {\ncn: {\n\"dtype\": cmd.dtype.name\nif not hasattr(cmd, \"datetime_config\")\nelse {\"name\": cmd.dtype.name, **cmd.datetime_config},\n\"categorical\": cmd.categorical,\n}\nfor cn, cmd in self._metadata.items()\n}\n}\n# We loop through the base dict above to add other parts if they are present in the metadata\nfor cn, cmd in self._metadata.items():\nif cmd.missingness_strategy:\nassembled_metadata[\"columns\"][cn][\"missingness\"] = (\ncmd.missingness_strategy.name\nif cmd.missingness_strategy.name != \"impute\"\nelse {\"name\": cmd.missingness_strategy.name, \"impute\": cmd.missingness_strategy.impute}\n)\nif cmd.transformer_config:\nassembled_metadata[\"columns\"][cn][\"transformer\"] = {\n**cmd.transformer_config,\n\"name\": cmd.transformer.__class__.__name__,\n}\n# Add back the dropped_columns not present in the metadata\nif self.dropped_columns:\nassembled_metadata[\"columns\"].update({cn: \"drop\" for cn in self.dropped_columns})\nif collapse_yaml:\nassembled_metadata = self._collapse(assembled_metadata)\n# We add the constraints section after all of the formatting and processing above\n# In general, the constraints are kept the same as the input (provided they passed validation)\n# If `collapse_yaml` is specified, we output the minimum set of equivalent constraints\nif self.constraints:\nassembled_metadata[\"constraints\"] = (\n[str(c) for c in self.constraints.minimal_constraints]\nif collapse_yaml\nelse self.constraints.raw_constraint_strings\n)\nreturn assembled_metadata\ndef save(self, path: pathlib.Path, collapse_yaml: bool) -&gt; None:\n\"\"\"\n        Writes metadata to a YAML file.\n        Args:\n            path: The path at which to write the metadata YAML file.\n            collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n        \"\"\"\nwith open(path, \"w\") as yaml_file:\nyaml.safe_dump(\nself._assemble(collapse_yaml),\nyaml_file,\ndefault_flow_style=False,\nsort_keys=False,\n)\ndef get_sdv_metadata(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n        Map combinations of our metadata implementation to SDV's as required by SDMetrics.\n        Returns:\n            A dictionary containing the SDV metadata.\n        \"\"\"\nsdv_metadata = {\n\"columns\": {\ncn: {\n\"sdtype\": \"boolean\"\nif cmd.boolean\nelse \"categorical\"\nif cmd.categorical\nelse \"datetime\"\nif cmd.dtype.kind == \"M\"\nelse \"numerical\",\n}\nfor cn, cmd in self._metadata.items()\n}\n}\nfor cn, cmd in self._metadata.items():\nif cmd.dtype.kind == \"M\":\nsdv_metadata[\"columns\"][cn][\"format\"] = cmd.datetime_config[\"format\"]\nreturn sdv_metadata\ndef save_constraint_graphs(self, path: pathlib.Path) -&gt; None:\n\"\"\"\n        Output the constraint graphs as HTML files.\n        Args:\n            path: The path at which to write the constraint graph HTML files.\n        \"\"\"\nself.constraints._output_graphs_html(path)\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.MetaData.ColumnMetaData","title":"<code>ColumnMetaData</code>","text":"Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>class ColumnMetaData:\ndef __init__(self, name: str, data: pd.Series, raw: dict) -&gt; None:\nself.name = name\nself.dtype: np.dtype = self._validate_dtype(data, raw.get(\"dtype\"))\nself.categorical: bool = self._validate_categorical(data, raw.get(\"categorical\"))\nself.missingness_strategy: GenericMissingnessStrategy = self._validate_missingness_strategy(\nraw.get(\"missingness\")\n)\nself.transformer: ColumnTransformer = self._validate_transformer(raw.get(\"transformer\"))\ndef _validate_dtype(self, data: pd.Series, dtype_raw: Optional[Union[dict, str]] = None) -&gt; np.dtype:\nif isinstance(dtype_raw, dict):\ndtype_name = dtype_raw.pop(\"name\", None)\nelif isinstance(dtype_raw, str):\ndtype_name = dtype_raw\nelse:\ndtype_name = self._infer_dtype(data)\ntry:\ndtype = np.dtype(dtype_name)\nexcept TypeError:\nwarnings.warn(\nf\"Invalid dtype specification '{dtype_name}' for column '{self.name}', ignoring dtype for this column\"\n)\ndtype = self._infer_dtype(data)\nif dtype.kind == \"M\":\nself._setup_datetime_config(data, dtype_raw)\nelif dtype.kind in [\"f\", \"i\", \"u\"]:\nself.rounding_scheme = self._validate_rounding_scheme(data, dtype, dtype_raw)\nreturn dtype\ndef _infer_dtype(self, data: pd.Series) -&gt; np.dtype:\nreturn data.dtype.name\ndef _infer_datetime_format(self, data: pd.Series) -&gt; str:\nreturn _guess_datetime_format_for_array(data[data.notna()].astype(str).to_numpy())\ndef _setup_datetime_config(self, data: pd.Series, datetime_config: dict) -&gt; dict:\n\"\"\"\n        Add keys to `datetime_config` corresponding to args from the `pd.to_datetime` function\n        (see [the docs](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html))\n        \"\"\"\nif not isinstance(datetime_config, dict):\ndatetime_config = {}\nelse:\ndatetime_config = filter_dict(datetime_config, {\"format\", \"floor\"}, include=True)\nif \"format\" not in datetime_config:\ndatetime_config[\"format\"] = self._infer_datetime_format(data)\nself.datetime_config = datetime_config\ndef _validate_rounding_scheme(self, data: pd.Series, dtype: np.dtype, dtype_dict: dict) -&gt; int:\nif dtype_dict and \"rounding_scheme\" in dtype_dict:\nreturn dtype_dict[\"rounding_scheme\"]\nelse:\nif dtype.kind != \"f\":\nreturn 1.0\nroundable_data = data[data.notna()]\nfor i in range(np.finfo(dtype).precision):\nif (roundable_data.round(i) == roundable_data).all():\nreturn 10**-i\nreturn None\ndef _validate_categorical(self, data: pd.Series, categorical: Optional[bool] = None) -&gt; bool:\nif categorical is None:\nreturn self._infer_categorical(data)\nelif not isinstance(categorical, bool):\nwarnings.warn(\nf\"Invalid categorical '{categorical}' for column '{self.name}', ignoring categorical for this column\"\n)\nreturn self._infer_categorical(data)\nelse:\nself.boolean = data.nunique() &lt;= 2\nreturn categorical\ndef _infer_categorical(self, data: pd.Series) -&gt; bool:\nself.boolean = data.nunique() &lt;= 2\nreturn data.nunique() &lt;= 10 or self.dtype.kind == \"O\"\ndef _validate_missingness_strategy(self, missingness_strategy: Optional[Union[dict, str]]) -&gt; tuple[str, dict]:\nif not missingness_strategy:\nreturn None\nif isinstance(missingness_strategy, dict):\nimpute = missingness_strategy.get(\"impute\", None)\nstrategy = \"impute\" if impute else missingness_strategy.get(\"strategy\", None)\nelse:\nstrategy = missingness_strategy\nif (\nstrategy not in MISSINGNESS_STRATEGIES\nor (strategy == \"impute\" and impute == \"mean\" and self.dtype.kind != \"f\")\nor (strategy == \"impute\" and not impute)\n):\nwarnings.warn(\nf\"Invalid missingness strategy '{missingness_strategy}' for column '{self.name}', ignoring missingness strategy for this column\"\n)\nreturn None\nreturn (\nMISSINGNESS_STRATEGIES[strategy](impute) if strategy == \"impute\" else MISSINGNESS_STRATEGIES[strategy]()\n)\ndef _validate_transformer(self, transformer: Optional[Union[dict, str]] = {}) -&gt; tuple[str, dict]:\n# if transformer is neither a dict nor a str statement below will raise a TypeError\nif isinstance(transformer, dict):\nself.transformer_name = transformer.get(\"name\")\nself.transformer_config = filter_dict(transformer, \"name\")\nelif isinstance(transformer, str):\nself.transformer_name = transformer\nself.transformer_config = {}\nelse:\nif transformer is not None:\nwarnings.warn(\nf\"Invalid transformer config '{transformer}' for column '{self.name}', ignoring transformer for this column\"\n)\nself.transformer_name = None\nself.transformer_config = {}\nif not self.transformer_name:\nreturn self._infer_transformer()\nelse:\ntry:\nreturn eval(self.transformer_name)(**self.transformer_config)\nexcept:\nwarnings.warn(\nf\"Invalid transformer '{self.transformer_name}' or config '{self.transformer_config}' for column '{self.name}', ignoring transformer for this column\"\n)\nreturn self._infer_transformer()\ndef _infer_transformer(self) -&gt; ColumnTransformer:\nif self.categorical:\ntransformer = OHECategoricalTransformer(**self.transformer_config)\nelse:\ntransformer = ClusterContinuousTransformer(**self.transformer_config)\nif self.dtype.kind == \"M\":\ntransformer = DatetimeTransformer(transformer)\nreturn transformer\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.MetaData.from_path","title":"<code>from_path(data, path_str)</code>  <code>classmethod</code>","text":"<p>Instantiate a MetaData object from a YAML file via a specified path.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to be used to infer / validate the metadata.</p> required <code>path_str</code> <code>str</code> <p>The path to the metadata YAML file.</p> required <p>Returns:</p> Type Description <p>The metadata object.</p> Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>@classmethod\ndef from_path(cls, data: pd.DataFrame, path_str: str):\n\"\"\"\n    Instantiate a MetaData object from a YAML file via a specified path.\n    Args:\n        data: The data to be used to infer / validate the metadata.\n        path_str: The path to the metadata YAML file.\n    Returns:\n        The metadata object.\n    \"\"\"\npath = pathlib.Path(path_str)\nif path.exists():\nwith open(path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out the expanded alias/anchor group as it is not needed\nmetadata = filter_dict(metadata, {\"column_types\"})\nelse:\nwarnings.warn(f\"No metadata found at {path}...\")\nmetadata = {\"columns\": {}}\nreturn cls(data, metadata)\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.MetaData.get_sdv_metadata","title":"<code>get_sdv_metadata()</code>","text":"<p>Map combinations of our metadata implementation to SDV's as required by SDMetrics.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, dict[str, str]]]</code> <p>A dictionary containing the SDV metadata.</p> Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def get_sdv_metadata(self) -&gt; dict[str, dict[str, dict[str, str]]]:\n\"\"\"\n    Map combinations of our metadata implementation to SDV's as required by SDMetrics.\n    Returns:\n        A dictionary containing the SDV metadata.\n    \"\"\"\nsdv_metadata = {\n\"columns\": {\ncn: {\n\"sdtype\": \"boolean\"\nif cmd.boolean\nelse \"categorical\"\nif cmd.categorical\nelse \"datetime\"\nif cmd.dtype.kind == \"M\"\nelse \"numerical\",\n}\nfor cn, cmd in self._metadata.items()\n}\n}\nfor cn, cmd in self._metadata.items():\nif cmd.dtype.kind == \"M\":\nsdv_metadata[\"columns\"][cn][\"format\"] = cmd.datetime_config[\"format\"]\nreturn sdv_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.MetaData.save","title":"<code>save(path, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def save(self, path: pathlib.Path, collapse_yaml: bool) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n    Args:\n        path: The path at which to write the metadata YAML file.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\nwith open(path, \"w\") as yaml_file:\nyaml.safe_dump(\nself._assemble(collapse_yaml),\nyaml_file,\ndefault_flow_style=False,\nsort_keys=False,\n)\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.MetaData.save_constraint_graphs","title":"<code>save_constraint_graphs(path)</code>","text":"<p>Output the constraint graphs as HTML files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path at which to write the constraint graph HTML files.</p> required Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def save_constraint_graphs(self, path: pathlib.Path) -&gt; None:\n\"\"\"\n    Output the constraint graphs as HTML files.\n    Args:\n        path: The path at which to write the constraint graph HTML files.\n    \"\"\"\nself.constraints._output_graphs_html(path)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/","title":"metatransformer","text":""},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code>MetaTransformer</code>","text":"<p>The metatransformer is responsible for transforming input dataset into a format that can be used by the <code>model</code> module, and for transforming this module's output back to the original format of the input dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <code>metadata</code> <code>Optional[MetaData]</code> <p>Optionally, a <code>MetaData</code> object containing the metadata for the dataset. If this is not provided it will be inferred from the dataset.</p> <code>None</code> <code>missingness_strategy</code> <code>Optional[str]</code> <p>The missingness strategy to use. Defaults to augmenting missing values in the data, see the missingness strategies for more information.</p> <code>'augment'</code> <code>impute_value</code> <code>Optional[Any]</code> <p>Only used when <code>missingness_strategy</code> is set to 'impute'. The value to use when imputing missing values in the data.</p> <code>None</code> <p>After calling <code>MetaTransformer.apply()</code>, the following attributes and methods will be available:</p> <p>Attributes:</p> Name Type Description <code>typed_dataset</code> <code>DataFrame</code> <p>The dataset with the dtypes applied.</p> <code>post_missingness_strategy_dataset</code> <code>DataFrame</code> <p>The dataset with the missingness strategies applied.</p> <code>transformed_dataset</code> <code>DataFrame</code> <p>The transformed dataset.</p> <code>single_column_indices</code> <code>list[int]</code> <p>The indices of the columns that were transformed into a single column.</p> <code>multi_column_indices</code> <code>list[list[int]]</code> <p>The indices of the columns that were transformed into multiple columns.</p> <p>Methods:</p> <ul> <li><code>get_typed_dataset()</code>: Returns the typed dataset.</li> <li><code>get_prepared_dataset()</code>: Returns the dataset with the missingness strategies applied.</li> <li><code>get_transformed_dataset()</code>: Returns the transformed dataset.</li> <li><code>get_multi_and_single_column_indices()</code>: Returns the indices of the columns that were transformed into one or multiple column(s).</li> <li><code>get_sdv_metadata()</code>: Returns the metadata in the correct format for SDMetrics.</li> <li><code>save_metadata()</code>: Saves the metadata to a file.</li> <li><code>save_constraint_graphs()</code>: Saves the constraint graphs to a file.</li> </ul> <p>Note that <code>mt.apply</code> is a helper function that runs <code>mt.apply_dtypes</code>, <code>mt.apply_missingness_strategy</code> and <code>mt.transform</code> in sequence. This is the recommended way to use the MetaTransformer to ensure that it is fully instantiated for use downstream.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    The metatransformer is responsible for transforming input dataset into a format that can be used by the `model` module, and for transforming\n    this module's output back to the original format of the input dataset.\n    Args:\n        dataset: The raw input DataFrame.\n        metadata: Optionally, a [`MetaData`][nhssynth.modules.dataloader.metadata.MetaData] object containing the metadata for the dataset. If this is not provided it will be inferred from the dataset.\n        missingness_strategy: The missingness strategy to use. Defaults to augmenting missing values in the data, see [the missingness strategies][nhssynth.modules.dataloader.missingness] for more information.\n        impute_value: Only used when `missingness_strategy` is set to 'impute'. The value to use when imputing missing values in the data.\n    After calling `MetaTransformer.apply()`, the following attributes and methods will be available:\n    Attributes:\n        typed_dataset (pd.DataFrame): The dataset with the dtypes applied.\n        post_missingness_strategy_dataset (pd.DataFrame): The dataset with the missingness strategies applied.\n        transformed_dataset (pd.DataFrame): The transformed dataset.\n        single_column_indices (list[int]): The indices of the columns that were transformed into a single column.\n        multi_column_indices (list[list[int]]): The indices of the columns that were transformed into multiple columns.\n    **Methods:**\n    - `get_typed_dataset()`: Returns the typed dataset.\n    - `get_prepared_dataset()`: Returns the dataset with the missingness strategies applied.\n    - `get_transformed_dataset()`: Returns the transformed dataset.\n    - `get_multi_and_single_column_indices()`: Returns the indices of the columns that were transformed into one or multiple column(s).\n    - `get_sdv_metadata()`: Returns the metadata in the correct format for SDMetrics.\n    - `save_metadata()`: Saves the metadata to a file.\n    - `save_constraint_graphs()`: Saves the constraint graphs to a file.\n    Note that `mt.apply` is a helper function that runs `mt.apply_dtypes`, `mt.apply_missingness_strategy` and `mt.transform` in sequence.\n    This is the recommended way to use the MetaTransformer to ensure that it is fully instantiated for use downstream.\n    \"\"\"\ndef __init__(\nself,\ndataset: pd.DataFrame,\nmetadata: Optional[MetaData] = None,\nmissingness_strategy: Optional[str] = \"augment\",\nimpute_value: Optional[Any] = None,\n):\nself._raw_dataset: pd.DataFrame = dataset\nself._metadata: MetaData = metadata or MetaData(dataset)\nif missingness_strategy == \"impute\":\nassert (\nimpute_value is not None\n), \"`impute_value` must be specified when using the imputation missingness strategy\"\nself._missingness_strategy = self._impute_missingness_strategy_generator(impute_value)\nelse:\nself._missingness_strategy = MISSINGNESS_STRATEGIES[missingness_strategy]\ndef _impute_missingness_strategy_generator(self, impute_value: Any) -&gt; Callable[[], ImputeMissingnessStrategy]:\n\"\"\"\n        Create a function to return a new instance of the impute missingness strategy with the given impute value.\n        Args:\n            impute_value: The value to use when imputing missing values in the data.\n        Returns:\n            A function that returns a new instance of the impute missingness strategy with the given impute value.\n        \"\"\"\ndef _impute_missingness_strategy() -&gt; ImputeMissingnessStrategy:\nreturn ImputeMissingnessStrategy(impute_value)\nreturn _impute_missingness_strategy\n@classmethod\ndef from_path(cls, dataset: pd.DataFrame, metadata_path: str, **kwargs) -&gt; Self:\n\"\"\"\n        Instantiates a MetaTransformer from a metadata file via a provided path.\n        Args:\n            dataset: The raw input DataFrame.\n            metadata_path: The path to the metadata file.\n        Returns:\n            A MetaTransformer object.\n        \"\"\"\nreturn cls(dataset, MetaData.from_path(dataset, metadata_path), **kwargs)\n@classmethod\ndef from_dict(cls, dataset: pd.DataFrame, metadata: dict, **kwargs) -&gt; Self:\n\"\"\"\n        Instantiates a MetaTransformer from a metadata dictionary.\n        Args:\n            dataset: The raw input DataFrame.\n            metadata: A dictionary of raw metadata.\n        Returns:\n            A MetaTransformer object.\n        \"\"\"\nreturn cls(dataset, MetaData(dataset, metadata), **kwargs)\ndef drop_columns(self) -&gt; None:\n\"\"\"\n        Drops columns from the dataset that are not in the `MetaData`.\n        \"\"\"\nself._raw_dataset = self._raw_dataset[self._metadata.columns]\ndef _apply_rounding_scheme(self, working_column: pd.Series, rounding_scheme: float) -&gt; pd.Series:\n\"\"\"\n        A rounding scheme takes the form of the smallest value that should be rounded to 0, i.e. 0.01 for 2dp.\n        We first round to the nearest multiple in the standard way, through dividing, rounding and then multiplying.\n        However, this can lead to floating point errors, so we then round to the number of decimal places required by the rounding scheme.\n        e.g. `np.round(0.15 / 0.1) * 0.1` will erroneously return 0.1.\n        Args:\n            working_column: The column to apply the rounding scheme to.\n            rounding_scheme: The rounding scheme to apply.\n        Returns:\n            The column with the rounding scheme applied.\n        \"\"\"\nworking_column = np.round(working_column / rounding_scheme) * rounding_scheme\nreturn working_column.round(max(0, int(np.ceil(np.log10(1 / rounding_scheme)))))\ndef _apply_dtype(\nself,\nworking_column: pd.Series,\ncolumn_metadata: MetaData.ColumnMetaData,\n) -&gt; pd.Series:\n\"\"\"\n        Given a `working_column`, the dtype specified in the `column_metadata` is applied to it.\n         - Datetime columns are floored, and their format is inferred.\n         - Rounding schemes are applied to numeric columns if specified.\n         - Columns with missing values have their dtype converted to the pandas equivalent to allow for NA values.\n        Args:\n            working_column: The column to apply the dtype to.\n            column_metadata: The metadata for the column.\n        Returns:\n            The column with the dtype applied.\n        \"\"\"\ndtype = column_metadata.dtype\ntry:\nif dtype.kind == \"M\":\nworking_column = pd.to_datetime(working_column, format=column_metadata.datetime_config.get(\"format\"))\nif column_metadata.datetime_config.get(\"floor\"):\nworking_column = working_column.dt.floor(column_metadata.datetime_config.get(\"floor\"))\ncolumn_metadata.datetime_config[\"format\"] = column_metadata._infer_datetime_format(working_column)\nreturn working_column\nelse:\nif hasattr(column_metadata, \"rounding_scheme\") and column_metadata.rounding_scheme is not None:\nworking_column = self._apply_rounding_scheme(working_column, column_metadata.rounding_scheme)\n# If there are missing values in the column, we need to use the pandas equivalent of the dtype to allow for NA values\nif working_column.isnull().any() and dtype.kind in [\"i\", \"u\", \"f\"]:\nreturn working_column.astype(dtype.name.capitalize())\nelse:\nreturn working_column.astype(dtype)\nexcept:\nraise ValueError(f\"{sys.exc_info()[1]}\\nError applying dtype '{dtype}' to column '{working_column.name}'\")\ndef apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `dataset`.\n        Returns:\n            The dataset with the dtypes applied.\n        \"\"\"\nworking_data = data.copy()\nfor column_metadata in self._metadata:\nworking_data[column_metadata.name] = self._apply_dtype(working_data[column_metadata.name], column_metadata)\nreturn working_data\ndef apply_missingness_strategy(self) -&gt; pd.DataFrame:\n\"\"\"\n        Resolves missingness in the dataset via the `MetaTransformer`'s global missingness strategy or\n        column-wise missingness strategies. In the case of the `AugmentMissingnessStrategy`, the missingness\n        is not resolved, instead a new column / value is added for later transformation.\n        Returns:\n            The dataset with the missingness strategies applied.\n        \"\"\"\nworking_data = self.typed_dataset.copy()\nfor column_metadata in self._metadata:\nif not column_metadata.missingness_strategy:\ncolumn_metadata.missingness_strategy = self._missingness_strategy()\nif not working_data[column_metadata.name].isnull().any():\ncontinue\nworking_data = column_metadata.missingness_strategy.remove(working_data, column_metadata)\nreturn working_data\n# def apply_constraints(self) -&gt; pd.DataFrame:\n#     working_data = self.post_missingness_strategy_dataset.copy()\n#     for constraint in self._metadata.constraints:\n#         working_data = constraint.apply(working_data)\n#     return working_data\ndef _get_missingness_carrier(self, column_metadata: MetaData.ColumnMetaData) -&gt; Union[pd.Series, Any]:\n\"\"\"\n        In the case of the `AugmentMissingnessStrategy`, a `missingness_carrier` has been determined for each column.\n        For continuous columns this is an indicator column for the presence of NaN values.\n        For categorical columns this is the value to be used to represent missingness as a category.\n        Args:\n            column_metadata: The metadata for the column.\n        Returns:\n            The missingness carrier for the column.\n        \"\"\"\nmissingness_carrier = getattr(column_metadata.missingness_strategy, \"missingness_carrier\", None)\nif missingness_carrier in self.post_missingness_strategy_dataset.columns:\nreturn self.post_missingness_strategy_dataset[missingness_carrier]\nelse:\nreturn missingness_carrier\ndef transform(self) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the dataset by applying each of the columns' transformers and recording the indices of the single and multi columns.\n        Returns:\n            The transformed dataset.\n        \"\"\"\ntransformed_columns = []\nself.single_column_indices = []\nself.multi_column_indices = []\ncol_counter = 0\nworking_data = self.post_missingness_strategy_dataset.copy()\n# iteratively build the transformed df\nfor column_metadata in tqdm(\nself._metadata, desc=\"Transforming data\", unit=\"column\", total=len(self._metadata.columns)\n):\nmissingness_carrier = self._get_missingness_carrier(column_metadata)\ntransformed_data = column_metadata.transformer.apply(\nworking_data[column_metadata.name], missingness_carrier\n)\ntransformed_columns.append(transformed_data)\n# track single and multi column indices to supply to the model\nif isinstance(transformed_data, pd.DataFrame) and transformed_data.shape[1] &gt; 1:\nnum_to_add = transformed_data.shape[1]\nif not column_metadata.categorical:\nself.single_column_indices.append(col_counter)\ncol_counter += 1\nnum_to_add -= 1\nself.multi_column_indices.append(list(range(col_counter, col_counter + num_to_add)))\ncol_counter += num_to_add\nelse:\nself.single_column_indices.append(col_counter)\ncol_counter += 1\nreturn pd.concat(transformed_columns, axis=1)\ndef apply(self) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n        Returns:\n            The transformed dataset.\n        \"\"\"\nself.drop_columns()\nself.typed_dataset = self.apply_dtypes(self._raw_dataset)\nself.post_missingness_strategy_dataset = self.apply_missingness_strategy()\n# self.constrained_dataset = self.apply_constraints()\nself.transformed_dataset = self.transform()\nreturn self.transformed_dataset\ndef inverse_apply(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n        Args:\n            dataset: The transformed dataset.\n        Returns:\n            The original dataset.\n        \"\"\"\nfor column_metadata in self._metadata:\ndataset = column_metadata.transformer.revert(dataset)\nreturn self.apply_dtypes(dataset)\ndef get_typed_dataset(self) -&gt; pd.DataFrame:\nif not hasattr(self, \"typed_dataset\"):\nraise ValueError(\n\"The typed dataset has not yet been created. Call `mt.apply()` (or `mt.apply_dtypes()`) first.\"\n)\nreturn self.typed_dataset\ndef get_prepared_dataset(self) -&gt; pd.DataFrame:\nif not hasattr(self, \"prepared_dataset\"):\nraise ValueError(\n\"The prepared dataset has not yet been created. Call `mt.apply()` (or `mt.apply_missingness_strategy()`) first.\"\n)\nreturn self.prepared_dataset\ndef get_transformed_dataset(self) -&gt; pd.DataFrame:\nif not hasattr(self, \"transformed_dataset\"):\nraise ValueError(\n\"The prepared dataset has not yet been created. Call `mt.apply()` (or `mt.transform()`) first.\"\n)\nreturn self.transformed_dataset\ndef get_multi_and_single_column_indices(self) -&gt; tuple[list[int], list[int]]:\n\"\"\"\n        Returns the indices of the columns that were transformed into one or multiple column(s).\n        Returns:\n            A tuple containing the indices of the single and multi columns.\n        \"\"\"\nif not hasattr(self, \"multi_column_indices\") or not hasattr(self, \"single_column_indices\"):\nraise ValueError(\n\"The single and multi column indices have not yet been created. Call `mt.apply()` (or `mt.transform()`) first.\"\n)\nreturn self.multi_column_indices, self.single_column_indices\ndef get_sdv_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Calls the `MetaData` method to reformat its contents into the correct format for use with SDMetrics.\n        Returns:\n            The metadata in the correct format for SDMetrics.\n        \"\"\"\nreturn self._metadata.get_sdv_metadata()\ndef save_metadata(self, path: pathlib.Path, collapse_yaml: bool = False) -&gt; None:\nreturn self._metadata.save(path, collapse_yaml)\ndef save_constraint_graphs(self, path: pathlib.Path) -&gt; None:\nreturn self._metadata.constraints._output_graphs_html(path)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply()</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed dataset.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def apply(self) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n    Returns:\n        The transformed dataset.\n    \"\"\"\nself.drop_columns()\nself.typed_dataset = self.apply_dtypes(self._raw_dataset)\nself.post_missingness_strategy_dataset = self.apply_missingness_strategy()\n# self.constrained_dataset = self.apply_constraints()\nself.transformed_dataset = self.transform()\nreturn self.transformed_dataset\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(data)</code>","text":"<p>Applies dtypes from the metadata to <code>dataset</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset with the dtypes applied.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `dataset`.\n    Returns:\n        The dataset with the dtypes applied.\n    \"\"\"\nworking_data = data.copy()\nfor column_metadata in self._metadata:\nworking_data[column_metadata.name] = self._apply_dtype(working_data[column_metadata.name], column_metadata)\nreturn working_data\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_missingness_strategy","title":"<code>apply_missingness_strategy()</code>","text":"<p>Resolves missingness in the dataset via the <code>MetaTransformer</code>'s global missingness strategy or column-wise missingness strategies. In the case of the <code>AugmentMissingnessStrategy</code>, the missingness is not resolved, instead a new column / value is added for later transformation.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset with the missingness strategies applied.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def apply_missingness_strategy(self) -&gt; pd.DataFrame:\n\"\"\"\n    Resolves missingness in the dataset via the `MetaTransformer`'s global missingness strategy or\n    column-wise missingness strategies. In the case of the `AugmentMissingnessStrategy`, the missingness\n    is not resolved, instead a new column / value is added for later transformation.\n    Returns:\n        The dataset with the missingness strategies applied.\n    \"\"\"\nworking_data = self.typed_dataset.copy()\nfor column_metadata in self._metadata:\nif not column_metadata.missingness_strategy:\ncolumn_metadata.missingness_strategy = self._missingness_strategy()\nif not working_data[column_metadata.name].isnull().any():\ncontinue\nworking_data = column_metadata.missingness_strategy.remove(working_data, column_metadata)\nreturn working_data\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.drop_columns","title":"<code>drop_columns()</code>","text":"<p>Drops columns from the dataset that are not in the <code>MetaData</code>.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def drop_columns(self) -&gt; None:\n\"\"\"\n    Drops columns from the dataset that are not in the `MetaData`.\n    \"\"\"\nself._raw_dataset = self._raw_dataset[self._metadata.columns]\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.from_dict","title":"<code>from_dict(dataset, metadata, **kwargs)</code>  <code>classmethod</code>","text":"<p>Instantiates a MetaTransformer from a metadata dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <code>metadata</code> <code>dict</code> <p>A dictionary of raw metadata.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A MetaTransformer object.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>@classmethod\ndef from_dict(cls, dataset: pd.DataFrame, metadata: dict, **kwargs) -&gt; Self:\n\"\"\"\n    Instantiates a MetaTransformer from a metadata dictionary.\n    Args:\n        dataset: The raw input DataFrame.\n        metadata: A dictionary of raw metadata.\n    Returns:\n        A MetaTransformer object.\n    \"\"\"\nreturn cls(dataset, MetaData(dataset, metadata), **kwargs)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.from_path","title":"<code>from_path(dataset, metadata_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Instantiates a MetaTransformer from a metadata file via a provided path.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <code>metadata_path</code> <code>str</code> <p>The path to the metadata file.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A MetaTransformer object.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>@classmethod\ndef from_path(cls, dataset: pd.DataFrame, metadata_path: str, **kwargs) -&gt; Self:\n\"\"\"\n    Instantiates a MetaTransformer from a metadata file via a provided path.\n    Args:\n        dataset: The raw input DataFrame.\n        metadata_path: The path to the metadata file.\n    Returns:\n        A MetaTransformer object.\n    \"\"\"\nreturn cls(dataset, MetaData.from_path(dataset, metadata_path), **kwargs)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_multi_and_single_column_indices","title":"<code>get_multi_and_single_column_indices()</code>","text":"<p>Returns the indices of the columns that were transformed into one or multiple column(s).</p> <p>Returns:</p> Type Description <code>tuple[list[int], list[int]]</code> <p>A tuple containing the indices of the single and multi columns.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def get_multi_and_single_column_indices(self) -&gt; tuple[list[int], list[int]]:\n\"\"\"\n    Returns the indices of the columns that were transformed into one or multiple column(s).\n    Returns:\n        A tuple containing the indices of the single and multi columns.\n    \"\"\"\nif not hasattr(self, \"multi_column_indices\") or not hasattr(self, \"single_column_indices\"):\nraise ValueError(\n\"The single and multi column indices have not yet been created. Call `mt.apply()` (or `mt.transform()`) first.\"\n)\nreturn self.multi_column_indices, self.single_column_indices\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_sdv_metadata","title":"<code>get_sdv_metadata()</code>","text":"<p>Calls the <code>MetaData</code> method to reformat its contents into the correct format for use with SDMetrics.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>The metadata in the correct format for SDMetrics.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def get_sdv_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Calls the `MetaData` method to reformat its contents into the correct format for use with SDMetrics.\n    Returns:\n        The metadata in the correct format for SDMetrics.\n    \"\"\"\nreturn self._metadata.get_sdv_metadata()\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(dataset)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>The transformed dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original dataset.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, dataset: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n    Args:\n        dataset: The transformed dataset.\n    Returns:\n        The original dataset.\n    \"\"\"\nfor column_metadata in self._metadata:\ndataset = column_metadata.transformer.revert(dataset)\nreturn self.apply_dtypes(dataset)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.transform","title":"<code>transform()</code>","text":"<p>Prepares the dataset by applying each of the columns' transformers and recording the indices of the single and multi columns.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed dataset.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def transform(self) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the dataset by applying each of the columns' transformers and recording the indices of the single and multi columns.\n    Returns:\n        The transformed dataset.\n    \"\"\"\ntransformed_columns = []\nself.single_column_indices = []\nself.multi_column_indices = []\ncol_counter = 0\nworking_data = self.post_missingness_strategy_dataset.copy()\n# iteratively build the transformed df\nfor column_metadata in tqdm(\nself._metadata, desc=\"Transforming data\", unit=\"column\", total=len(self._metadata.columns)\n):\nmissingness_carrier = self._get_missingness_carrier(column_metadata)\ntransformed_data = column_metadata.transformer.apply(\nworking_data[column_metadata.name], missingness_carrier\n)\ntransformed_columns.append(transformed_data)\n# track single and multi column indices to supply to the model\nif isinstance(transformed_data, pd.DataFrame) and transformed_data.shape[1] &gt; 1:\nnum_to_add = transformed_data.shape[1]\nif not column_metadata.categorical:\nself.single_column_indices.append(col_counter)\ncol_counter += 1\nnum_to_add -= 1\nself.multi_column_indices.append(list(range(col_counter, col_counter + num_to_add)))\ncol_counter += num_to_add\nelse:\nself.single_column_indices.append(col_counter)\ncol_counter += 1\nreturn pd.concat(transformed_columns, axis=1)\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/","title":"missingness","text":""},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.AugmentMissingnessStrategy","title":"<code>AugmentMissingnessStrategy</code>","text":"<p>             Bases: <code>GenericMissingnessStrategy</code></p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>class AugmentMissingnessStrategy(GenericMissingnessStrategy):\ndef __init__(self) -&gt; None:\nsuper().__init__(\"augment\")\ndef remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"\n        Impute missingness with the model. To do this we create a new column for continuous features and a new category for categorical features.\n        Args:\n            data: The dataset.\n            column_metadata: The column metadata enabling the correct set up of the missingness strategy.\n        Returns:\n            The dataset, potentially with a new column representing the missingness for the column added.\n        \"\"\"\nif column_metadata.categorical:\nif column_metadata.dtype.kind == \"O\":\nself.missingness_carrier = column_metadata.name + \"_missing\"\nelse:\nself.missingness_carrier = data[column_metadata.name].min() - 1\nelse:\nself.missingness_carrier = column_metadata.name + \"_missing\"\ndata[self.missingness_carrier] = data[column_metadata.name].isnull().astype(int)\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.AugmentMissingnessStrategy.remove","title":"<code>remove(data, column_metadata)</code>","text":"<p>Impute missingness with the model. To do this we create a new column for continuous features and a new category for categorical features.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset.</p> required <code>column_metadata</code> <code>ColumnMetaData</code> <p>The column metadata enabling the correct set up of the missingness strategy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset, potentially with a new column representing the missingness for the column added.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>def remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"\n    Impute missingness with the model. To do this we create a new column for continuous features and a new category for categorical features.\n    Args:\n        data: The dataset.\n        column_metadata: The column metadata enabling the correct set up of the missingness strategy.\n    Returns:\n        The dataset, potentially with a new column representing the missingness for the column added.\n    \"\"\"\nif column_metadata.categorical:\nif column_metadata.dtype.kind == \"O\":\nself.missingness_carrier = column_metadata.name + \"_missing\"\nelse:\nself.missingness_carrier = data[column_metadata.name].min() - 1\nelse:\nself.missingness_carrier = column_metadata.name + \"_missing\"\ndata[self.missingness_carrier] = data[column_metadata.name].isnull().astype(int)\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.DropMissingnessStrategy","title":"<code>DropMissingnessStrategy</code>","text":"<p>             Bases: <code>GenericMissingnessStrategy</code></p> <p>Drop missingness strategy.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>class DropMissingnessStrategy(GenericMissingnessStrategy):\n\"\"\"Drop missingness strategy.\"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__(\"drop\")\ndef remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"\n        Drop rows containing missing values in the appropriate column.\n        Args:\n            data: The dataset.\n            column_metadata: The column metadata.\n        Returns:\n            The dataset with rows containing missing values in the appropriate column dropped.\n        \"\"\"\nreturn data.dropna(subset=[column_metadata.name]).reset_index(drop=True)\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.DropMissingnessStrategy.remove","title":"<code>remove(data, column_metadata)</code>","text":"<p>Drop rows containing missing values in the appropriate column.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset.</p> required <code>column_metadata</code> <code>ColumnMetaData</code> <p>The column metadata.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset with rows containing missing values in the appropriate column dropped.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>def remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"\n    Drop rows containing missing values in the appropriate column.\n    Args:\n        data: The dataset.\n        column_metadata: The column metadata.\n    Returns:\n        The dataset with rows containing missing values in the appropriate column dropped.\n    \"\"\"\nreturn data.dropna(subset=[column_metadata.name]).reset_index(drop=True)\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.GenericMissingnessStrategy","title":"<code>GenericMissingnessStrategy</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Generic missingness strategy.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>class GenericMissingnessStrategy(ABC):\n\"\"\"Generic missingness strategy.\"\"\"\ndef __init__(self, name: str) -&gt; None:\nsuper().__init__()\nself.name: str = name\n@abstractmethod\ndef remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"Remove missingness.\"\"\"\npass\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.GenericMissingnessStrategy.remove","title":"<code>remove(data, column_metadata)</code>  <code>abstractmethod</code>","text":"<p>Remove missingness.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>@abstractmethod\ndef remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"Remove missingness.\"\"\"\npass\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.ImputeMissingnessStrategy","title":"<code>ImputeMissingnessStrategy</code>","text":"<p>             Bases: <code>GenericMissingnessStrategy</code></p> <p>Impute missingness with mean strategy.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>class ImputeMissingnessStrategy(GenericMissingnessStrategy):\n\"\"\"Impute missingness with mean strategy.\"\"\"\ndef __init__(self, impute: Any) -&gt; None:\nsuper().__init__(\"impute\")\nself.impute = impute.lower() if isinstance(impute, str) else impute\ndef remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"\n        Impute missingness in the data via the `impute` strategy. 'Special' values trigger specific behaviour.\n        Args:\n            data: The dataset.\n            column_metadata: The column metadata.\n        Returns:\n            The dataset with missing values in the appropriate column replaced with imputed ones.\n        \"\"\"\nif self.impute == \"mean\":\nself.imputation_value = data[column_metadata.name].mean()\nelif self.impute == \"median\":\nself.imputation_value = data[column_metadata.name].median()\nelif self.impute == \"mode\":\nself.imputation_value = data[column_metadata.name].mode()[0]\nelse:\nself.imputation_value = self.impute\ndata[column_metadata.name].fillna(self.imputation_value, inplace=True)\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.ImputeMissingnessStrategy.remove","title":"<code>remove(data, column_metadata)</code>","text":"<p>Impute missingness in the data via the <code>impute</code> strategy. 'Special' values trigger specific behaviour.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset.</p> required <code>column_metadata</code> <code>ColumnMetaData</code> <p>The column metadata.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset with missing values in the appropriate column replaced with imputed ones.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>def remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"\n    Impute missingness in the data via the `impute` strategy. 'Special' values trigger specific behaviour.\n    Args:\n        data: The dataset.\n        column_metadata: The column metadata.\n    Returns:\n        The dataset with missing values in the appropriate column replaced with imputed ones.\n    \"\"\"\nif self.impute == \"mean\":\nself.imputation_value = data[column_metadata.name].mean()\nelif self.impute == \"median\":\nself.imputation_value = data[column_metadata.name].median()\nelif self.impute == \"mode\":\nself.imputation_value = data[column_metadata.name].mode()[0]\nelse:\nself.imputation_value = self.impute\ndata[column_metadata.name].fillna(self.imputation_value, inplace=True)\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.NullMissingnessStrategy","title":"<code>NullMissingnessStrategy</code>","text":"<p>             Bases: <code>GenericMissingnessStrategy</code></p> <p>Null missingness strategy.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>class NullMissingnessStrategy(GenericMissingnessStrategy):\n\"\"\"Null missingness strategy.\"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__(\"none\")\ndef remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"Do nothing.\"\"\"\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/missingness/#nhssynth.modules.dataloader.missingness.NullMissingnessStrategy.remove","title":"<code>remove(data, column_metadata)</code>","text":"<p>Do nothing.</p> Source code in <code>src/nhssynth/modules/dataloader/missingness.py</code> <pre><code>def remove(self, data: pd.DataFrame, column_metadata: ColumnMetaData) -&gt; pd.DataFrame:\n\"\"\"Do nothing.\"\"\"\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/run/","title":"run","text":""},{"location":"reference/modules/dataloader/transformers/","title":"transformers","text":""},{"location":"reference/modules/dataloader/transformers/base/","title":"base","text":""},{"location":"reference/modules/dataloader/transformers/base/#nhssynth.modules.dataloader.transformers.base.ColumnTransformer","title":"<code>ColumnTransformer</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A generic column transformer class to prototype all of the transformers applied via the <code>MetaTransformer</code>.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/base.py</code> <pre><code>class ColumnTransformer(ABC):\n\"\"\"A generic column transformer class to prototype all of the transformers applied via the [`MetaTransformer`][nhssynth.modules.dataloader.metatransformer.MetaTransformer].\"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__()\n@abstractmethod\ndef apply(self, data: pd.DataFrame, missingness_column: Optional[pd.Series]) -&gt; None:\n\"\"\"Apply the transformer to the data.\"\"\"\npass\n@abstractmethod\ndef revert(self, data: pd.DataFrame) -&gt; None:\n\"\"\"Revert data to pre-transformer state.\"\"\"\npass\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/base/#nhssynth.modules.dataloader.transformers.base.ColumnTransformer.apply","title":"<code>apply(data, missingness_column)</code>  <code>abstractmethod</code>","text":"<p>Apply the transformer to the data.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/base.py</code> <pre><code>@abstractmethod\ndef apply(self, data: pd.DataFrame, missingness_column: Optional[pd.Series]) -&gt; None:\n\"\"\"Apply the transformer to the data.\"\"\"\npass\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/base/#nhssynth.modules.dataloader.transformers.base.ColumnTransformer.revert","title":"<code>revert(data)</code>  <code>abstractmethod</code>","text":"<p>Revert data to pre-transformer state.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/base.py</code> <pre><code>@abstractmethod\ndef revert(self, data: pd.DataFrame) -&gt; None:\n\"\"\"Revert data to pre-transformer state.\"\"\"\npass\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/base/#nhssynth.modules.dataloader.transformers.base.TransformerWrapper","title":"<code>TransformerWrapper</code>","text":"<p>             Bases: <code>ABC</code></p> <p>A class to facilitate nesting of <code>ColumnTransformer</code>s.</p> <p>Parameters:</p> Name Type Description Default <code>wrapped_transformer</code> <code>ColumnTransformer</code> <p>The <code>ColumnTransformer</code> to wrap.</p> required Source code in <code>src/nhssynth/modules/dataloader/transformers/base.py</code> <pre><code>class TransformerWrapper(ABC):\n\"\"\"\n    A class to facilitate nesting of [`ColumnTransformer`][nhssynth.modules.dataloader.transformers.base.ColumnTransformer]s.\n    Args:\n        wrapped_transformer: The [`ColumnTransformer`][nhssynth.modules.dataloader.transformers.base.ColumnTransformer] to wrap.\n    \"\"\"\ndef __init__(self, wrapped_transformer: ColumnTransformer) -&gt; None:\nsuper().__init__()\nself._wrapped_transformer: ColumnTransformer = wrapped_transformer\ndef apply(self, data: pd.Series, missingness_column: Optional[pd.Series], **kwargs) -&gt; pd.DataFrame:\n\"\"\"Method for applying the wrapped transformer to the data.\"\"\"\nreturn self._wrapped_transformer.apply(data, missingness_column, **kwargs)\ndef revert(self, data: pd.Series, **kwargs) -&gt; pd.DataFrame:\n\"\"\"Method for reverting the passed data via the wrapped transformer.\"\"\"\nreturn self._wrapped_transformer.revert(data, **kwargs)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/base/#nhssynth.modules.dataloader.transformers.base.TransformerWrapper.apply","title":"<code>apply(data, missingness_column, **kwargs)</code>","text":"<p>Method for applying the wrapped transformer to the data.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/base.py</code> <pre><code>def apply(self, data: pd.Series, missingness_column: Optional[pd.Series], **kwargs) -&gt; pd.DataFrame:\n\"\"\"Method for applying the wrapped transformer to the data.\"\"\"\nreturn self._wrapped_transformer.apply(data, missingness_column, **kwargs)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/base/#nhssynth.modules.dataloader.transformers.base.TransformerWrapper.revert","title":"<code>revert(data, **kwargs)</code>","text":"<p>Method for reverting the passed data via the wrapped transformer.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/base.py</code> <pre><code>def revert(self, data: pd.Series, **kwargs) -&gt; pd.DataFrame:\n\"\"\"Method for reverting the passed data via the wrapped transformer.\"\"\"\nreturn self._wrapped_transformer.revert(data, **kwargs)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/categorical/","title":"categorical","text":""},{"location":"reference/modules/dataloader/transformers/categorical/#nhssynth.modules.dataloader.transformers.categorical.OHECategoricalTransformer","title":"<code>OHECategoricalTransformer</code>","text":"<p>             Bases: <code>ColumnTransformer</code></p> <p>A transformer to one-hot encode categorical features via sklearn's <code>OneHotEncoder</code>. Essentially wraps the <code>fit_transformer</code> and <code>inverse_transform</code> methods of <code>OneHotEncoder</code> to comply with the <code>ColumnTransformer</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>drop</code> <code>Optional[Union[list, str]]</code> <p>str or list of str, to pass to <code>OneHotEncoder</code>'s <code>drop</code> parameter.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>missing_value</code> <code>Any</code> <p>The value used to fill missing values in the data.</p> <p>After applying the transformer, the following attributes will be populated:</p> <p>Attributes:</p> Name Type Description <code>original_column_name</code> <p>The name of the original column.</p> <code>new_column_names</code> <p>The names of the columns generated by the transformer.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/categorical.py</code> <pre><code>class OHECategoricalTransformer(ColumnTransformer):\n\"\"\"\n    A transformer to one-hot encode categorical features via sklearn's `OneHotEncoder`.\n    Essentially wraps the `fit_transformer` and `inverse_transform` methods of `OneHotEncoder` to comply with the `ColumnTransformer` interface.\n    Args:\n        drop: str or list of str, to pass to `OneHotEncoder`'s `drop` parameter.\n    Attributes:\n        missing_value: The value used to fill missing values in the data.\n    After applying the transformer, the following attributes will be populated:\n    Attributes:\n        original_column_name: The name of the original column.\n        new_column_names: The names of the columns generated by the transformer.\n    \"\"\"\ndef __init__(self, drop: Optional[Union[list, str]] = None) -&gt; None:\nsuper().__init__()\nself._drop: Union[list, str] = drop\nself._transformer: OneHotEncoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=self._drop)\nself.missing_value: Any = None\ndef apply(self, data: pd.Series, missing_value: Optional[Any] = None) -&gt; pd.DataFrame:\n\"\"\"\n        Apply the transformer to the data via sklearn's `OneHotEncoder`'s `fit_transform` method. Name the new columns via manipulation of the original column name.\n        If `missing_value` is provided, fill missing values with this value before applying the transformer to ensure a new category is added.\n        Args:\n            data: The column of data to transform.\n            missing_value: The value learned by the `MetaTransformer` to represent missingness, this is only used as part of the `AugmentMissingnessStrategy`.\n        \"\"\"\nself.original_column_name = data.name\nif missing_value:\ndata = data.fillna(missing_value)\nself.missing_value = missing_value\ntransformed_data = pd.DataFrame(\nself._transformer.fit_transform(data.values.reshape(-1, 1)),\ncolumns=self._transformer.get_feature_names_out(input_features=[data.name]),\n)\nself.new_column_names = transformed_data.columns\nreturn transformed_data\ndef revert(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Revert data to pre-transformer state via sklearn's `OneHotEncoder`'s `inverse_transform` method.\n        If `missing_value` is provided, replace instances of this value in the data with `np.nan` to ensure missing values are represented correctly in the case\n        where `missing_value` was 'modelled' and thus generated.\n        Args:\n            data: The full dataset including the column(s) to be reverted to their pre-transformer state.\n        Returns:\n            The dataset with a single categorical column that is analogous to the original column, with the same name, and without the generated one-hot columns.\n        \"\"\"\ndata[self.original_column_name] = pd.Series(\nself._transformer.inverse_transform(data[self.new_column_names].values).flatten(),\nindex=data.index,\nname=self.original_column_name,\n)\nif self.missing_value:\ndata[self.original_column_name] = data[self.original_column_name].replace(self.missing_value, np.nan)\nreturn data.drop(self.new_column_names, axis=1)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/categorical/#nhssynth.modules.dataloader.transformers.categorical.OHECategoricalTransformer.apply","title":"<code>apply(data, missing_value=None)</code>","text":"<p>Apply the transformer to the data via sklearn's <code>OneHotEncoder</code>'s <code>fit_transform</code> method. Name the new columns via manipulation of the original column name. If <code>missing_value</code> is provided, fill missing values with this value before applying the transformer to ensure a new category is added.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The column of data to transform.</p> required <code>missing_value</code> <code>Optional[Any]</code> <p>The value learned by the <code>MetaTransformer</code> to represent missingness, this is only used as part of the <code>AugmentMissingnessStrategy</code>.</p> <code>None</code> Source code in <code>src/nhssynth/modules/dataloader/transformers/categorical.py</code> <pre><code>def apply(self, data: pd.Series, missing_value: Optional[Any] = None) -&gt; pd.DataFrame:\n\"\"\"\n    Apply the transformer to the data via sklearn's `OneHotEncoder`'s `fit_transform` method. Name the new columns via manipulation of the original column name.\n    If `missing_value` is provided, fill missing values with this value before applying the transformer to ensure a new category is added.\n    Args:\n        data: The column of data to transform.\n        missing_value: The value learned by the `MetaTransformer` to represent missingness, this is only used as part of the `AugmentMissingnessStrategy`.\n    \"\"\"\nself.original_column_name = data.name\nif missing_value:\ndata = data.fillna(missing_value)\nself.missing_value = missing_value\ntransformed_data = pd.DataFrame(\nself._transformer.fit_transform(data.values.reshape(-1, 1)),\ncolumns=self._transformer.get_feature_names_out(input_features=[data.name]),\n)\nself.new_column_names = transformed_data.columns\nreturn transformed_data\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/categorical/#nhssynth.modules.dataloader.transformers.categorical.OHECategoricalTransformer.revert","title":"<code>revert(data)</code>","text":"<p>Revert data to pre-transformer state via sklearn's <code>OneHotEncoder</code>'s <code>inverse_transform</code> method. If <code>missing_value</code> is provided, replace instances of this value in the data with <code>np.nan</code> to ensure missing values are represented correctly in the case where <code>missing_value</code> was 'modelled' and thus generated.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The full dataset including the column(s) to be reverted to their pre-transformer state.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset with a single categorical column that is analogous to the original column, with the same name, and without the generated one-hot columns.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/categorical.py</code> <pre><code>def revert(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Revert data to pre-transformer state via sklearn's `OneHotEncoder`'s `inverse_transform` method.\n    If `missing_value` is provided, replace instances of this value in the data with `np.nan` to ensure missing values are represented correctly in the case\n    where `missing_value` was 'modelled' and thus generated.\n    Args:\n        data: The full dataset including the column(s) to be reverted to their pre-transformer state.\n    Returns:\n        The dataset with a single categorical column that is analogous to the original column, with the same name, and without the generated one-hot columns.\n    \"\"\"\ndata[self.original_column_name] = pd.Series(\nself._transformer.inverse_transform(data[self.new_column_names].values).flatten(),\nindex=data.index,\nname=self.original_column_name,\n)\nif self.missing_value:\ndata[self.original_column_name] = data[self.original_column_name].replace(self.missing_value, np.nan)\nreturn data.drop(self.new_column_names, axis=1)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/continuous/","title":"continuous","text":""},{"location":"reference/modules/dataloader/transformers/continuous/#nhssynth.modules.dataloader.transformers.continuous.ClusterContinuousTransformer","title":"<code>ClusterContinuousTransformer</code>","text":"<p>             Bases: <code>ColumnTransformer</code></p> <p>A transformer to cluster continuous features via sklearn's <code>BayesianGaussianMixture</code>. Essentially wraps the process of fitting the BGM model and generating cluster assignments and normalised values for the data to comply with the <code>ColumnTransformer</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>The number of components to use in the BGM model.</p> <code>10</code> <code>n_init</code> <code>int</code> <p>The number of initialisations to use in the BGM model.</p> <code>1</code> <code>init_params</code> <code>str</code> <p>The initialisation method to use in the BGM model.</p> <code>'kmeans'</code> <code>random_state</code> <code>int</code> <p>The random state to use in the BGM model.</p> <code>0</code> <code>max_iter</code> <code>int</code> <p>The maximum number of iterations to use in the BGM model.</p> <code>1000</code> <code>remove_unused_components</code> <code>bool</code> <p>Whether to remove components that have no data assigned EXPERIMENTAL.</p> <code>False</code> <code>clip_output</code> <code>bool</code> <p>Whether to clip the output normalised values to the range [-1, 1].</p> <code>False</code> <p>After applying the transformer, the following attributes will be populated:</p> <p>Attributes:</p> Name Type Description <code>means</code> <p>The means of the components in the BGM model.</p> <code>stds</code> <p>The standard deviations of the components in the BGM model.</p> <code>new_column_names</code> <p>The names of the columns generated by the transformer (one for the normalised values and one for each cluster component).</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/continuous.py</code> <pre><code>class ClusterContinuousTransformer(ColumnTransformer):\n\"\"\"\n    A transformer to cluster continuous features via sklearn's `BayesianGaussianMixture`.\n    Essentially wraps the process of fitting the BGM model and generating cluster assignments and normalised values for the data to comply with the `ColumnTransformer` interface.\n    Args:\n        n_components: The number of components to use in the BGM model.\n        n_init: The number of initialisations to use in the BGM model.\n        init_params: The initialisation method to use in the BGM model.\n        random_state: The random state to use in the BGM model.\n        max_iter: The maximum number of iterations to use in the BGM model.\n        remove_unused_components: Whether to remove components that have no data assigned EXPERIMENTAL.\n        clip_output: Whether to clip the output normalised values to the range [-1, 1].\n    After applying the transformer, the following attributes will be populated:\n    Attributes:\n        means: The means of the components in the BGM model.\n        stds: The standard deviations of the components in the BGM model.\n        new_column_names: The names of the columns generated by the transformer (one for the normalised values and one for each cluster component).\n    \"\"\"\ndef __init__(\nself,\nn_components: int = 10,\nn_init: int = 1,\ninit_params: str = \"kmeans\",\nrandom_state: int = 0,\nmax_iter: int = 1000,\nremove_unused_components: bool = False,\nclip_output: bool = False,\n) -&gt; None:\nsuper().__init__()\nself._transformer = BayesianGaussianMixture(\nn_components=n_components,\nrandom_state=random_state,\nn_init=n_init,\ninit_params=init_params,\nmax_iter=max_iter,\nweight_concentration_prior=1e-3,\n)\nself._n_components = n_components\nself._std_multiplier = 4\nself._missingness_column_name = None\nself._max_iter = max_iter\nself.remove_unused_components = remove_unused_components\nself.clip_output = clip_output\ndef apply(self, data: pd.Series, missingness_column: Optional[pd.Series] = None) -&gt; pd.DataFrame:\n\"\"\"\n        Apply the transformer to the data via sklearn's `BayesianGaussianMixture`'s `fit` and `predict_proba` methods.\n        Name the new columns via the original column name.\n        If `missingness_column` is provided, use this to extract the non-missing data; the missing values are assigned to a new pseudo-cluster with mean 0\n        (i.e. all values in the normalised column are 0.0). We do this by taking the full index before subsetting to non-missing data, then reindexing.\n        Args:\n            data: The column of data to transform.\n            missingness_column: The column of data representing missingness, this is only used as part of the `AugmentMissingnessStrategy`.\n        Returns:\n            The transformed data (will be multiple columns if `n_components` &gt; 1 at initialisation).\n        \"\"\"\nself.original_column_name = data.name\nif missingness_column is not None:\nself._missingness_column_name = missingness_column.name\nfull_index = data.index\ndata = data[missingness_column == 0]\nindex = data.index\ndata = np.array(data.values.reshape(-1, 1), dtype=data.dtype.name.lower())\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nself._transformer.fit(data)\nself.means = self._transformer.means_.reshape(-1)\nself.stds = np.sqrt(self._transformer.covariances_).reshape(-1)\ncomponents = np.argmax(self._transformer.predict_proba(data), axis=1)\nnormalised_values = (data - self.means.reshape(1, -1)) / (self._std_multiplier * self.stds.reshape(1, -1))\nnormalised = normalised_values[np.arange(len(data)), components]\nnormalised = np.clip(normalised, -1.0, 1.0)\ncomponents = np.eye(self._n_components, dtype=int)[components]\ntransformed_data = pd.DataFrame(\nnp.hstack([normalised.reshape(-1, 1), components]),\nindex=index,\ncolumns=[f\"{self.original_column_name}_normalised\"]\n+ [f\"{self.original_column_name}_c{i + 1}\" for i in range(self._n_components)],\n)\n# EXPERIMENTAL feature, removing components from the column matrix that have no data assigned to them\nif self.remove_unused_components:\nnunique = transformed_data.iloc[:, 1:].nunique(dropna=False)\nunused_components = nunique[nunique == 1].index\nunused_component_idx = [transformed_data.columns.get_loc(col_name) - 1 for col_name in unused_components]\nself.means = np.delete(self.means, unused_component_idx)\nself.stds = np.delete(self.stds, unused_component_idx)\ntransformed_data.drop(unused_components, axis=1, inplace=True)\nif missingness_column is not None:\ntransformed_data = pd.concat([transformed_data.reindex(full_index).fillna(0.0), missingness_column], axis=1)\nself.new_column_names = transformed_data.columns\nreturn transformed_data.astype(\n{col_name: int for col_name in transformed_data.columns if re.search(r\"_c\\d+\", col_name)}\n)\ndef revert(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Revert data to pre-transformer state via the means and stds of the BGM. Extract the relevant columns from the data via the `new_column_names` attribute.\n        If `missingness_column` was provided to the `apply` method, drop the missing values from the data before reverting and use the `full_index` to\n        reintroduce missing values when `original_column_name` is constructed.\n        Args:\n            data: The full dataset including the column(s) to be reverted to their pre-transformer state.\n        Returns:\n            The dataset with a single continuous column that is analogous to the original column, with the same name, and without the generated columns from which it is derived.\n        \"\"\"\nworking_data = data[self.new_column_names]\nfull_index = working_data.index\nif self._missingness_column_name is not None:\nworking_data = working_data[working_data[self._missingness_column_name] == 0]\nworking_data = working_data.drop(self._missingness_column_name, axis=1)\nindex = working_data.index\ncomponents = np.argmax(working_data.filter(regex=r\".*_c\\d+\").values, axis=1)\nworking_data = working_data.filter(like=\"_normalised\").values.reshape(-1)\nif self.clip_output:\nworking_data = np.clip(working_data, -1.0, 1.0)\nmean_t = self.means[components]\nstd_t = self.stds[components]\ndata[self.original_column_name] = pd.Series(\nworking_data * self._std_multiplier * std_t + mean_t, index=index, name=self.original_column_name\n).reindex(full_index)\ndata.drop(self.new_column_names, axis=1, inplace=True)\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/continuous/#nhssynth.modules.dataloader.transformers.continuous.ClusterContinuousTransformer.apply","title":"<code>apply(data, missingness_column=None)</code>","text":"<p>Apply the transformer to the data via sklearn's <code>BayesianGaussianMixture</code>'s <code>fit</code> and <code>predict_proba</code> methods. Name the new columns via the original column name.</p> <p>If <code>missingness_column</code> is provided, use this to extract the non-missing data; the missing values are assigned to a new pseudo-cluster with mean 0 (i.e. all values in the normalised column are 0.0). We do this by taking the full index before subsetting to non-missing data, then reindexing.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The column of data to transform.</p> required <code>missingness_column</code> <code>Optional[Series]</code> <p>The column of data representing missingness, this is only used as part of the <code>AugmentMissingnessStrategy</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data (will be multiple columns if <code>n_components</code> &gt; 1 at initialisation).</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/continuous.py</code> <pre><code>def apply(self, data: pd.Series, missingness_column: Optional[pd.Series] = None) -&gt; pd.DataFrame:\n\"\"\"\n    Apply the transformer to the data via sklearn's `BayesianGaussianMixture`'s `fit` and `predict_proba` methods.\n    Name the new columns via the original column name.\n    If `missingness_column` is provided, use this to extract the non-missing data; the missing values are assigned to a new pseudo-cluster with mean 0\n    (i.e. all values in the normalised column are 0.0). We do this by taking the full index before subsetting to non-missing data, then reindexing.\n    Args:\n        data: The column of data to transform.\n        missingness_column: The column of data representing missingness, this is only used as part of the `AugmentMissingnessStrategy`.\n    Returns:\n        The transformed data (will be multiple columns if `n_components` &gt; 1 at initialisation).\n    \"\"\"\nself.original_column_name = data.name\nif missingness_column is not None:\nself._missingness_column_name = missingness_column.name\nfull_index = data.index\ndata = data[missingness_column == 0]\nindex = data.index\ndata = np.array(data.values.reshape(-1, 1), dtype=data.dtype.name.lower())\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nself._transformer.fit(data)\nself.means = self._transformer.means_.reshape(-1)\nself.stds = np.sqrt(self._transformer.covariances_).reshape(-1)\ncomponents = np.argmax(self._transformer.predict_proba(data), axis=1)\nnormalised_values = (data - self.means.reshape(1, -1)) / (self._std_multiplier * self.stds.reshape(1, -1))\nnormalised = normalised_values[np.arange(len(data)), components]\nnormalised = np.clip(normalised, -1.0, 1.0)\ncomponents = np.eye(self._n_components, dtype=int)[components]\ntransformed_data = pd.DataFrame(\nnp.hstack([normalised.reshape(-1, 1), components]),\nindex=index,\ncolumns=[f\"{self.original_column_name}_normalised\"]\n+ [f\"{self.original_column_name}_c{i + 1}\" for i in range(self._n_components)],\n)\n# EXPERIMENTAL feature, removing components from the column matrix that have no data assigned to them\nif self.remove_unused_components:\nnunique = transformed_data.iloc[:, 1:].nunique(dropna=False)\nunused_components = nunique[nunique == 1].index\nunused_component_idx = [transformed_data.columns.get_loc(col_name) - 1 for col_name in unused_components]\nself.means = np.delete(self.means, unused_component_idx)\nself.stds = np.delete(self.stds, unused_component_idx)\ntransformed_data.drop(unused_components, axis=1, inplace=True)\nif missingness_column is not None:\ntransformed_data = pd.concat([transformed_data.reindex(full_index).fillna(0.0), missingness_column], axis=1)\nself.new_column_names = transformed_data.columns\nreturn transformed_data.astype(\n{col_name: int for col_name in transformed_data.columns if re.search(r\"_c\\d+\", col_name)}\n)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/continuous/#nhssynth.modules.dataloader.transformers.continuous.ClusterContinuousTransformer.revert","title":"<code>revert(data)</code>","text":"<p>Revert data to pre-transformer state via the means and stds of the BGM. Extract the relevant columns from the data via the <code>new_column_names</code> attribute. If <code>missingness_column</code> was provided to the <code>apply</code> method, drop the missing values from the data before reverting and use the <code>full_index</code> to reintroduce missing values when <code>original_column_name</code> is constructed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The full dataset including the column(s) to be reverted to their pre-transformer state.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataset with a single continuous column that is analogous to the original column, with the same name, and without the generated columns from which it is derived.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/continuous.py</code> <pre><code>def revert(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Revert data to pre-transformer state via the means and stds of the BGM. Extract the relevant columns from the data via the `new_column_names` attribute.\n    If `missingness_column` was provided to the `apply` method, drop the missing values from the data before reverting and use the `full_index` to\n    reintroduce missing values when `original_column_name` is constructed.\n    Args:\n        data: The full dataset including the column(s) to be reverted to their pre-transformer state.\n    Returns:\n        The dataset with a single continuous column that is analogous to the original column, with the same name, and without the generated columns from which it is derived.\n    \"\"\"\nworking_data = data[self.new_column_names]\nfull_index = working_data.index\nif self._missingness_column_name is not None:\nworking_data = working_data[working_data[self._missingness_column_name] == 0]\nworking_data = working_data.drop(self._missingness_column_name, axis=1)\nindex = working_data.index\ncomponents = np.argmax(working_data.filter(regex=r\".*_c\\d+\").values, axis=1)\nworking_data = working_data.filter(like=\"_normalised\").values.reshape(-1)\nif self.clip_output:\nworking_data = np.clip(working_data, -1.0, 1.0)\nmean_t = self.means[components]\nstd_t = self.stds[components]\ndata[self.original_column_name] = pd.Series(\nworking_data * self._std_multiplier * std_t + mean_t, index=index, name=self.original_column_name\n).reindex(full_index)\ndata.drop(self.new_column_names, axis=1, inplace=True)\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/datetime/","title":"datetime","text":""},{"location":"reference/modules/dataloader/transformers/datetime/#nhssynth.modules.dataloader.transformers.datetime.DatetimeTransformer","title":"<code>DatetimeTransformer</code>","text":"<p>             Bases: <code>TransformerWrapper</code></p> <p>A transformer to convert datetime features to numeric features. Before applying an underlying (wrapped) transformer. The datetime features are converted to nanoseconds since the epoch, and missing values are assigned to 0.0 under the <code>AugmentMissingnessStrategy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>ColumnTransformer</code> <p>The <code>ColumnTransformer</code> to wrap.</p> required <p>After applying the transformer, the following attributes will be populated:</p> <p>Attributes:</p> Name Type Description <code>original_column_name</code> <p>The name of the original column.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/datetime.py</code> <pre><code>class DatetimeTransformer(TransformerWrapper):\n\"\"\"\n    A transformer to convert datetime features to numeric features. Before applying an underlying (wrapped) transformer.\n    The datetime features are converted to nanoseconds since the epoch, and missing values are assigned to 0.0 under the `AugmentMissingnessStrategy`.\n    Args:\n        transformer: The [`ColumnTransformer`][nhssynth.modules.dataloader.transformers.base.ColumnTransformer] to wrap.\n    After applying the transformer, the following attributes will be populated:\n    Attributes:\n        original_column_name: The name of the original column.\n    \"\"\"\ndef __init__(self, transformer: ColumnTransformer) -&gt; None:\nsuper().__init__(transformer)\ndef apply(self, data: pd.Series, missingness_column: Optional[pd.Series] = None, **kwargs) -&gt; pd.DataFrame:\n\"\"\"\n        Firstly, the datetime data is floored to the nano-second level. Next, the floored data is converted to float nanoseconds since the epoch.\n        The float value of `pd.NaT` under the operation above is then replaced with `np.nan` to ensure missing values are represented correctly.\n        Finally, the wrapped transformer is applied to the data.\n        Args:\n            data: The column of data to transform.\n            missingness_column: The column of missingness indicators to augment the data with.\n        Returns:\n            The transformed data.\n        \"\"\"\nself.original_column_name = data.name\nfloored_data = pd.Series(data.dt.floor(\"ns\").to_numpy().astype(float), name=data.name)\nnan_corrected_data = floored_data.replace(pd.to_datetime(pd.NaT).to_numpy().astype(float), np.nan)\nreturn super().apply(nan_corrected_data, missingness_column, **kwargs)\ndef revert(self, data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:\n\"\"\"\n        The wrapped transformer's `revert` method is applied to the data. The data is then converted back to datetime format.\n        Args:\n            data: The full dataset including the column(s) to be reverted to their pre-transformer state.\n        Returns:\n            The reverted data.\n        \"\"\"\nreverted_data = super().revert(data, **kwargs)\ndata[self.original_column_name] = pd.to_datetime(\nreverted_data[self.original_column_name].astype(\"Int64\"), unit=\"ns\"\n)\nreturn data\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/datetime/#nhssynth.modules.dataloader.transformers.datetime.DatetimeTransformer.apply","title":"<code>apply(data, missingness_column=None, **kwargs)</code>","text":"<p>Firstly, the datetime data is floored to the nano-second level. Next, the floored data is converted to float nanoseconds since the epoch. The float value of <code>pd.NaT</code> under the operation above is then replaced with <code>np.nan</code> to ensure missing values are represented correctly. Finally, the wrapped transformer is applied to the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The column of data to transform.</p> required <code>missingness_column</code> <code>Optional[Series]</code> <p>The column of missingness indicators to augment the data with.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/datetime.py</code> <pre><code>def apply(self, data: pd.Series, missingness_column: Optional[pd.Series] = None, **kwargs) -&gt; pd.DataFrame:\n\"\"\"\n    Firstly, the datetime data is floored to the nano-second level. Next, the floored data is converted to float nanoseconds since the epoch.\n    The float value of `pd.NaT` under the operation above is then replaced with `np.nan` to ensure missing values are represented correctly.\n    Finally, the wrapped transformer is applied to the data.\n    Args:\n        data: The column of data to transform.\n        missingness_column: The column of missingness indicators to augment the data with.\n    Returns:\n        The transformed data.\n    \"\"\"\nself.original_column_name = data.name\nfloored_data = pd.Series(data.dt.floor(\"ns\").to_numpy().astype(float), name=data.name)\nnan_corrected_data = floored_data.replace(pd.to_datetime(pd.NaT).to_numpy().astype(float), np.nan)\nreturn super().apply(nan_corrected_data, missingness_column, **kwargs)\n</code></pre>"},{"location":"reference/modules/dataloader/transformers/datetime/#nhssynth.modules.dataloader.transformers.datetime.DatetimeTransformer.revert","title":"<code>revert(data, **kwargs)</code>","text":"<p>The wrapped transformer's <code>revert</code> method is applied to the data. The data is then converted back to datetime format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The full dataset including the column(s) to be reverted to their pre-transformer state.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The reverted data.</p> Source code in <code>src/nhssynth/modules/dataloader/transformers/datetime.py</code> <pre><code>def revert(self, data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:\n\"\"\"\n    The wrapped transformer's `revert` method is applied to the data. The data is then converted back to datetime format.\n    Args:\n        data: The full dataset including the column(s) to be reverted to their pre-transformer state.\n    Returns:\n        The reverted data.\n    \"\"\"\nreverted_data = super().revert(data, **kwargs)\ndata[self.original_column_name] = pd.to_datetime(\nreverted_data[self.original_column_name].astype(\"Int64\"), unit=\"ns\"\n)\nreturn data\n</code></pre>"},{"location":"reference/modules/evaluation/","title":"evaluation","text":""},{"location":"reference/modules/evaluation/aequitas/","title":"aequitas","text":""},{"location":"reference/modules/evaluation/io/","title":"io","text":""},{"location":"reference/modules/evaluation/io/#nhssynth.modules.evaluation.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_synthetic_datasets, fn_sdv_metadata, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed real dataset file.</p> required <code>fn_synthetic_datasets</code> <code>str</code> <p>The filename of the collection of synethtic datasets.</p> required <code>fn_sdv_metadata</code> <code>str</code> <p>The name of the SDV metadata file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>src/nhssynth/modules/evaluation/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_typed: str, fn_synthetic_datasets: str, fn_sdv_metadata: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed real dataset file.\n        fn_synthetic_datasets: The filename of the collection of synethtic datasets.\n        fn_sdv_metadata: The name of the SDV metadata file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset = Path(fn_dataset).stem\nfn_typed, fn_synthetic_datasets, fn_sdv_metadata = consistent_endings(\n[fn_typed, fn_synthetic_datasets, fn_sdv_metadata]\n)\nfn_typed, fn_synthetic_datasets, fn_sdv_metadata = potential_suffixes(\n[fn_typed, fn_synthetic_datasets, fn_sdv_metadata], fn_dataset\n)\nwarn_if_path_supplied([fn_typed, fn_synthetic_datasets, fn_sdv_metadata], dir_experiment)\ncheck_exists([fn_typed, fn_synthetic_datasets, fn_sdv_metadata], dir_experiment)\nreturn fn_dataset, fn_typed, fn_synthetic_datasets, fn_sdv_metadata\n</code></pre>"},{"location":"reference/modules/evaluation/io/#nhssynth.modules.evaluation.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, DataFrame, DataFrame, dict[str, dict[str, Any]]]</code> <p>The dataset name, the real data, the bundle of synthetic data from the modelling stage, and the SDV metadata.</p> Source code in <code>src/nhssynth/modules/evaluation/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The dataset name, the real data, the bundle of synthetic data from the modelling stage, and the SDV metadata.\n    \"\"\"\nif all(x in args.module_handover for x in [\"dataset\", \"typed\", \"synthetic_datasets\", \"sdv_metadata\"]):\nreturn (\nargs.module_handover[\"dataset\"],\nargs.module_handover[\"typed\"],\nargs.module_handover[\"synthetic_datasets\"],\nargs.module_handover[\"sdv_metadata\"],\n)\nelse:\nfn_dataset, fn_typed, fn_synthetic_datasets, fn_sdv_metadata = check_input_paths(\nargs.dataset, args.typed, args.synthetic_datasets, args.sdv_metadata, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f).contents\nwith open(dir_experiment / fn_sdv_metadata, \"rb\") as f:\nsdv_metadata = pickle.load(f)\nwith open(dir_experiment / fn_synthetic_datasets, \"rb\") as f:\nsynthetic_datasets = pickle.load(f).contents\nreturn fn_dataset, real_data, synthetic_datasets, sdv_metadata\n</code></pre>"},{"location":"reference/modules/evaluation/io/#nhssynth.modules.evaluation.io.output_eval","title":"<code>output_eval(evaluations, fn_dataset, fn_evaluations, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>DataFrame</code> <p>The evaluations to output.</p> required <code>fn_dataset</code> <code>Path</code> <p>The base name of the dataset.</p> required <code>fn_evaluations</code> <code>str</code> <p>The filename of the collection of evaluations.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The path to output the model.</p> Source code in <code>src/nhssynth/modules/evaluation/io.py</code> <pre><code>def output_eval(\nevaluations: pd.DataFrame,\nfn_dataset: Path,\nfn_evaluations: str,\ndir_experiment: Path,\n) -&gt; None:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        evaluations: The evaluations to output.\n        fn_dataset: The base name of the dataset.\n        fn_evaluations: The filename of the collection of evaluations.\n        dir_experiment: The path to the experiment output directory.\n    Returns:\n        The path to output the model.\n    \"\"\"\nfn_evaluations = consistent_ending(fn_evaluations)\nfn_evaluations = potential_suffix(fn_evaluations, fn_dataset)\nwarn_if_path_supplied([fn_evaluations], dir_experiment)\nwith open(dir_experiment / fn_evaluations, \"wb\") as f:\npickle.dump(Evaluations(evaluations), f)\n</code></pre>"},{"location":"reference/modules/evaluation/metrics/","title":"metrics","text":""},{"location":"reference/modules/evaluation/run/","title":"run","text":""},{"location":"reference/modules/evaluation/tasks/","title":"tasks","text":""},{"location":"reference/modules/evaluation/tasks/#nhssynth.modules.evaluation.tasks.Task","title":"<code>Task</code>","text":"<p>A task offers a light-touch way for users to specify any arbitrary downstream task that they want to run on a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the task.</p> required <code>run</code> <code>Callable</code> <p>The function to run.</p> required <code>supports_aequitas</code> <p>Whether the task supports Aequitas evaluation.</p> <code>False</code> <code>description</code> <code>str</code> <p>The description of the task.</p> <code>''</code> Source code in <code>src/nhssynth/modules/evaluation/tasks.py</code> <pre><code>class Task:\n\"\"\"\n    A task offers a light-touch way for users to specify any arbitrary downstream task that they want to run on a dataset.\n    Args:\n        name: The name of the task.\n        run: The function to run.\n        supports_aequitas: Whether the task supports Aequitas evaluation.\n        description: The description of the task.\n    \"\"\"\ndef __init__(self, name: str, run: Callable, supports_aequitas=False, description: str = \"\"):\nself._name: str = name\nself._run: Callable = run\nself._supports_aequitas: bool = supports_aequitas\nself._description: str = description\ndef __str__(self) -&gt; str:\nreturn f\"{self.name}: {self.description}\" if self.description else self.name\ndef __repr__(self) -&gt; str:\nreturn str([self.name, self.run, self.supports_aequitas, self.description])\ndef run(self, *args, **kwargs):\nreturn self._run(*args, **kwargs)\n</code></pre>"},{"location":"reference/modules/evaluation/tasks/#nhssynth.modules.evaluation.tasks.get_tasks","title":"<code>get_tasks(fn_dataset, tasks_root)</code>","text":"<p>Searches for and imports all tasks in the tasks directory for a given dataset. Uses <code>importlib</code> to extract the task from the file.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The name of the dataset.</p> required <code>tasks_root</code> <code>str</code> <p>The root directory for downstream tasks.</p> required <p>Returns:</p> Type Description <code>list[Task]</code> <p>A list of tasks.</p> Source code in <code>src/nhssynth/modules/evaluation/tasks.py</code> <pre><code>def get_tasks(\nfn_dataset: str,\ntasks_root: str,\n) -&gt; list[Task]:\n\"\"\"\n    Searches for and imports all tasks in the tasks directory for a given dataset.\n    Uses `importlib` to extract the task from the file.\n    Args:\n        fn_dataset: The name of the dataset.\n        tasks_root: The root directory for downstream tasks.\n    Returns:\n        A list of tasks.\n    \"\"\"\ntasks_dir = Path(tasks_root) / fn_dataset\nassert (\ntasks_dir.exists()\n), f\"Downstream tasks directory does not exist ({tasks_dir}), NB there should be a directory in TASKS_DIR with the same name as the dataset.\"\ntasks = []\nfor task_path in tasks_dir.iterdir():\nif task_path.name.startswith((\".\", \"__\")):\ncontinue\nassert task_path.suffix == \".py\", f\"Downstream task file must be a python file ({task_path.name})\"\nspec = importlib.util.spec_from_file_location(\n\"nhssynth_task_\" + task_path.name, os.getcwd() + \"/\" + str(task_path)\n)\ntask_module = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(task_module)\ntasks.append(task_module.task)\nreturn tasks\n</code></pre>"},{"location":"reference/modules/evaluation/utils/","title":"utils","text":""},{"location":"reference/modules/evaluation/utils/#nhssynth.modules.evaluation.utils.EvalFrame","title":"<code>EvalFrame</code>","text":"<p>Data structure for specifying and recording the evaluations of a set of synthetic datasets against a real dataset. All of the choices made by the user in the evaluation module are consolidated into this class.</p> <p>After running <code>evaluate</code> on a set of synthetic datasets, the evaluations can be retrieved using <code>get_evaluations</code>. They are stored in a dict of dataframes with indices matching that of the supplied dataframe of synthetic datasets.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>list[Task]</code> <p>A list of downstream tasks to run on the experiments.</p> required <code>metrics</code> <code>list[str]</code> <p>A list of metrics to calculate on the experiments.</p> required <code>sdv_metadata</code> <code>dict[str, dict[str, str]]</code> <p>The SDV metadata for the dataset.</p> required <code>aequitas</code> <code>bool</code> <p>Whether to run Aequitas on the results of supported downstream tasks.</p> <code>False</code> <code>aequitas_attributes</code> <code>list[str]</code> <p>The fairness-related attributes to use for Aequitas analysis.</p> <code>[]</code> <code>key_numerical_fields</code> <code>list[str]</code> <p>The numerical fields to use for SDV privacy metrics.</p> <code>[]</code> <code>sensitive_numerical_fields</code> <code>list[str]</code> <p>The numerical fields to use for SDV privacy metrics.</p> <code>[]</code> <code>key_categorical_fields</code> <code>list[str]</code> <p>The categorical fields to use for SDV privacy metrics.</p> <code>[]</code> <code>sensitive_categorical_fields</code> <code>list[str]</code> <p>The categorical fields to use for SDV privacy metrics.</p> <code>[]</code> Source code in <code>src/nhssynth/modules/evaluation/utils.py</code> <pre><code>class EvalFrame:\n\"\"\"\n    Data structure for specifying and recording the evaluations of a set of synthetic datasets against a real dataset.\n    All of the choices made by the user in the evaluation module are consolidated into this class.\n    After running `evaluate` on a set of synthetic datasets, the evaluations can be retrieved using `get_evaluations`.\n    They are stored in a dict of dataframes with indices matching that of the supplied dataframe of synthetic datasets.\n    Args:\n        tasks: A list of downstream tasks to run on the experiments.\n        metrics: A list of metrics to calculate on the experiments.\n        sdv_metadata: The SDV metadata for the dataset.\n        aequitas: Whether to run Aequitas on the results of supported downstream tasks.\n        aequitas_attributes: The fairness-related attributes to use for Aequitas analysis.\n        key_numerical_fields: The numerical fields to use for SDV privacy metrics.\n        sensitive_numerical_fields: The numerical fields to use for SDV privacy metrics.\n        key_categorical_fields: The categorical fields to use for SDV privacy metrics.\n        sensitive_categorical_fields: The categorical fields to use for SDV privacy metrics.\n    \"\"\"\ndef __init__(\nself,\ntasks: list[Task],\nmetrics: list[str],\nsdv_metadata: dict[str, dict[str, str]],\naequitas: bool = False,\naequitas_attributes: list[str] = [],\nkey_numerical_fields: list[str] = [],\nsensitive_numerical_fields: list[str] = [],\nkey_categorical_fields: list[str] = [],\nsensitive_categorical_fields: list[str] = [],\n):\nself._tasks = tasks\nself._aequitas = aequitas\nself._aequitas_attributes = aequitas_attributes\nself._metrics = metrics\nself._sdv_metadata = sdv_metadata\nself._key_numerical_fields = key_numerical_fields\nself._sensitive_numerical_fields = sensitive_numerical_fields\nself._key_categorical_fields = key_categorical_fields\nself._sensitive_categorical_fields = sensitive_categorical_fields\nassert all([metric not in NUMERICAL_PRIVACY_METRICS for metric in self._metrics]) or (\nself._key_numerical_fields and self._sensitive_numerical_fields\n), \"Numerical key and sensitive fields must be provided when an SDV privacy metric is used.\"\nassert all([metric not in CATEGORICAL_PRIVACY_METRICS for metric in self._metrics]) or (\nself._key_categorical_fields and self._sensitive_categorical_fields\n), \"Categorical key and sensitive fields must be provided when an SDV privacy metric is used.\"\nself._metric_groups = self._build_metric_groups()\ndef _build_metric_groups(self) -&gt; list[str]:\n\"\"\"\n        Iterate through the concatenated list of metrics provided by the user and refer to the\n        [defined metric groups][nhssynth.common.constants] to identify which to evaluate.\n        Returns:\n            A list of metric groups to evaluate.\n        \"\"\"\nmetric_groups = set()\nif self._tasks:\nmetric_groups.add(\"task\")\nif self._aequitas:\nmetric_groups.add(\"aequitas\")\nfor metric in self._metrics:\nif metric in TABLE_METRICS:\nmetric_groups.add(\"table\")\nif metric in NUMERICAL_PRIVACY_METRICS or metric in CATEGORICAL_PRIVACY_METRICS:\nmetric_groups.add(\"privacy\")\nif metric in TABLE_METRICS and issubclass(TABLE_METRICS[metric], MultiSingleColumnMetric):\nmetric_groups.add(\"columnwise\")\nif metric in TABLE_METRICS and issubclass(TABLE_METRICS[metric], MultiColumnPairsMetric):\nmetric_groups.add(\"pairwise\")\nreturn list(metric_groups)\ndef evaluate(self, real_dataset: pd.DataFrame, synthetic_datasets: list[dict[str, Any]]) -&gt; None:\n\"\"\"\n        Evaluate a set of synthetic datasets against a real dataset.\n        Args:\n            real_dataset: The real dataset to evaluate against.\n            synthetic_datasets: The synthetic datasets to evaluate.\n        \"\"\"\nassert not any(\"Real\" in i for i in synthetic_datasets.index), \"Real is a reserved dataset ID.\"\nassert synthetic_datasets.index.is_unique, \"Dataset IDs must be unique.\"\nself._evaluations = pd.DataFrame(index=synthetic_datasets.index, columns=self._metric_groups)\nself._evaluations.loc[(\"Real\", None, None)] = self._step(real_dataset)\npbar = tqdm(synthetic_datasets.iterrows(), desc=\"Evaluating\", total=len(synthetic_datasets))\nfor i, dataset in pbar:\npbar.set_description(f\"Evaluating {i[0]}, repeat {i[1]}, config {i[2]}\")\nself._evaluations.loc[i] = self._step(real_dataset, dataset.values[0])\ndef get_evaluations(self) -&gt; dict[str, pd.DataFrame]:\n\"\"\"\n        Unpack the `self._evaluations` dataframe, where each metric group is a column, into a dict of dataframes.\n        Returns:\n            A dict of dataframes, one for each metric group, containing the evaluations.\n        \"\"\"\nassert hasattr(\nself, \"_evaluations\"\n), \"You must first run `evaluate` on a `real_dataset` and set of `synthetic_datasets`.\"\nreturn {\nmetric_group: pd.DataFrame(\nself._evaluations[metric_group].values.tolist(), index=self._evaluations.index\n).dropna(how=\"all\")\nfor metric_group in self._metric_groups\n}\ndef _task_step(self, data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n        Run the downstream tasks on the dataset. Optionally run Aequitas on the results of the tasks.\n        Args:\n            data: The dataset to run the tasks on.\n        Returns:\n            A dict of dicts, one for each metric group, to be populated with each groups metric values.\n        \"\"\"\nmetric_dict = {metric_group: {} for metric_group in self._metric_groups}\nfor task in tqdm(self._tasks, desc=\"Running downstream tasks\", leave=False):\ntask_pred_column, task_metric_values = task.run(data)\nmetric_dict[\"task\"].update(task_metric_values)\nif self._aequitas and task.supports_aequitas:\nmetric_dict[\"aequitas\"].update(run_aequitas(data[self._aequitas_attributes].join(task_pred_column)))\nreturn metric_dict\ndef _compute_metric(\nself, metric_dict: dict, metric: str, real_data: pd.DataFrame, synthetic_data: pd.DataFrame\n) -&gt; dict[str, dict]:\n\"\"\"\n        Given a metric, determine the correct way to evaluate it via the lists defined in `nhssynth.common.constants`.\n        Args:\n            metric_dict: The dict of dicts to populate with metric values.\n            metric: The metric to evaluate.\n            real_data: The real dataset to evaluate against.\n            synthetic_data: The synthetic dataset to evaluate.\n        Returns:\n            The metric_dict updated with the value of the metric.\n        \"\"\"\nwith pd.option_context(\"mode.chained_assignment\", None), warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", message=\"ConvergenceWarning\")\nif metric in TABLE_METRICS:\nmetric_dict[\"table\"][metric] = TABLE_METRICS[metric].compute(\nreal_data, synthetic_data, self._sdv_metadata\n)\nif issubclass(TABLE_METRICS[metric], MultiSingleColumnMetric):\nmetric_dict[\"columnwise\"][metric] = TABLE_METRICS[metric].compute_breakdown(\nreal_data, synthetic_data, self._sdv_metadata\n)\nelif issubclass(TABLE_METRICS[metric], MultiColumnPairsMetric):\nmetric_dict[\"pairwise\"][metric] = TABLE_METRICS[metric].compute_breakdown(\nreal_data, synthetic_data, self._sdv_metadata\n)\nelif metric in NUMERICAL_PRIVACY_METRICS:\nmetric_dict[\"privacy\"][metric] = NUMERICAL_PRIVACY_METRICS[metric].compute(\nreal_data.dropna(),\nsynthetic_data.dropna(),\nself._sdv_metadata,\nself._key_numerical_fields,\nself._sensitive_numerical_fields,\n)\nelif metric in CATEGORICAL_PRIVACY_METRICS:\nmetric_dict[\"privacy\"][metric] = CATEGORICAL_PRIVACY_METRICS[metric].compute(\nreal_data.dropna(),\nsynthetic_data.dropna(),\nself._sdv_metadata,\nself._key_categorical_fields,\nself._sensitive_categorical_fields,\n)\nreturn metric_dict\ndef _step(self, real_data: pd.DataFrame, synthetic_data: pd.DataFrame = None) -&gt; dict[str, dict]:\n\"\"\"\n        Run the two functions above (or only the tasks when no synthetic data is provided).\n        Args:\n            real_data: The real dataset to evaluate against.\n            synthetic_data: The synthetic dataset to evaluate.\n        Returns:\n            A dict of dicts, one for each metric grou, to populate a row of `self._evaluations` corresponding to the `synthetic_data`.\n        \"\"\"\nif synthetic_data is None:\nmetric_dict = self._task_step(real_data)\nelse:\nmetric_dict = self._task_step(synthetic_data)\nfor metric in tqdm(self._metrics, desc=\"Running metrics\", leave=False):\nmetric_dict = self._compute_metric(metric_dict, metric, real_data, synthetic_data)\nreturn metric_dict\n</code></pre>"},{"location":"reference/modules/evaluation/utils/#nhssynth.modules.evaluation.utils.EvalFrame.evaluate","title":"<code>evaluate(real_dataset, synthetic_datasets)</code>","text":"<p>Evaluate a set of synthetic datasets against a real dataset.</p> <p>Parameters:</p> Name Type Description Default <code>real_dataset</code> <code>DataFrame</code> <p>The real dataset to evaluate against.</p> required <code>synthetic_datasets</code> <code>list[dict[str, Any]]</code> <p>The synthetic datasets to evaluate.</p> required Source code in <code>src/nhssynth/modules/evaluation/utils.py</code> <pre><code>def evaluate(self, real_dataset: pd.DataFrame, synthetic_datasets: list[dict[str, Any]]) -&gt; None:\n\"\"\"\n    Evaluate a set of synthetic datasets against a real dataset.\n    Args:\n        real_dataset: The real dataset to evaluate against.\n        synthetic_datasets: The synthetic datasets to evaluate.\n    \"\"\"\nassert not any(\"Real\" in i for i in synthetic_datasets.index), \"Real is a reserved dataset ID.\"\nassert synthetic_datasets.index.is_unique, \"Dataset IDs must be unique.\"\nself._evaluations = pd.DataFrame(index=synthetic_datasets.index, columns=self._metric_groups)\nself._evaluations.loc[(\"Real\", None, None)] = self._step(real_dataset)\npbar = tqdm(synthetic_datasets.iterrows(), desc=\"Evaluating\", total=len(synthetic_datasets))\nfor i, dataset in pbar:\npbar.set_description(f\"Evaluating {i[0]}, repeat {i[1]}, config {i[2]}\")\nself._evaluations.loc[i] = self._step(real_dataset, dataset.values[0])\n</code></pre>"},{"location":"reference/modules/evaluation/utils/#nhssynth.modules.evaluation.utils.EvalFrame.get_evaluations","title":"<code>get_evaluations()</code>","text":"<p>Unpack the <code>self._evaluations</code> dataframe, where each metric group is a column, into a dict of dataframes.</p> <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>A dict of dataframes, one for each metric group, containing the evaluations.</p> Source code in <code>src/nhssynth/modules/evaluation/utils.py</code> <pre><code>def get_evaluations(self) -&gt; dict[str, pd.DataFrame]:\n\"\"\"\n    Unpack the `self._evaluations` dataframe, where each metric group is a column, into a dict of dataframes.\n    Returns:\n        A dict of dataframes, one for each metric group, containing the evaluations.\n    \"\"\"\nassert hasattr(\nself, \"_evaluations\"\n), \"You must first run `evaluate` on a `real_dataset` and set of `synthetic_datasets`.\"\nreturn {\nmetric_group: pd.DataFrame(\nself._evaluations[metric_group].values.tolist(), index=self._evaluations.index\n).dropna(how=\"all\")\nfor metric_group in self._metric_groups\n}\n</code></pre>"},{"location":"reference/modules/evaluation/utils/#nhssynth.modules.evaluation.utils.validate_metric_args","title":"<code>validate_metric_args(args, fn_dataset, columns)</code>","text":"<p>Validate the arguments for downstream tasks and Aequitas.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The argument namespace to validate.</p> required <code>fn_dataset</code> <code>str</code> <p>The name of the dataset.</p> required <code>columns</code> <code>Index</code> <p>The columns in the dataset.</p> required <p>Returns:</p> Type Description <code>tuple[list[Task], Namespace]</code> <p>The validated arguments, the list of tasks and the list of metrics.</p> Source code in <code>src/nhssynth/modules/evaluation/utils.py</code> <pre><code>def validate_metric_args(\nargs: argparse.Namespace, fn_dataset: str, columns: pd.Index\n) -&gt; tuple[list[Task], argparse.Namespace]:\n\"\"\"\n    Validate the arguments for downstream tasks and Aequitas.\n    Args:\n        args: The argument namespace to validate.\n        fn_dataset: The name of the dataset.\n        columns: The columns in the dataset.\n    Returns:\n        The validated arguments, the list of tasks and the list of metrics.\n    \"\"\"\nif args.downstream_tasks:\ntasks = get_tasks(fn_dataset, args.tasks_dir)\nif not tasks:\nwarnings.warn(\"No valid downstream tasks found.\")\nelse:\ntasks = []\nif args.aequitas:\nif not args.downstream_tasks or not any([task.supports_aequitas for task in tasks]):\nwarnings.warn(\n\"Aequitas can only work in context of downstream tasks involving binary classification problems.\"\n)\nif not args.aequitas_attributes:\nwarnings.warn(\"No attributes specified for Aequitas analysis, defaulting to all columns in the dataset.\")\nargs.aequitas_attributes = columns.tolist()\nassert all(\n[attr in columns for attr in args.aequitas_attributes]\n), \"Invalid attribute(s) specified for Aequitas analysis.\"\nmetrics = {}\nfor metric_group in METRIC_CHOICES:\nselected_metrics = getattr(args, \"_\".join(metric_group.split()).lower() + \"_metrics\") or []\nmetrics.update({metric_name: METRIC_CHOICES[metric_group][metric_name] for metric_name in selected_metrics})\nreturn args, tasks, metrics\n</code></pre>"},{"location":"reference/modules/model/","title":"model","text":""},{"location":"reference/modules/model/io/","title":"io","text":""},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_transformed, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_transformed</code> <code>str</code> <p>The name of the transformed data file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>src/nhssynth/modules/model/io.py</code> <pre><code>def check_input_paths(\nfn_dataset: str, fn_transformed: str, fn_metatransformer: str, dir_experiment: Path\n) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_transformed: The name of the transformed data file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset = Path(fn_dataset).stem\nfn_transformed, fn_metatransformer = consistent_endings([fn_transformed, fn_metatransformer])\nfn_transformed, fn_metatransformer = potential_suffixes([fn_transformed, fn_metatransformer], fn_dataset)\nwarn_if_path_supplied([fn_transformed, fn_metatransformer], dir_experiment)\ncheck_exists([fn_transformed, fn_metatransformer], dir_experiment)\nreturn fn_dataset, fn_transformed, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, DataFrame, dict[str, int], MetaTransformer]</code> <p>The data, metadata and metatransformer.</p> Source code in <code>src/nhssynth/modules/model/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"dataset\", \"transformed\", \"metatransformer\"]):\nreturn (\nargs.module_handover[\"dataset\"],\nargs.module_handover[\"transformed\"],\nargs.module_handover[\"metatransformer\"],\n)\nelse:\nfn_dataset, fn_transformed, fn_metatransformer = check_input_paths(\nargs.dataset, args.transformed, args.metatransformer, dir_experiment\n)\nwith open(dir_experiment / fn_transformed, \"rb\") as f:\ndata = pickle.load(f)\nwith open(dir_experiment / fn_metatransformer, \"rb\") as f:\nmt = pickle.load(f)\nreturn fn_dataset, data, mt\n</code></pre>"},{"location":"reference/modules/model/run/","title":"run","text":""},{"location":"reference/modules/model/utils/","title":"utils","text":""},{"location":"reference/modules/model/utils/#nhssynth.modules.model.utils.configs_from_arg_combinations","title":"<code>configs_from_arg_combinations(args, arg_list)</code>","text":"<p>Generates a list of configurations from a list of arguments. Each configuration is one of a cartesian product of the arguments provided and identified in <code>arg_list</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments.</p> required <code>arg_list</code> <code>list[str]</code> <p>The list of arguments to generate configurations from.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of configurations.</p> Source code in <code>src/nhssynth/modules/model/utils.py</code> <pre><code>def configs_from_arg_combinations(args: argparse.Namespace, arg_list: list[str]) -&gt; list[dict[str, Any]]:\n\"\"\"\n    Generates a list of configurations from a list of arguments. Each configuration is one of a cartesian product of\n    the arguments provided and identified in `arg_list`.\n    Args:\n        args: The arguments.\n        arg_list: The list of arguments to generate configurations from.\n    Returns:\n        A list of configurations.\n    \"\"\"\nwrapped_args = {arg: wrap_arg(getattr(args, arg)) for arg in arg_list}\ncombinations = list(itertools.product(*wrapped_args.values()))\nreturn [{k: v for k, v in zip(wrapped_args.keys(), values) if v is not None} for values in combinations]\n</code></pre>"},{"location":"reference/modules/model/utils/#nhssynth.modules.model.utils.get_experiments","title":"<code>get_experiments(args)</code>","text":"<p>Generates a dataframe of experiments from the arguments provided.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe of experiments indexed by architecture, repeat and config ID.</p> Source code in <code>src/nhssynth/modules/model/utils.py</code> <pre><code>def get_experiments(args: argparse.Namespace) -&gt; pd.DataFrame:\n\"\"\"\n    Generates a dataframe of experiments from the arguments provided.\n    Args:\n        args: The arguments.\n    Returns:\n        A dataframe of experiments indexed by architecture, repeat and config ID.\n    \"\"\"\nexperiments = pd.DataFrame(\ncolumns=[\"architecture\", \"repeat\", \"config\", \"model_config\", \"seed\", \"train_config\", \"num_configs\"]\n)\ntrain_configs = configs_from_arg_combinations(args, [\"num_epochs\", \"patience\"])\nfor arch_name, repeat in itertools.product(*[wrap_arg(args.architecture), list(range(args.repeats))]):\narch = MODELS[arch_name]\nmodel_configs = configs_from_arg_combinations(args, arch.get_args() + [\"batch_size\", \"use_gpu\"])\nfor i, (train_config, model_config) in enumerate(itertools.product(train_configs, model_configs)):\nexperiments.loc[len(experiments.index)] = {\n\"architecture\": arch_name,\n\"repeat\": repeat + 1,\n\"config\": i + 1,\n\"model_config\": model_config,\n\"num_configs\": len(model_configs) * len(train_configs),\n\"seed\": args.seed + repeat if args.seed else None,\n\"train_config\": train_config,\n}\nreturn experiments.set_index([\"architecture\", \"repeat\", \"config\"], drop=True)\n</code></pre>"},{"location":"reference/modules/model/utils/#nhssynth.modules.model.utils.wrap_arg","title":"<code>wrap_arg(arg)</code>","text":"<p>Wraps a single argument in a list if it is not already a list or tuple.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>Any</code> <p>The argument to wrap.</p> required <p>Returns:</p> Type Description <code>Union[list, tuple]</code> <p>The wrapped argument.</p> Source code in <code>src/nhssynth/modules/model/utils.py</code> <pre><code>def wrap_arg(arg: Any) -&gt; Union[list, tuple]:\n\"\"\"\n    Wraps a single argument in a list if it is not already a list or tuple.\n    Args:\n        arg: The argument to wrap.\n    Returns:\n        The wrapped argument.\n    \"\"\"\nif not isinstance(arg, list) and not isinstance(arg, tuple):\nreturn [arg]\nreturn arg\n</code></pre>"},{"location":"reference/modules/model/common/","title":"common","text":""},{"location":"reference/modules/model/common/dp/","title":"dp","text":""},{"location":"reference/modules/model/common/dp/#nhssynth.modules.model.common.dp.DPMixin","title":"<code>DPMixin</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Mixin class to make a <code>Model</code> differentially private</p> <p>Parameters:</p> Name Type Description Default <code>target_epsilon</code> <code>float</code> <p>The target epsilon for the model during training</p> <code>3.0</code> <code>target_delta</code> <code>Optional[float]</code> <p>The target delta for the model during training</p> <code>None</code> <code>max_grad_norm</code> <code>float</code> <p>The maximum norm for the gradients, they are trimmed to this norm if they are larger</p> <code>5.0</code> <code>secure_mode</code> <code>bool</code> <p>Whether to use the 'secure mode' of PyTorch's DP-SGD implementation via the <code>csprng</code> package</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>target_epsilon</code> <code>float</code> <p>The target epsilon for the model during training</p> <code>target_delta</code> <code>float</code> <p>The target delta for the model during training</p> <code>max_grad_norm</code> <code>float</code> <p>The maximum norm for the gradients, they are trimmed to this norm if they are larger</p> <code>secure_mode</code> <code>bool</code> <p>Whether to use the 'secure mode' of PyTorch's DP-SGD implementation via the <code>csprng</code> package</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the inheritor is not a <code>Model</code></p> Source code in <code>src/nhssynth/modules/model/common/dp.py</code> <pre><code>class DPMixin(ABC):\n\"\"\"\n    Mixin class to make a [`Model`][nhssynth.modules.model.common.model.Model] differentially private\n    Args:\n        target_epsilon: The target epsilon for the model during training\n        target_delta: The target delta for the model during training\n        max_grad_norm: The maximum norm for the gradients, they are trimmed to this norm if they are larger\n        secure_mode: Whether to use the 'secure mode' of PyTorch's DP-SGD implementation via the `csprng` package\n    Attributes:\n        target_epsilon: The target epsilon for the model during training\n        target_delta: The target delta for the model during training\n        max_grad_norm: The maximum norm for the gradients, they are trimmed to this norm if they are larger\n        secure_mode: Whether to use the 'secure mode' of PyTorch's DP-SGD implementation via the `csprng` package\n    Raises:\n        TypeError: If the inheritor is not a `Model`\n    \"\"\"\ndef __init__(\nself,\n*args,\ntarget_epsilon: float = 3.0,\ntarget_delta: Optional[float] = None,\nmax_grad_norm: float = 5.0,\nsecure_mode: bool = False,\n**kwargs,\n):\nif not isinstance(self, Model):\nraise TypeError(\"DPMixin can only be used with Model classes\")\nsuper(DPMixin, self).__init__(*args, **kwargs)\nself.target_epsilon: float = target_epsilon\nself.target_delta: float = target_delta or 1 / self.nrows\nself.max_grad_norm: float = max_grad_norm\nself.secure_mode: bool = secure_mode\ndef make_private(self, num_epochs: int, module: Optional[nn.Module] = None) -&gt; GradSampleModule:\n\"\"\"\n        Make the passed module (or the full model if a module is not passed), and its associated optimizer and data loader private.\n        Args:\n            num_epochs: The number of epochs to train for, used to calculate the privacy budget.\n            module: The module to make private.\n        Returns:\n            The privatised module.\n        \"\"\"\nmodule = module or self\nself.privacy_engine = PrivacyEngine(secure_mode=self.secure_mode)\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log\")\nwarnings.filterwarnings(\"ignore\", message=\"Optimal order is the largest alpha\")\nmodule, module.optim, self.data_loader = self.privacy_engine.make_private_with_epsilon(\nmodule=module,\noptimizer=module.optim,\ndata_loader=self.data_loader,\nepochs=num_epochs,\ntarget_epsilon=self.target_epsilon,\ntarget_delta=self.target_delta,\nmax_grad_norm=self.max_grad_norm,\n)\nprint(\nf\"Using sigma={module.optim.noise_multiplier} and C={self.max_grad_norm} to target (\u03b5, \u03b4) = ({self.target_epsilon}, {self.target_delta})-differential privacy.\".format()\n)\nself.get_epsilon = self.privacy_engine.accountant.get_epsilon\nreturn module\ndef _generate_metric_str(self, key) -&gt; str:\n\"\"\"Generates a string to display the current value of the metric `key`.\"\"\"\nif key == \"Privacy\":\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log\")\nwarnings.filterwarnings(\"ignore\", message=\"Optimal order is the largest alpha\")\nval = self.get_epsilon(self.target_delta)\nself.metrics[key] = np.append(self.metrics[key], val)\nreturn f\"{(key + ' \u03b5 Spent:').ljust(self.max_length)}  {val:.4f}\"\nelse:\nreturn super()._generate_metric_str(key)\ndef get_args() -&gt; list[str]:\nreturn [\"target_epsilon\", \"target_delta\", \"max_grad_norm\", \"secure_mode\"]\ndef _start_training(self, num_epochs, patience, displayed_metrics):\nself.make_private(num_epochs)\nsuper()._start_training(num_epochs, patience, displayed_metrics)\n</code></pre>"},{"location":"reference/modules/model/common/dp/#nhssynth.modules.model.common.dp.DPMixin.make_private","title":"<code>make_private(num_epochs, module=None)</code>","text":"<p>Make the passed module (or the full model if a module is not passed), and its associated optimizer and data loader private.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>The number of epochs to train for, used to calculate the privacy budget.</p> required <code>module</code> <code>Optional[Module]</code> <p>The module to make private.</p> <code>None</code> <p>Returns:</p> Type Description <code>GradSampleModule</code> <p>The privatised module.</p> Source code in <code>src/nhssynth/modules/model/common/dp.py</code> <pre><code>def make_private(self, num_epochs: int, module: Optional[nn.Module] = None) -&gt; GradSampleModule:\n\"\"\"\n    Make the passed module (or the full model if a module is not passed), and its associated optimizer and data loader private.\n    Args:\n        num_epochs: The number of epochs to train for, used to calculate the privacy budget.\n        module: The module to make private.\n    Returns:\n        The privatised module.\n    \"\"\"\nmodule = module or self\nself.privacy_engine = PrivacyEngine(secure_mode=self.secure_mode)\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log\")\nwarnings.filterwarnings(\"ignore\", message=\"Optimal order is the largest alpha\")\nmodule, module.optim, self.data_loader = self.privacy_engine.make_private_with_epsilon(\nmodule=module,\noptimizer=module.optim,\ndata_loader=self.data_loader,\nepochs=num_epochs,\ntarget_epsilon=self.target_epsilon,\ntarget_delta=self.target_delta,\nmax_grad_norm=self.max_grad_norm,\n)\nprint(\nf\"Using sigma={module.optim.noise_multiplier} and C={self.max_grad_norm} to target (\u03b5, \u03b4) = ({self.target_epsilon}, {self.target_delta})-differential privacy.\".format()\n)\nself.get_epsilon = self.privacy_engine.accountant.get_epsilon\nreturn module\n</code></pre>"},{"location":"reference/modules/model/common/model/","title":"model","text":""},{"location":"reference/modules/model/common/model/#nhssynth.modules.model.common.model.Model","title":"<code>Model</code>","text":"<p>             Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for all NHSSynth models</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to train on</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>A <code>MetaTransformer</code> to use for converting the generated data to match the original data</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use during training</p> <code>32</code> <code>use_gpu</code> <code>bool</code> <p>Flag to determine whether to use the GPU (if available)</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>nrows</code> <p>The number of rows in the <code>data</code></p> <code>ncols</code> <p>The number of columns in the <code>data</code></p> <code>columns</code> <code>Index</code> <p>The names of the columns in the <code>data</code></p> <code>metatransformer</code> <p>The <code>MetaTransformer</code> (potentially) associated with the model</p> <code>multi_column_indices</code> <code>list[list[int]]</code> <p>A list of lists of column indices, where each sublist containts the indices for a one-hot encoded column</p> <code>single_column_indices</code> <code>list[int]</code> <p>Indices of all non-onehot columns</p> <code>data_loader</code> <code>DataLoader</code> <p>A PyTorch DataLoader for the <code>data</code></p> <code>private</code> <code>DataLoader</code> <p>Whether the model is private, i.e. whether the <code>DPMixin</code> class has been inherited</p> <code>device</code> <code>DataLoader</code> <p>The device to use for training (CPU or GPU)</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>Model</code> class is directly instantiated (i.e. not inherited)</p> <code>AssertionError</code> <p>If the number of columns in the <code>data</code> does not match the number of indices in <code>multi_column_indices</code> and <code>single_column_indices</code></p> <code>UserWarning</code> <p>If <code>use_gpu</code> is True but no GPU is available</p> Source code in <code>src/nhssynth/modules/model/common/model.py</code> <pre><code>class Model(nn.Module, ABC):\n\"\"\"\n    Abstract base class for all NHSSynth models\n    Args:\n        data: The data to train on\n        metatransformer: A `MetaTransformer` to use for converting the generated data to match the original data\n        batch_size: The batch size to use during training\n        use_gpu: Flag to determine whether to use the GPU (if available)\n    Attributes:\n        nrows: The number of rows in the `data`\n        ncols: The number of columns in the `data`\n        columns: The names of the columns in the `data`\n        metatransformer: The `MetaTransformer` (potentially) associated with the model\n        multi_column_indices: A list of lists of column indices, where each sublist containts the indices for a one-hot encoded column\n        single_column_indices: Indices of all non-onehot columns\n        data_loader: A PyTorch DataLoader for the `data`\n        private: Whether the model is private, i.e. whether the `DPMixin` class has been inherited\n        device: The device to use for training (CPU or GPU)\n    Raises:\n        TypeError: If the `Model` class is directly instantiated (i.e. not inherited)\n        AssertionError: If the number of columns in the `data` does not match the number of indices in `multi_column_indices` and `single_column_indices`\n        UserWarning: If `use_gpu` is True but no GPU is available\n    \"\"\"\ndef __init__(\nself,\ndata: pd.DataFrame,\nmetatransformer: MetaTransformer,\nbatch_size: int = 32,\nuse_gpu: bool = False,\n) -&gt; None:\nif type(self) is Model:\nraise TypeError(\"Cannot directly instantiate the `Model` class\")\nsuper().__init__()\nself.nrows, self.ncols = data.shape\nself.columns: pd.Index = data.columns\nself.metatransformer = metatransformer\nself.multi_column_indices: list[list[int]] = metatransformer.multi_column_indices\nself.single_column_indices: list[int] = metatransformer.single_column_indices\nassert len(self.single_column_indices) + sum([len(x) for x in self.multi_column_indices]) == self.ncols\nself.data_loader: DataLoader = DataLoader(\n# TODO Should the data also all be turned into floats?\nTensorDataset(torch.Tensor(data.to_numpy())),\npin_memory=True,\nbatch_size=batch_size,\n)\nself.setup_device(use_gpu)\ndef setup_device(self, use_gpu: bool) -&gt; None:\n\"\"\"Sets up the device to use for training (CPU or GPU) depending on `use_gpu` and device availability.\"\"\"\nif use_gpu:\nif torch.cuda.is_available():\nself.device: torch.device = torch.device(\"cuda:0\")\nelse:\nwarnings.warn(\"`use_gpu` was provided but no GPU is available, using CPU\")\nself.device: torch.device = torch.device(\"cpu\")\ndef save(self, filename: str) -&gt; None:\n\"\"\"Saves the model to `filename`.\"\"\"\ntorch.save(self.state_dict(), filename)\ndef load(self, path: str) -&gt; None:\n\"\"\"Loads the model from `path`.\"\"\"\nself.load_state_dict(torch.load(path))\n@classmethod\n@abstractmethod\ndef get_args() -&gt; list[str]:\n\"\"\"Returns the list of arguments to look for in an `argparse.Namespace`, these must map to the arguments of the inheritor.\"\"\"\nraise NotImplementedError\ndef _start_training(self, num_epochs: int, patience: int, displayed_metrics: list[str]) -&gt; None:\n\"\"\"\n        Initialises the training process.\n        Args:\n            num_epochs: The number of epochs to train for\n            patience: The number of epochs to wait before stopping training early if the loss does not improve\n            displayed_metrics: The metrics to display during training, this should be set to an empty list if running `train` in a notebook or the output may be messy\n        Attributes:\n            metrics: A dictionary of lists of tracked metrics, where each list contains the values for each batch\n            stats_bars: A dictionary of tqdm status bars for each tracked metric\n            max_length: The maximum length of the tracked metric names, used for formatting the tqdm status bars\n            start_time: The time at which training started\n            update_time: The time at which the tqdm status bars were last updated\n        \"\"\"\nself.num_epochs = num_epochs\nself.patience = patience\nself.metrics = {metric: np.empty(0, dtype=float) for metric in TRACKED_METRICS}\nif not hasattr(self, \"target_epsilon\"):\nself.metrics.pop(\"Privacy\", None)\nif \"Privacy\" in displayed_metrics:\ndisplayed_metrics.remove(\"Privacy\")\nself.stats_bars = {\nmetric: tqdm(total=0, desc=\"\", position=i, bar_format=\"{desc}\", leave=True)\nfor i, metric in enumerate(displayed_metrics)\n}\nself.max_length = max([len(add_spaces_before_caps(s)) + 5 for s in displayed_metrics] + [20])\nself.start_time = self.update_time = time.time()\ndef _generate_metric_str(self, key) -&gt; str:\n\"\"\"Generates a string to display the current value of the metric `key`.\"\"\"\nreturn f\"{(add_spaces_before_caps(key) + ':').ljust(self.max_length)}  {np.mean(self.metrics[key][-len(self.data_loader) :]):.4f}\"\ndef _record_metrics(self, losses):\n\"\"\"Records the metrics for the current batch to file and updates the tqdm status bars.\"\"\"\nfor key in self.metrics.keys():\nif key in losses:\nif losses[key]:\nself.metrics[key] = np.append(self.metrics[key], losses[key].item())\nif time.time() - self.update_time &gt; 0.5:\nfor key, stats_bar in self.stats_bars.items():\nstats_bar.set_description_str(self._generate_metric_str(key))\nself.update_time = time.time()\ndef _check_patience(self, epoch: int, metric: float) -&gt; bool:\n\"\"\"Maintains `_min_metric` and `_stop_counter` to determine whether to stop training early according to `patience`.\"\"\"\nif epoch == 0:\nself._stop_counter = 0\nself._min_metric = metric\nself._patience_delta = self._min_metric / 1e4\nif metric &lt; (self._min_metric - self._patience_delta):\nself._min_metric = metric\nself._stop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nself._stop_counter += 1\nreturn self._stop_counter == self.patience\ndef _finish_training(self, num_epochs: int) -&gt; None:\n\"\"\"Closes each of the tqdm status bars and prints the time taken to do `num_epochs`.\"\"\"\nfor stats_bar in self.stats_bars.values():\nstats_bar.close()\ntqdm.write(f\"Completed {num_epochs} epochs in {time.time() - self.start_time:.2f} seconds.\\033[0m\")\n</code></pre>"},{"location":"reference/modules/model/common/model/#nhssynth.modules.model.common.model.Model.get_args","title":"<code>get_args()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Returns the list of arguments to look for in an <code>argparse.Namespace</code>, these must map to the arguments of the inheritor.</p> Source code in <code>src/nhssynth/modules/model/common/model.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_args() -&gt; list[str]:\n\"\"\"Returns the list of arguments to look for in an `argparse.Namespace`, these must map to the arguments of the inheritor.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/modules/model/common/model/#nhssynth.modules.model.common.model.Model.load","title":"<code>load(path)</code>","text":"<p>Loads the model from <code>path</code>.</p> Source code in <code>src/nhssynth/modules/model/common/model.py</code> <pre><code>def load(self, path: str) -&gt; None:\n\"\"\"Loads the model from `path`.\"\"\"\nself.load_state_dict(torch.load(path))\n</code></pre>"},{"location":"reference/modules/model/common/model/#nhssynth.modules.model.common.model.Model.save","title":"<code>save(filename)</code>","text":"<p>Saves the model to <code>filename</code>.</p> Source code in <code>src/nhssynth/modules/model/common/model.py</code> <pre><code>def save(self, filename: str) -&gt; None:\n\"\"\"Saves the model to `filename`.\"\"\"\ntorch.save(self.state_dict(), filename)\n</code></pre>"},{"location":"reference/modules/model/common/model/#nhssynth.modules.model.common.model.Model.setup_device","title":"<code>setup_device(use_gpu)</code>","text":"<p>Sets up the device to use for training (CPU or GPU) depending on <code>use_gpu</code> and device availability.</p> Source code in <code>src/nhssynth/modules/model/common/model.py</code> <pre><code>def setup_device(self, use_gpu: bool) -&gt; None:\n\"\"\"Sets up the device to use for training (CPU or GPU) depending on `use_gpu` and device availability.\"\"\"\nif use_gpu:\nif torch.cuda.is_available():\nself.device: torch.device = torch.device(\"cuda:0\")\nelse:\nwarnings.warn(\"`use_gpu` was provided but no GPU is available, using CPU\")\nself.device: torch.device = torch.device(\"cpu\")\n</code></pre>"},{"location":"reference/modules/model/models/","title":"models","text":""},{"location":"reference/modules/model/models/dpvae/","title":"dpvae","text":""},{"location":"reference/modules/model/models/dpvae/#nhssynth.modules.model.models.dpvae.DPVAE","title":"<code>DPVAE</code>","text":"<p>             Bases: <code>DPMixin</code>, <code>VAE</code></p> <p>A differentially private VAE. Accepts <code>VAE</code> arguments as well as <code>DPMixin</code> arguments.</p> Source code in <code>src/nhssynth/modules/model/models/dpvae.py</code> <pre><code>class DPVAE(DPMixin, VAE):\n\"\"\"\n    A differentially private VAE. Accepts [`VAE`][nhssynth.modules.model.models.vae.VAE] arguments\n    as well as [`DPMixin`][nhssynth.modules.model.common.dp.DPMixin] arguments.\n    \"\"\"\ndef __init__(\nself,\n*args,\ntarget_epsilon: float = 3.0,\ntarget_delta: Optional[float] = None,\nmax_grad_norm: float = 5.0,\nsecure_mode: bool = False,\nshared_optimizer: bool = False,\n**kwargs,\n) -&gt; None:\nsuper(DPVAE, self).__init__(\n*args,\ntarget_epsilon=target_epsilon,\ntarget_delta=target_delta,\nmax_grad_norm=max_grad_norm,\nsecure_mode=secure_mode,\n# TODO fix shared_optimizer workflow for DP models\nshared_optimizer=False,\n**kwargs,\n)\ndef make_private(self, num_epochs: int) -&gt; GradSampleModule:\n\"\"\"\n        Make the [`Decoder`][nhssynth.modules.model.models.vae.Decoder] differentially private\n        unless `shared_optimizer` is True, in which case the whole VAE will be privatised.\n        Args:\n            num_epochs: The number of epochs to train for\n        \"\"\"\nif self.shared_optimizer:\nsuper().make_private(num_epochs)\nelse:\nself.decoder = super().make_private(num_epochs, self.decoder)\n@classmethod\ndef get_args(cls) -&gt; list[str]:\nreturn VAE.get_args() + DPMixin.get_args()\n</code></pre>"},{"location":"reference/modules/model/models/dpvae/#nhssynth.modules.model.models.dpvae.DPVAE.make_private","title":"<code>make_private(num_epochs)</code>","text":"<p>Make the <code>Decoder</code> differentially private unless <code>shared_optimizer</code> is True, in which case the whole VAE will be privatised.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>The number of epochs to train for</p> required Source code in <code>src/nhssynth/modules/model/models/dpvae.py</code> <pre><code>def make_private(self, num_epochs: int) -&gt; GradSampleModule:\n\"\"\"\n    Make the [`Decoder`][nhssynth.modules.model.models.vae.Decoder] differentially private\n    unless `shared_optimizer` is True, in which case the whole VAE will be privatised.\n    Args:\n        num_epochs: The number of epochs to train for\n    \"\"\"\nif self.shared_optimizer:\nsuper().make_private(num_epochs)\nelse:\nself.decoder = super().make_private(num_epochs, self.decoder)\n</code></pre>"},{"location":"reference/modules/model/models/vae/","title":"vae","text":""},{"location":"reference/modules/model/models/vae/#nhssynth.modules.model.models.vae.Decoder","title":"<code>Decoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>src/nhssynth/modules/model/models/vae.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\ndef __init__(\nself,\noutput_dim: int,\ndecoder_latent_dim: int,\ndecoder_hidden_dim: int,\ndecoder_activation: str,\ndecoder_learning_rate: float,\nshared_optimizer: bool,\n) -&gt; None:\nsuper().__init__()\nactivation = ACTIVATION_FUNCTIONS[decoder_activation]\nself.net = nn.Sequential(\nnn.Linear(decoder_latent_dim, decoder_hidden_dim),\nactivation(),\nnn.Linear(decoder_hidden_dim, decoder_hidden_dim),\nactivation(),\nnn.Linear(decoder_hidden_dim, output_dim),\n)\nif not shared_optimizer:\nself.optim = torch.optim.Adam(self.parameters(), lr=decoder_learning_rate)\ndef forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/models/vae/#nhssynth.modules.model.models.vae.Encoder","title":"<code>Encoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>src/nhssynth/modules/model/models/vae.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)\"\"\"\ndef __init__(\nself,\ninput_dim: int,\nencoder_latent_dim: int,\nencoder_hidden_dim: int,\nencoder_activation: str,\nencoder_learning_rate: float,\nshared_optimizer: bool,\n) -&gt; None:\nsuper().__init__()\nactivation = ACTIVATION_FUNCTIONS[encoder_activation]\nself.latent_dim = encoder_latent_dim\nself.net = nn.Sequential(\nnn.Linear(input_dim, encoder_hidden_dim),\nactivation(),\nnn.Linear(encoder_hidden_dim, encoder_hidden_dim),\nactivation(),\nnn.Linear(encoder_hidden_dim, 2 * encoder_latent_dim),\n)\nif not shared_optimizer:\nself.optim = torch.optim.Adam(self.parameters(), lr=encoder_learning_rate)\ndef forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/models/vae/#nhssynth.modules.model.models.vae.VAE","title":"<code>VAE</code>","text":"<p>             Bases: <code>Model</code></p> <p>A Variational Autoencoder (VAE) model. Accepts <code>Model</code> arguments as well as the following:</p> <p>Parameters:</p> Name Type Description Default <code>encoder_latent_dim</code> <code>int</code> <p>The dimensionality of the latent space.</p> <code>256</code> <code>encoder_hidden_dim</code> <code>int</code> <p>The dimensionality of the hidden layers in the encoder.</p> <code>256</code> <code>encoder_activation</code> <code>str</code> <p>The activation function to use in the encoder.</p> <code>'leaky_relu'</code> <code>encoder_learning_rate</code> <code>float</code> <p>The learning rate for the encoder.</p> <code>0.001</code> <code>decoder_latent_dim</code> <code>int</code> <p>The dimensionality of the hidden layers in the decoder.</p> <code>256</code> <code>decoder_hidden_dim</code> <code>int</code> <p>The dimensionality of the hidden layers in the decoder.</p> <code>32</code> <code>decoder_activation</code> <code>str</code> <p>The activation function to use in the decoder.</p> <code>'leaky_relu'</code> <code>decoder_learning_rate</code> <code>float</code> <p>The learning rate for the decoder.</p> <code>0.001</code> <code>shared_optimizer</code> <code>bool</code> <p>Whether to use a shared optimizer for the encoder and decoder.</p> <code>True</code> Source code in <code>src/nhssynth/modules/model/models/vae.py</code> <pre><code>class VAE(Model):\n\"\"\"\n    A Variational Autoencoder (VAE) model. Accepts [`Model`][nhssynth.modules.model.common.model.Model] arguments as well as the following:\n    Args:\n        encoder_latent_dim: The dimensionality of the latent space.\n        encoder_hidden_dim: The dimensionality of the hidden layers in the encoder.\n        encoder_activation: The activation function to use in the encoder.\n        encoder_learning_rate: The learning rate for the encoder.\n        decoder_latent_dim: The dimensionality of the hidden layers in the decoder.\n        decoder_hidden_dim: The dimensionality of the hidden layers in the decoder.\n        decoder_activation: The activation function to use in the decoder.\n        decoder_learning_rate: The learning rate for the decoder.\n        shared_optimizer: Whether to use a shared optimizer for the encoder and decoder.\n    \"\"\"\ndef __init__(\nself,\n*args,\nencoder_latent_dim: int = 256,\nencoder_hidden_dim: int = 256,\nencoder_activation: str = \"leaky_relu\",\nencoder_learning_rate: float = 1e-3,\ndecoder_latent_dim: int = 256,\ndecoder_hidden_dim: int = 32,\ndecoder_activation: str = \"leaky_relu\",\ndecoder_learning_rate: float = 1e-3,\nshared_optimizer: bool = True,\n**kwargs,\n) -&gt; None:\nsuper(VAE, self).__init__(*args, **kwargs)\nself.shared_optimizer = shared_optimizer\nself.encoder = Encoder(\nself.ncols,\nencoder_latent_dim,\nencoder_hidden_dim,\nencoder_activation,\nencoder_learning_rate,\nself.shared_optimizer,\n).to(self.device)\nself.decoder = Decoder(\nself.ncols,\ndecoder_latent_dim,\ndecoder_hidden_dim,\ndecoder_activation,\ndecoder_learning_rate,\nself.shared_optimizer,\n).to(self.device)\nself.noiser = Noiser(\nlen(self.single_column_indices),\n).to(self.device)\nif self.shared_optimizer:\nassert (\nencoder_learning_rate == decoder_learning_rate\n), \"If `shared_optimizer` is True, `encoder_learning_rate` must equal `decoder_learning_rate`\"\nself.optim = torch.optim.Adam(\nlist(self.encoder.parameters()) + list(self.decoder.parameters()),\nlr=encoder_learning_rate,\n)\nself.zero_grad = self.optim.zero_grad\nself.step = self.optim.step\nelse:\nself.zero_grad = lambda: (self.encoder.optim.zero_grad(), self.decoder.optim.zero_grad())\nself.step = lambda: (self.encoder.optim.step(), self.decoder.optim.step())\n@classmethod\ndef get_args(cls) -&gt; list[str]:\nreturn [\n\"encoder_latent_dim\",\n\"encoder_hidden_dim\",\n\"encoder_activation\",\n\"encoder_learning_rate\",\n\"decoder_latent_dim\",\n\"decoder_hidden_dim\",\n\"decoder_activation\",\n\"decoder_learning_rate\",\n\"shared_optimizer\",\n]\ndef reconstruct(self, X):\nmu_z, logsigma_z = self.encoder(X)\nx_recon = self.decoder(mu_z)\nreturn x_recon\ndef generate(self, N: Optional[int] = None) -&gt; pd.DataFrame:\nN = N or self.nrows\nz_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", message=\"Using a non-full backward hook\")\nx_gen = self.decoder(z_samples)\nx_gen_ = torch.ones_like(x_gen, device=self.device)\nif self.multi_column_indices != [[]]:\nfor cat_idxs in self.multi_column_indices:\nx_gen_[:, cat_idxs] = torch.distributions.one_hot_categorical.OneHotCategorical(\nlogits=x_gen[:, cat_idxs]\n).sample()\nx_gen_[:, self.single_column_indices] = x_gen[:, self.single_column_indices] + torch.exp(\nself.noiser(x_gen[:, self.single_column_indices])\n) * torch.randn_like(x_gen[:, self.single_column_indices])\nif torch.cuda.is_available():\nx_gen_ = x_gen_.cpu()\nreturn self.metatransformer.inverse_apply(pd.DataFrame(x_gen_.detach(), columns=self.columns))\ndef loss(self, X):\nmu_z, logsigma_z = self.encoder(X)\np = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\nq = Normal(mu_z, torch.exp(logsigma_z))\nkld = torch.sum(torch.distributions.kl_divergence(q, p))\ns = torch.randn_like(mu_z)\nz_samples = mu_z + s * torch.exp(logsigma_z)\nx_recon = self.decoder(z_samples)\ncategoric_loglik = 0\nif self.multi_column_indices != [[]]:\nfor cat_idxs in self.multi_column_indices:\ncategoric_loglik += -torch.nn.functional.cross_entropy(\nx_recon[:, cat_idxs],\ntorch.max(X[:, cat_idxs], 1)[1],\n).sum()\ngauss_loglik = 0\nif self.single_column_indices:\ngauss_loglik = (\nNormal(\nloc=x_recon[:, self.single_column_indices],\nscale=torch.exp(self.noiser(x_recon[:, self.single_column_indices])),\n)\n.log_prob(X[:, self.single_column_indices])\n.sum()\n)\nreconstruction_loss = -(categoric_loglik + gauss_loglik)\nelbo = kld + reconstruction_loss\nreturn {\n\"ELBO\": elbo / X.size()[0],\n\"ReconstructionLoss\": reconstruction_loss / X.size()[0],\n\"KLD\": kld / X.size()[0],\n\"CategoricalLoss\": categoric_loglik / X.size()[0],\n\"NumericalLoss\": gauss_loglik / X.size()[0],\n}\ndef train(\nself,\nnum_epochs: int = 100,\npatience: int = 5,\ndisplayed_metrics: list[str] = [\"ELBO\"],\n) -&gt; tuple[int, dict[str, list[float]]]:\n\"\"\"\n        Train the model.\n        Args:\n            num_epochs: Number of epochs to train for.\n            patience: Number of epochs to wait for improvement before early stopping.\n            displayed_metrics: List of metrics to display during training.\n        Returns:\n            The number of epochs trained for and a dictionary of the tracked metrics.\n        \"\"\"\nprint(\"\")\nself._start_training(num_epochs, patience, displayed_metrics)\nself.encoder.train()\nself.decoder.train()\nself.noiser.train()\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=len(self.stats_bars), leave=False):\nfor (Y_subset,) in tqdm(self.data_loader, desc=\"Batches\", position=len(self.stats_bars) + 1, leave=False):\nself.zero_grad()\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", message=\"Using a non-full backward hook\")\nlosses = self.loss(Y_subset.to(self.device))\nlosses[\"ELBO\"].backward()\nself.step()\nself._record_metrics(losses)\nelbo = np.mean(self.metrics[\"ELBO\"][-len(self.data_loader) :])\nif self._check_patience(epoch, elbo):\nnum_epochs = epoch + 1\nbreak\nself._finish_training(num_epochs)\nreturn (num_epochs, self.metrics)\n</code></pre>"},{"location":"reference/modules/model/models/vae/#nhssynth.modules.model.models.vae.VAE.train","title":"<code>train(num_epochs=100, patience=5, displayed_metrics=['ELBO'])</code>","text":"<p>Train the model.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>Number of epochs to train for.</p> <code>100</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait for improvement before early stopping.</p> <code>5</code> <code>displayed_metrics</code> <code>list[str]</code> <p>List of metrics to display during training.</p> <code>['ELBO']</code> <p>Returns:</p> Type Description <code>tuple[int, dict[str, list[float]]]</code> <p>The number of epochs trained for and a dictionary of the tracked metrics.</p> Source code in <code>src/nhssynth/modules/model/models/vae.py</code> <pre><code>def train(\nself,\nnum_epochs: int = 100,\npatience: int = 5,\ndisplayed_metrics: list[str] = [\"ELBO\"],\n) -&gt; tuple[int, dict[str, list[float]]]:\n\"\"\"\n    Train the model.\n    Args:\n        num_epochs: Number of epochs to train for.\n        patience: Number of epochs to wait for improvement before early stopping.\n        displayed_metrics: List of metrics to display during training.\n    Returns:\n        The number of epochs trained for and a dictionary of the tracked metrics.\n    \"\"\"\nprint(\"\")\nself._start_training(num_epochs, patience, displayed_metrics)\nself.encoder.train()\nself.decoder.train()\nself.noiser.train()\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=len(self.stats_bars), leave=False):\nfor (Y_subset,) in tqdm(self.data_loader, desc=\"Batches\", position=len(self.stats_bars) + 1, leave=False):\nself.zero_grad()\nwith warnings.catch_warnings():\nwarnings.filterwarnings(\"ignore\", message=\"Using a non-full backward hook\")\nlosses = self.loss(Y_subset.to(self.device))\nlosses[\"ELBO\"].backward()\nself.step()\nself._record_metrics(losses)\nelbo = np.mean(self.metrics[\"ELBO\"][-len(self.data_loader) :])\nif self._check_patience(epoch, elbo):\nnum_epochs = epoch + 1\nbreak\nself._finish_training(num_epochs)\nreturn (num_epochs, self.metrics)\n</code></pre>"},{"location":"reference/modules/plotting/","title":"plotting","text":""},{"location":"reference/modules/plotting/io/","title":"io","text":""},{"location":"reference/modules/plotting/io/#nhssynth.modules.plotting.io.check_input_paths","title":"<code>check_input_paths(fn_dataset, fn_typed, fn_evaluations, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_dataset</code> <code>str</code> <p>The base name of the dataset.</p> required <code>fn_typed</code> <code>str</code> <p>The name of the typed data file.</p> required <code>fn_evaluations</code> <code>str</code> <p>The name of the file containing the evaluation bundle.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>src/nhssynth/modules/plotting/io.py</code> <pre><code>def check_input_paths(fn_dataset: str, fn_typed: str, fn_evaluations: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_dataset: The base name of the dataset.\n        fn_typed: The name of the typed data file.\n        fn_evaluations: The name of the file containing the evaluation bundle.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_dataset, fn_typed, fn_evaluations = consistent_endings([fn_dataset, fn_typed, fn_evaluations])\nfn_typed, fn_evaluations = potential_suffixes([fn_typed, fn_evaluations], fn_dataset)\nwarn_if_path_supplied([fn_dataset, fn_typed, fn_evaluations], dir_experiment)\ncheck_exists([fn_typed], dir_experiment)\nreturn fn_dataset, fn_typed, fn_evaluations\n</code></pre>"},{"location":"reference/modules/plotting/io/#nhssynth.modules.plotting.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, DataFrame, DataFrame, dict[str, dict[str, Any]]]</code> <p>The data, metadata and metatransformer.</p> Source code in <code>src/nhssynth/modules/plotting/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, pd.DataFrame, dict[str, dict[str, Any]]]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif all(x in args.module_handover for x in [\"dataset\", \"typed\", \"evaluations\"]):\nreturn (\nargs.module_handover[\"dataset\"],\nargs.module_handover[\"typed\"],\nargs.module_handover[\"evaluations\"],\n)\nelse:\nfn_dataset, fn_typed, fn_evaluations = check_input_paths(\nargs.dataset, args.typed, args.evaluations, dir_experiment\n)\nwith open(dir_experiment / fn_typed, \"rb\") as f:\nreal_data = pickle.load(f)\nwith open(dir_experiment / fn_evaluations, \"rb\") as f:\nevaluations = pickle.load(f)\nreturn fn_dataset, real_data, evaluations\n</code></pre>"},{"location":"reference/modules/plotting/plots/","title":"plots","text":""},{"location":"reference/modules/plotting/plots/#nhssynth.modules.plotting.plots.factorize_all_categoricals","title":"<code>factorize_all_categoricals(df)</code>","text":"<p>Factorize all categorical columns in a dataframe.</p> Source code in <code>src/nhssynth/modules/plotting/plots.py</code> <pre><code>def factorize_all_categoricals(\ndf: pd.DataFrame,\n) -&gt; pd.DataFrame:\n\"\"\"Factorize all categorical columns in a dataframe.\"\"\"\nfor col in df.columns:\nif df[col].dtype == \"object\":\ndf[col] = pd.factorize(df[col])[0]\nelif df[col].dtype == \"datetime64[ns]\":\ndf[col] = pd.to_numeric(df[col])\nmin_val = df[col].min()\nmax_val = df[col].max()\ndf[col] = (df[col] - min_val) / (max_val - min_val)\nreturn df\n</code></pre>"},{"location":"reference/modules/plotting/run/","title":"run","text":""},{"location":"reference/modules/structure/","title":"structure","text":""},{"location":"reference/modules/structure/run/","title":"run","text":""}]}
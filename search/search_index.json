{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NHS Synth","text":"<p>Under construction, see the Code Reference or Model Card.</p>"},{"location":"model_card/","title":"Model Card: Variational AutoEncoder with Differential Privacy","text":"<p>Following Model Cards for Model Reporting (Mitchell et al.) and Lessons from Archives (Jo &amp; Gebru), we're providing some information about about the Variational AutoEncoder (VAE) with Differential Privacy within this repository.</p>"},{"location":"model_card/#model-details","title":"Model Details","text":"<p>The implementation of the Variational AutoEncoder (VAE) with Differential Privacy within this repository was created as part of an NHSX Analytics Unit PhD internship project undertaken by Dominic Danks (last commit to the repository: commit 88a4bdf). This model card describes the updated version of the model, released in March 2022.  Further information about the previous version created by Dom and its model implementation can be found in Section 5.4 of the associated report.</p>"},{"location":"model_card/#model-use","title":"Model Use","text":""},{"location":"model_card/#intended-use","title":"Intended Use","text":"<p>This model is intended for use in experimenting with differential privacy and VAEs.</p>"},{"location":"model_card/#training-data","title":"Training Data","text":"<p>Experiments in this repository are run against the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) dataset accessed via the pycox python library. We also performed further analysis on a single table that we extracted from MIMIC-III.</p>"},{"location":"model_card/#performance-and-limitations","title":"Performance and Limitations","text":"<p>A from-scratch VAE implementation was compared against various models available within the SDV framework using a variety of quality and privacy metrics on the SUPPORT dataset. The VAE was found to be competitive with all of these models across the various metrics. Differential Privacy (DP) was introduced via DP-SGD and the performance of the VAE for different levels of privacy was evaluated. It was found that as the level of Differential Privacy introduced by DP-SGD was increased, it became easier to distinguish between synthetic and real data.</p> <p>Proper evaluation of quality and privacy of synthetic data is challenging. In this work, we utilised metrics from the SDV library due to their natural integration with the rest of the codebase. A valuable extension of this work would be to apply a variety of external metrics, including more advanced adversarial attacks to more thoroughly evaluate the privacy of the considered methods, including as the level of DP is varied. It would also be of interest to apply DP-SGD and/or PATE to all of the considered methods and evaluate whether the performance drop as a function of implemented privacy is similar or different across the models.</p> <p>Currently the SynthVAE model only works for data which is 'clean'. I.e data that has no missingness or NaNs within its input. It can handle continuous, categorical and datetime variables. Special types such as nominal data cannot be handled properly however the model may still run. Column names have to be specified in the code for the variable group they belong to.</p> <p>Hyperparameter tuning of the model can result in errors if certain parameter values are selected. Most commonly, changing learning rate in our example results in errors during training. An extensive test to evaluate plausible ranges has not been performed as of yet. If you get errors during tuning then consider your hyperparameter values and adjust accordingly.</p>"},{"location":"modules/","title":"Modules","text":"<p>This folder contains all of the modules contained in this package. They can be used together or independently - through importing them into your existing codebase or using the CLI to select which / all modules to run.</p>"},{"location":"modules/#importing-a-module-from-this-package","title":"Importing a module from this package","text":"<p>After installing the package, you can simply do: <pre><code>from nhssynth.modules import &lt;module&gt;\n</code></pre> and you will be able to use it in your code!</p>"},{"location":"modules/#creating-a-new-module-and-folding-it-into-the-cli","title":"Creating a new module and folding it into the CLI","text":"<p>The following instructions specify how to extend this package with a new module:</p> <ol> <li>Create a folder for your module within the package, i.e. <code>src/nhssynth/modules/mymodule</code></li> <li> <p>Include within it a main executor function that accepts arguments from the CLI, i.e.</p> <pre><code>def myexecutor(args):\n...\n</code></pre> <p>In <code>mymodule/executor.py</code> and export it by adding <code>from .executor import myexecutor</code> to <code>mymodule/__init__.py</code>.</p> </li> <li> <p>In the <code>cli</code> folder, add a corresponding function to <code>arguments.py</code> and populate with arguments you want to expose in the CLI:</p> <pre><code>def add_mymodule_args(parser: argparse.ArgumentParser, override=False):\n...\n</code></pre> </li> <li> <p>Next, in <code>module_setup.py</code> make the following adjustments the following code:</p> <pre><code>from nhssynth.modules import ..., mymodule, ...\n</code></pre> <pre><code>MODULE_MAP = {\n...\n\"mymodule\": ModuleConfig(\nmymodule.myexecutor,\nadd_mymodule_args,\n\"&lt;description&gt;\",\n\"&lt;short help&gt;\",\n),\n...\n}\n</code></pre> <p>And (optionally) edit the following block if you want your module to be included in a full pipeline run:</p> <pre><code>PIPELINE = [..., mymodule, ...]  # NOTE this determines the order of a pipeline run\n</code></pre> </li> <li> <p>Congrats, your module is implemented!</p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli<ul> <li>config</li> <li>module_arguments</li> <li>module_setup</li> <li>run</li> </ul> </li> <li>common<ul> <li>common</li> <li>constants</li> <li>dicts</li> <li>io</li> </ul> </li> <li>modules<ul> <li>dataloader<ul> <li>io</li> <li>metadata</li> <li>metatransformer</li> <li>run</li> </ul> </li> <li>evaluation<ul> <li>metrics</li> <li>run</li> </ul> </li> <li>model<ul> <li>DPVAE</li> <li>io</li> <li>run</li> </ul> </li> <li>plotting<ul> <li>plot</li> <li>run</li> </ul> </li> <li>structure<ul> <li>run</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/cli/","title":"cli","text":""},{"location":"reference/cli/#nhssynth.cli.config","title":"<code>config</code>","text":""},{"location":"reference/cli/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>cli/config.py</code> <pre><code>def assemble_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\nargs_dict = vars(args)\nmodules_to_run = args_dict.pop(\"modules_to_run\")\nif len(modules_to_run) == 1:\nrun_type = modules_to_run[0]\nelif modules_to_run == PIPELINE:\nrun_type = \"pipeline\"\nelse:\nraise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n# Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\nmodule_args = {\nmodule_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\nfor module_name in modules_to_run\n}\n# Use the flat namespace to populate a nested (by module) dictionary of config args and values\nout_dict = {}\nfor module_name in modules_to_run:\nfor k in args_dict.copy().keys():\nif k in module_args[module_name]:\nif out_dict.get(module_name):\nout_dict[module_name].update({k: args_dict.pop(k)})\nelse:\nout_dict[module_name] = {k: args_dict.pop(k)}\n# Assemble the final dictionary in YAML-compliant form\nreturn {\n**({\"run_type\": run_type} if run_type else {}),\n**{\nk: v\nfor k, v in args_dict.items()\nif k not in {\"func\", \"experiment_name\", \"save_config\", \"save_config_path\"}\n},\n**out_dict,\n}\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>ArgumentParser</code> <p>The top-level parser.</p> required <code>module_parsers</code> <code>dict</code> <p>The dict of module-level parsers mapped to their names.</p> required <p>Returns:</p> Type Description <code>A tuple containing two elements</code> <ul> <li>A dictionary containing all arguments and their default values.<ul> <li>A list of kvps of the required arguments and their associated module.</li> </ul> </li> </ul> Source code in <code>cli/config.py</code> <pre><code>def get_default_and_required_args(\ntop_parser: argparse.ArgumentParser,\nmodule_parsers: dict[str, argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n    Args:\n        top_parser: The top-level parser.\n        module_parsers: The dict of module-level parsers mapped to their names.\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of kvps of the required arguments and their associated module.\n    \"\"\"\nall_actions = {\n\"top-level\": top_parser._actions,\n**{m: p._actions for m, p in module_parsers.items()},\n}\ndefaults = {}\nrequired_args = []\nfor module, actions in all_actions.items():\nfor action in actions:\nif action.dest not in [\"help\", \"==SUPPRESS==\"]:\ndefaults[action.dest] = action.default\nif action.required:\nrequired_args.append({\"arg\": action.dest, \"module\": module})\nreturn defaults, required_args\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.get_modules_to_run","title":"<code>get_modules_to_run(executor)</code>","text":"<p>Get the list of modules to run from the passed executor function.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Callable</code> <p>The executor function to run.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of module names to run.</p> Source code in <code>cli/config.py</code> <pre><code>def get_modules_to_run(executor: Callable) -&gt; list[str]:\n\"\"\"\n    Get the list of modules to run from the passed executor function.\n    Args:\n        executor: The executor function to run.\n    Returns:\n        A list of module names to run.\n    \"\"\"\nif executor == run_pipeline:\nreturn PIPELINE\nelse:\nreturn [get_key_by_value({mn: mc.func for mn, mc in MODULE_MAP.items()}, executor)]\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config Namespace object for the inferred modules to run and executes.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default config from the parser and <code>all_subparsers</code> for the <code>modules_to_run</code></li> <li>Overwrite this config with the specified subset (or full set) of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting config</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>ArgumentParser</code> <p>top-level ArgumentParser object</p> required <code>all_subparsers</code> <code>dict</code> <p>dictionary of ArgumentParser objects, one for each module</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>Namespace object containing the assembled configuration settings</p> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file</p> Source code in <code>cli/config.py</code> <pre><code>def read_config(\nargs: argparse.Namespace,\nparser: argparse.ArgumentParser,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config Namespace object for the inferred modules to run and executes.\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default config from the parser and `all_subparsers` for the `modules_to_run`\n    4. Overwrite this config with the specified subset (or full set) of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting config\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level ArgumentParser object\n        all_subparsers: dictionary of ArgumentParser objects, one for each module\n    Returns:\n        Namespace object containing the assembled configuration settings\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file\n    \"\"\"\n# Open the passed yaml file and load into a dictionary\nwith open(f\"config/{args.input_config}.yaml\") as stream:\nconfig_dict = yaml.safe_load(stream)\nvalid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\nrun_type = config_dict.pop(\"run_type\", None)\nif run_type == \"pipeline\":\nmodules_to_run = PIPELINE\nelse:\nmodules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\nif not args.custom_pipeline:\nmodules_to_run = sorted(modules_to_run, key=lambda x: PIPELINE.index(x))\nif not modules_to_run:\nwarnings.warn(\n\"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n)\nmodules_to_run = PIPELINE\n# Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\nargs_dict, required_args = get_default_and_required_args(\nparser, filter_dict(all_subparsers, modules_to_run, include=True)\n)\n# Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\nnon_default_passed_args_dict = {\nk: v\nfor k, v in vars(args).items()\nif k in [\"input_config\", \"custom_pipeline\"] or (k in args_dict and k != \"func\" and v != args_dict[k])\n}\n# Overwrite the default arguments with the ones from the yaml file\nargs_dict.update(flatten_dict(config_dict))\n# Overwrite the result of the above with any non-default CLI args\nargs_dict.update(non_default_passed_args_dict)\n# Create a new Namespace using the assembled dictionary\nnew_args = argparse.Namespace(**args_dict)\nassert all(\ngetattr(new_args, req_arg[\"arg\"]) for req_arg in required_args\n), f\"Required arguments are missing from the passed config file: {[ra['module'] + ':' + ra['arg'] for ra in required_args if not getattr(new_args, ra['arg'])]}\"\n# Run the appropriate execution function(s)\nnew_args.modules_to_run = modules_to_run\nfor module in new_args.modules_to_run:\nMODULE_MAP[module].func(new_args)\nreturn new_args\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary containing all subparsers for the config args.</p> required Source code in <code>cli/config.py</code> <pre><code>def write_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n    \"\"\"\nif not args.save_config_path:\nargs.save_config_path = f\"experiments/{args.experiment_name}/config_{args.experiment_name}.yaml\"\nif args.sdv_workflow:\ndel args.synthesizer\nargs_dict = assemble_config(args, all_subparsers)\nwith open(f\"{args.save_config_path}\", \"w\") as yaml_file:\nyaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments","title":"<code>module_arguments</code>","text":""},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\nparser.add_argument(\n\"-i\",\n\"--input\",\ntype=str,\nrequired=(not override),\nhelp=\"the name of the `.csv` file to prepare\",\n)\nparser.add_argument(\n\"--output\",\ntype=str,\ndefault=\"_prepared\",\nhelp=\"where to write the prepared data, defaults to `experiments/&lt;args.experiment_name&gt;/&lt;args.input_file&gt;_prepared\\{.csv/.pkl\\}`, only used when `--write-all` is provided and/or this is a full pipeline run / one that involves the `model` module\",\n)\nparser.add_argument(\n\"--metadata\",\ntype=str,\ndefault=\"_metadata\",\nhelp=\"metadata for the input data, defaults to `&lt;args.input_dir&gt;/&lt;args.input_file&gt;_metadata.yaml`\",\n)\nparser.add_argument(\n\"--discard-metadata\",\naction=\"store_true\",\nhelp=\"discard the generated metadata file (not recommended, this is required for reproducibility)\",\n)\nparser.add_argument(\n\"--metatransformer\",\ntype=str,\ndefault=\"_metatransformer\",\nhelp=\"name of the file to dump the `metatransformer` object used on the input data, defaults to `experiments/&lt;args.experiment_name&gt;/&lt;args.input_file&gt;_metatransformer.pkl`, only used when `--write-all` is provided and/or this is a full pipeline run / one that involves the `model` module\",\n)\nparser.add_argument(\n\"-d\",\n\"--data-dir\",\ntype=str,\ndefault=\"./data\",\nhelp=\"the directory to read and write data from and to\",\n)\nparser.add_argument(\n\"--index-col\",\ndefault=None,\nchoices=[None, 0],\nhelp=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n)\nparser.add_argument(\n\"--sdv-workflow\",\naction=\"store_true\",\nhelp=\"utilise the SDV synthesizer workflow for transformation and metadata, rather than a `HyperTransformer` from RDT\",\n)\nparser.add_argument(\n\"--allow-null-transformers\",\naction=\"store_true\",\nhelp=\"allow null / None transformers, i.e. leave some columns as they are\",\n)\nparser.add_argument(\n\"--collapse-yaml\",\naction=\"store_true\",\nhelp=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n)\n# TODO might be good to have something like this, needs some thought in how to only apply to appropriate transformers, without overriding metadata\n# parser.add_argument(\n#     \"--imputation-strategy\",\n#     default=None,\n#     help=\"imputation strategy for missing values, pick one of None, `mean`, `mode`, or a number\",\n# )\nparser.add_argument(\n\"-s\",\n\"--synthesizer\",\ntype=str,\ndefault=\"TVAE\",\nchoices=list(SDV_SYNTHESIZER_CHOICES.keys()),\nhelp=\"pick a synthesizer to use (note this can also be specified in the model module, these must match)\",\n)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\npass\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\nparser.add_argument(\n\"-r\",\n\"--real-data\",\ntype=str,\nhelp=\"name of the dataset, only REQUIRED when this module is run on its own\",\n)\nparser.add_argument(\n\"-p\",\n\"--prepared-data\",\ntype=str,\ndefault=\"_prepared\",\nhelp=\"name of the prepared dataset to load from `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_prepared.pkl`, only REQUIRED when this module is run on its own\",\n)\nparser.add_argument(\n\"-m\",\n\"--real-metatransformer\",\ndefault=\"_metatransformer\",\ntype=str,\nhelp=\"name of the `.pkl` file of the MetaTransformer used on the prepared data to load from `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_metatransformer.pkl` only REQUIRED when this module is run on its own\",\n)\nparser.add_argument(\n\"--model-file\",\ntype=str,\ndefault=\"_model\",\nhelp=\"specify the filename of the model to be saved in `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_model.pt`\",\n)\nparser.add_argument(\n\"--discard-model\",\naction=\"store_true\",\nhelp=\"discard the model after training\",\n)\nparser.add_argument(\n\"--synthetic-data\",\ntype=str,\ndefault=\"_synthetic\",\nhelp=\"specify the filename of the synthetic data to be written in `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_synthetic.csv`\",\n)\nparser.add_argument(\n\"--discard-synthetic\",\naction=\"store_true\",\nhelp=\"do not output the synthetic data generated during the run\",\n)\nparser.add_argument(\n\"--use-gpu\",\naction=\"store_true\",\nhelp=\"use the GPU for training\",\n)\nparser.add_argument(\n\"--non-private-training\",\naction=\"store_true\",\nhelp=\"train the model in a non-private way\",\n)\nparser.add_argument(\n\"--secure-rng\",\naction=\"store_true\",\ndefault=False,\nhelp=\"Enable Secure RNG to have trustworthy privacy guarantees. Comes at a performance cost\",\n)\nparser.add_argument(\n\"--num-epochs\",\ntype=int,\ndefault=100,\nhelp=\"number of epochs to train for\",\n)\nparser.add_argument(\n\"--latent-dim\",\ntype=int,\ndefault=256,\nhelp=\"the latent dimension of the model\",\n)\nparser.add_argument(\n\"--hidden-dim\",\ntype=int,\ndefault=256,\nhelp=\"the hidden dimension of the model\",\n)\nparser.add_argument(\n\"--learning-rate\",\ntype=float,\ndefault=1e-3,\nhelp=\"the learning rate for the model\",\n)\nparser.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=\"the batch size for the model\",\n)\nparser.add_argument(\n\"--patience\",\ntype=int,\ndefault=5,\nhelp=\"how many epochs the model is allowed to train for without improvement\",\n)\nparser.add_argument(\n\"--delta\",\ntype=int,\ndefault=10,\nhelp=\"the difference in successive ELBO values that register as an 'improvement'\",\n)\nparser.add_argument(\n\"--target-epsilon\",\ntype=float,\ndefault=1.0,\nhelp=\"the target epsilon for differential privacy\",\n)\nparser.add_argument(\n\"--target-delta\",\ntype=float,\ndefault=1e-5,\nhelp=\"the target delta for differential privacy\",\n)\nparser.add_argument(\n\"--max-grad-norm\",\ntype=int,\ndefault=10,\nhelp=\"the clipping threshold for gradients (only relevant under differential privacy)\",\n)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\npass\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_arguments.add_top_level_args","title":"<code>add_top_level_args(parser)</code>","text":"<p>Adds top-level arguments to an existing ArgumentParser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_top_level_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds top-level arguments to an existing ArgumentParser instance.\"\"\"\nparser.add_argument(\n\"-e\",\n\"--experiment-name\",\ntype=str,\ndefault=TIME,\nhelp=f\"name the experiment run to affect logging, config, and default-behaviour io, defaults to current time, i.e. `{TIME}`\",\n)\nparser.add_argument(\n\"--save-config\",\naction=\"store_true\",\nhelp=\"save the config provided via the cli\",\n)\nparser.add_argument(\n\"--save-config-path\",\ntype=str,\nhelp=\"where to save the config when `-sc` is provided, defaults to `experiments/&lt;experiment_name&gt;/config_&lt;experiment_name&gt;.yaml`\",\n)\nparser.add_argument(\n\"-s\",\n\"--seed\",\ntype=int,\nhelp=\"specify a seed for reproducibility\",\n)\nparser.add_argument(\n\"--write-all\",\naction=\"store_true\",\nhelp=\"write all outputs to disk, including those that are not strictly necessary i.e. intermediary outputs in a full pipeline run\",\n)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup","title":"<code>module_setup</code>","text":""},{"location":"reference/cli/#nhssynth.cli.module_setup.ModuleConfig","title":"<code> ModuleConfig        </code>","text":"Source code in <code>cli/module_setup.py</code> <pre><code>class ModuleConfig:\ndef __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\n) -&gt; None:\n\"\"\"\n        Represents a module's configuration, containing the following attributes:\n        Args:\n            func: A callable that executes the module's functionality.\n            add_args_func: A callable that populates the module's sub-parser arguments.\n            description: A description of the module's functionality.\n            help: A help message for the module's command-line interface.\n        \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.ModuleConfig.__init__","title":"<code>__init__(self, func, add_args_func, description, help)</code>  <code>special</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>A callable that executes the module's functionality.</p> required <code>add_args_func</code> <code>Callable[..., Any]</code> <p>A callable that populates the module's sub-parser arguments.</p> required <code>description</code> <code>str</code> <p>A description of the module's functionality.</p> required <code>help</code> <code>str</code> <p>A help message for the module's command-line interface.</p> required Source code in <code>cli/module_setup.py</code> <pre><code>def __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\n) -&gt; None:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n    Args:\n        func: A callable that executes the module's functionality.\n        add_args_func: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n    \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\nparser.add_argument(\n\"-c\",\n\"--input-config\",\nrequired=True,\nhelp=\"specify the config file name\",\n)\nparser.add_argument(\n\"-cp\",\n\"--custom-pipeline\",\naction=\"store_true\",\nhelp=\"infer a custom pipeline running order of modules from the config\",\n)\nfor module_name in VALID_MODULES:\ngroup = parser.add_argument_group(title=f\"{module_name} overrides\")\nMODULE_MAP[module_name].add_args_func(group, override=True)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> for each module in the pipeline.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` for each module in the pipeline.\"\"\"\nfor module_name in PIPELINE:\ngroup = parser.add_argument_group(title=module_name)\nMODULE_MAP[module_name].add_args_func(group)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>config</code> <code>ModuleConfig</code> <p>A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_subparser(\nsubparsers: argparse._SubParsersAction,\nname: str,\nconfig: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        config: A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.\n    Returns:\n        The newly created subparser.\n    \"\"\"\nparser = subparsers.add_parser(\nname=name,\ndescription=config.description,\nhelp=config.help,\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\nconfig.add_args_func(parser)\nparser.set_defaults(func=config.func)\nreturn parser\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\nprint(\"Running full pipeline...\")\nargs.modules_to_run = PIPELINE\nfor module_name in PIPELINE:\nargs = MODULE_MAP[module_name].func(args)\n</code></pre>"},{"location":"reference/cli/#nhssynth.cli.run","title":"<code>run</code>","text":""},{"location":"reference/cli/#nhssynth.cli.run.run","title":"<code>run()</code>","text":"<p>CLI for preparing, training and evaluating a synthetic data generator.</p> Source code in <code>cli/run.py</code> <pre><code>def run() -&gt; None:\n\"\"\"CLI for preparing, training and evaluating a synthetic data generator.\"\"\"\nparser = argparse.ArgumentParser(\nprog=\"nhssynth\", description=\"CLI for preparing, training and evaluating a synthetic data generator.\"\n)\nadd_top_level_args(parser)\n# Below we instantiate one subparser for each module + one for running with a config file and one for\n# doing a full pipeline run with CLI-specified config\nsubparsers = parser.add_subparsers()\nall_subparsers = {\nname: add_subparser(subparsers, name, option_config) for name, option_config in MODULE_MAP.items()\n}\nargs = parser.parse_args()\n# Use get to return None when no function has been set, i.e. user made no running choice\nexecutor = vars(args).get(\"func\")\nif executor:\nargs.modules_to_run = get_modules_to_run(executor)\nexecutor(args)\nelif hasattr(args, \"input_config\"):\nargs = read_config(args, parser, all_subparsers)\nelse:\nparser.print_help()\n# Whenever either are specified, we want to dump the configuration to allow for this run to be replicated\nif args.save_config or args.save_config_path:\nwrite_config(args, all_subparsers)\nprint(\"Complete!\")\n</code></pre>"},{"location":"reference/cli/config/","title":"config","text":""},{"location":"reference/cli/config/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>cli/config.py</code> <pre><code>def assemble_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\nargs_dict = vars(args)\nmodules_to_run = args_dict.pop(\"modules_to_run\")\nif len(modules_to_run) == 1:\nrun_type = modules_to_run[0]\nelif modules_to_run == PIPELINE:\nrun_type = \"pipeline\"\nelse:\nraise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n# Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\nmodule_args = {\nmodule_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\nfor module_name in modules_to_run\n}\n# Use the flat namespace to populate a nested (by module) dictionary of config args and values\nout_dict = {}\nfor module_name in modules_to_run:\nfor k in args_dict.copy().keys():\nif k in module_args[module_name]:\nif out_dict.get(module_name):\nout_dict[module_name].update({k: args_dict.pop(k)})\nelse:\nout_dict[module_name] = {k: args_dict.pop(k)}\n# Assemble the final dictionary in YAML-compliant form\nreturn {\n**({\"run_type\": run_type} if run_type else {}),\n**{\nk: v\nfor k, v in args_dict.items()\nif k not in {\"func\", \"experiment_name\", \"save_config\", \"save_config_path\"}\n},\n**out_dict,\n}\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>ArgumentParser</code> <p>The top-level parser.</p> required <code>module_parsers</code> <code>dict</code> <p>The dict of module-level parsers mapped to their names.</p> required <p>Returns:</p> Type Description <code>A tuple containing two elements</code> <ul> <li>A dictionary containing all arguments and their default values.<ul> <li>A list of kvps of the required arguments and their associated module.</li> </ul> </li> </ul> Source code in <code>cli/config.py</code> <pre><code>def get_default_and_required_args(\ntop_parser: argparse.ArgumentParser,\nmodule_parsers: dict[str, argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n    Args:\n        top_parser: The top-level parser.\n        module_parsers: The dict of module-level parsers mapped to their names.\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of kvps of the required arguments and their associated module.\n    \"\"\"\nall_actions = {\n\"top-level\": top_parser._actions,\n**{m: p._actions for m, p in module_parsers.items()},\n}\ndefaults = {}\nrequired_args = []\nfor module, actions in all_actions.items():\nfor action in actions:\nif action.dest not in [\"help\", \"==SUPPRESS==\"]:\ndefaults[action.dest] = action.default\nif action.required:\nrequired_args.append({\"arg\": action.dest, \"module\": module})\nreturn defaults, required_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_modules_to_run","title":"<code>get_modules_to_run(executor)</code>","text":"<p>Get the list of modules to run from the passed executor function.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Callable</code> <p>The executor function to run.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of module names to run.</p> Source code in <code>cli/config.py</code> <pre><code>def get_modules_to_run(executor: Callable) -&gt; list[str]:\n\"\"\"\n    Get the list of modules to run from the passed executor function.\n    Args:\n        executor: The executor function to run.\n    Returns:\n        A list of module names to run.\n    \"\"\"\nif executor == run_pipeline:\nreturn PIPELINE\nelse:\nreturn [get_key_by_value({mn: mc.func for mn, mc in MODULE_MAP.items()}, executor)]\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config Namespace object for the inferred modules to run and executes.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default config from the parser and <code>all_subparsers</code> for the <code>modules_to_run</code></li> <li>Overwrite this config with the specified subset (or full set) of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting config</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>ArgumentParser</code> <p>top-level ArgumentParser object</p> required <code>all_subparsers</code> <code>dict</code> <p>dictionary of ArgumentParser objects, one for each module</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>Namespace object containing the assembled configuration settings</p> <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file</p> Source code in <code>cli/config.py</code> <pre><code>def read_config(\nargs: argparse.Namespace,\nparser: argparse.ArgumentParser,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config Namespace object for the inferred modules to run and executes.\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default config from the parser and `all_subparsers` for the `modules_to_run`\n    4. Overwrite this config with the specified subset (or full set) of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting config\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level ArgumentParser object\n        all_subparsers: dictionary of ArgumentParser objects, one for each module\n    Returns:\n        Namespace object containing the assembled configuration settings\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file\n    \"\"\"\n# Open the passed yaml file and load into a dictionary\nwith open(f\"config/{args.input_config}.yaml\") as stream:\nconfig_dict = yaml.safe_load(stream)\nvalid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\nrun_type = config_dict.pop(\"run_type\", None)\nif run_type == \"pipeline\":\nmodules_to_run = PIPELINE\nelse:\nmodules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\nif not args.custom_pipeline:\nmodules_to_run = sorted(modules_to_run, key=lambda x: PIPELINE.index(x))\nif not modules_to_run:\nwarnings.warn(\n\"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n)\nmodules_to_run = PIPELINE\n# Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\nargs_dict, required_args = get_default_and_required_args(\nparser, filter_dict(all_subparsers, modules_to_run, include=True)\n)\n# Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\nnon_default_passed_args_dict = {\nk: v\nfor k, v in vars(args).items()\nif k in [\"input_config\", \"custom_pipeline\"] or (k in args_dict and k != \"func\" and v != args_dict[k])\n}\n# Overwrite the default arguments with the ones from the yaml file\nargs_dict.update(flatten_dict(config_dict))\n# Overwrite the result of the above with any non-default CLI args\nargs_dict.update(non_default_passed_args_dict)\n# Create a new Namespace using the assembled dictionary\nnew_args = argparse.Namespace(**args_dict)\nassert all(\ngetattr(new_args, req_arg[\"arg\"]) for req_arg in required_args\n), f\"Required arguments are missing from the passed config file: {[ra['module'] + ':' + ra['arg'] for ra in required_args if not getattr(new_args, ra['arg'])]}\"\n# Run the appropriate execution function(s)\nnew_args.modules_to_run = modules_to_run\nfor module in new_args.modules_to_run:\nMODULE_MAP[module].func(new_args)\nreturn new_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict</code> <p>A dictionary containing all subparsers for the config args.</p> required Source code in <code>cli/config.py</code> <pre><code>def write_config(\nargs: argparse.Namespace,\nall_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n    \"\"\"\nif not args.save_config_path:\nargs.save_config_path = f\"experiments/{args.experiment_name}/config_{args.experiment_name}.yaml\"\nif args.sdv_workflow:\ndel args.synthesizer\nargs_dict = assemble_config(args, all_subparsers)\nwith open(f\"{args.save_config_path}\", \"w\") as yaml_file:\nyaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/module_arguments/","title":"module_arguments","text":""},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\nparser.add_argument(\n\"-i\",\n\"--input\",\ntype=str,\nrequired=(not override),\nhelp=\"the name of the `.csv` file to prepare\",\n)\nparser.add_argument(\n\"--output\",\ntype=str,\ndefault=\"_prepared\",\nhelp=\"where to write the prepared data, defaults to `experiments/&lt;args.experiment_name&gt;/&lt;args.input_file&gt;_prepared\\{.csv/.pkl\\}`, only used when `--write-all` is provided and/or this is a full pipeline run / one that involves the `model` module\",\n)\nparser.add_argument(\n\"--metadata\",\ntype=str,\ndefault=\"_metadata\",\nhelp=\"metadata for the input data, defaults to `&lt;args.input_dir&gt;/&lt;args.input_file&gt;_metadata.yaml`\",\n)\nparser.add_argument(\n\"--discard-metadata\",\naction=\"store_true\",\nhelp=\"discard the generated metadata file (not recommended, this is required for reproducibility)\",\n)\nparser.add_argument(\n\"--metatransformer\",\ntype=str,\ndefault=\"_metatransformer\",\nhelp=\"name of the file to dump the `metatransformer` object used on the input data, defaults to `experiments/&lt;args.experiment_name&gt;/&lt;args.input_file&gt;_metatransformer.pkl`, only used when `--write-all` is provided and/or this is a full pipeline run / one that involves the `model` module\",\n)\nparser.add_argument(\n\"-d\",\n\"--data-dir\",\ntype=str,\ndefault=\"./data\",\nhelp=\"the directory to read and write data from and to\",\n)\nparser.add_argument(\n\"--index-col\",\ndefault=None,\nchoices=[None, 0],\nhelp=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n)\nparser.add_argument(\n\"--sdv-workflow\",\naction=\"store_true\",\nhelp=\"utilise the SDV synthesizer workflow for transformation and metadata, rather than a `HyperTransformer` from RDT\",\n)\nparser.add_argument(\n\"--allow-null-transformers\",\naction=\"store_true\",\nhelp=\"allow null / None transformers, i.e. leave some columns as they are\",\n)\nparser.add_argument(\n\"--collapse-yaml\",\naction=\"store_true\",\nhelp=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n)\n# TODO might be good to have something like this, needs some thought in how to only apply to appropriate transformers, without overriding metadata\n# parser.add_argument(\n#     \"--imputation-strategy\",\n#     default=None,\n#     help=\"imputation strategy for missing values, pick one of None, `mean`, `mode`, or a number\",\n# )\nparser.add_argument(\n\"-s\",\n\"--synthesizer\",\ntype=str,\ndefault=\"TVAE\",\nchoices=list(SDV_SYNTHESIZER_CHOICES.keys()),\nhelp=\"pick a synthesizer to use (note this can also be specified in the model module, these must match)\",\n)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\npass\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\nparser.add_argument(\n\"-r\",\n\"--real-data\",\ntype=str,\nhelp=\"name of the dataset, only REQUIRED when this module is run on its own\",\n)\nparser.add_argument(\n\"-p\",\n\"--prepared-data\",\ntype=str,\ndefault=\"_prepared\",\nhelp=\"name of the prepared dataset to load from `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_prepared.pkl`, only REQUIRED when this module is run on its own\",\n)\nparser.add_argument(\n\"-m\",\n\"--real-metatransformer\",\ndefault=\"_metatransformer\",\ntype=str,\nhelp=\"name of the `.pkl` file of the MetaTransformer used on the prepared data to load from `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_metatransformer.pkl` only REQUIRED when this module is run on its own\",\n)\nparser.add_argument(\n\"--model-file\",\ntype=str,\ndefault=\"_model\",\nhelp=\"specify the filename of the model to be saved in `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_model.pt`\",\n)\nparser.add_argument(\n\"--discard-model\",\naction=\"store_true\",\nhelp=\"discard the model after training\",\n)\nparser.add_argument(\n\"--synthetic-data\",\ntype=str,\ndefault=\"_synthetic\",\nhelp=\"specify the filename of the synthetic data to be written in `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_synthetic.csv`\",\n)\nparser.add_argument(\n\"--discard-synthetic\",\naction=\"store_true\",\nhelp=\"do not output the synthetic data generated during the run\",\n)\nparser.add_argument(\n\"--use-gpu\",\naction=\"store_true\",\nhelp=\"use the GPU for training\",\n)\nparser.add_argument(\n\"--non-private-training\",\naction=\"store_true\",\nhelp=\"train the model in a non-private way\",\n)\nparser.add_argument(\n\"--secure-rng\",\naction=\"store_true\",\ndefault=False,\nhelp=\"Enable Secure RNG to have trustworthy privacy guarantees. Comes at a performance cost\",\n)\nparser.add_argument(\n\"--num-epochs\",\ntype=int,\ndefault=100,\nhelp=\"number of epochs to train for\",\n)\nparser.add_argument(\n\"--latent-dim\",\ntype=int,\ndefault=256,\nhelp=\"the latent dimension of the model\",\n)\nparser.add_argument(\n\"--hidden-dim\",\ntype=int,\ndefault=256,\nhelp=\"the hidden dimension of the model\",\n)\nparser.add_argument(\n\"--learning-rate\",\ntype=float,\ndefault=1e-3,\nhelp=\"the learning rate for the model\",\n)\nparser.add_argument(\n\"--batch-size\",\ntype=int,\ndefault=32,\nhelp=\"the batch size for the model\",\n)\nparser.add_argument(\n\"--patience\",\ntype=int,\ndefault=5,\nhelp=\"how many epochs the model is allowed to train for without improvement\",\n)\nparser.add_argument(\n\"--delta\",\ntype=int,\ndefault=10,\nhelp=\"the difference in successive ELBO values that register as an 'improvement'\",\n)\nparser.add_argument(\n\"--target-epsilon\",\ntype=float,\ndefault=1.0,\nhelp=\"the target epsilon for differential privacy\",\n)\nparser.add_argument(\n\"--target-delta\",\ntype=float,\ndefault=1e-5,\nhelp=\"the target delta for differential privacy\",\n)\nparser.add_argument(\n\"--max-grad-norm\",\ntype=int,\ndefault=10,\nhelp=\"the clipping threshold for gradients (only relevant under differential privacy)\",\n)\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\npass\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_top_level_args","title":"<code>add_top_level_args(parser)</code>","text":"<p>Adds top-level arguments to an existing ArgumentParser instance.</p> Source code in <code>cli/module_arguments.py</code> <pre><code>def add_top_level_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds top-level arguments to an existing ArgumentParser instance.\"\"\"\nparser.add_argument(\n\"-e\",\n\"--experiment-name\",\ntype=str,\ndefault=TIME,\nhelp=f\"name the experiment run to affect logging, config, and default-behaviour io, defaults to current time, i.e. `{TIME}`\",\n)\nparser.add_argument(\n\"--save-config\",\naction=\"store_true\",\nhelp=\"save the config provided via the cli\",\n)\nparser.add_argument(\n\"--save-config-path\",\ntype=str,\nhelp=\"where to save the config when `-sc` is provided, defaults to `experiments/&lt;experiment_name&gt;/config_&lt;experiment_name&gt;.yaml`\",\n)\nparser.add_argument(\n\"-s\",\n\"--seed\",\ntype=int,\nhelp=\"specify a seed for reproducibility\",\n)\nparser.add_argument(\n\"--write-all\",\naction=\"store_true\",\nhelp=\"write all outputs to disk, including those that are not strictly necessary i.e. intermediary outputs in a full pipeline run\",\n)\n</code></pre>"},{"location":"reference/cli/module_setup/","title":"module_setup","text":""},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig","title":"<code> ModuleConfig        </code>","text":"Source code in <code>cli/module_setup.py</code> <pre><code>class ModuleConfig:\ndef __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\n) -&gt; None:\n\"\"\"\n        Represents a module's configuration, containing the following attributes:\n        Args:\n            func: A callable that executes the module's functionality.\n            add_args_func: A callable that populates the module's sub-parser arguments.\n            description: A description of the module's functionality.\n            help: A help message for the module's command-line interface.\n        \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig.__init__","title":"<code>__init__(self, func, add_args_func, description, help)</code>  <code>special</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>A callable that executes the module's functionality.</p> required <code>add_args_func</code> <code>Callable[..., Any]</code> <p>A callable that populates the module's sub-parser arguments.</p> required <code>description</code> <code>str</code> <p>A description of the module's functionality.</p> required <code>help</code> <code>str</code> <p>A help message for the module's command-line interface.</p> required Source code in <code>cli/module_setup.py</code> <pre><code>def __init__(\nself,\nfunc: Callable[..., Any],\nadd_args_func: Callable[..., Any],\ndescription: str,\nhelp: str,\n) -&gt; None:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n    Args:\n        func: A callable that executes the module's functionality.\n        add_args_func: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n    \"\"\"\nself.func = func\nself.add_args_func = add_args_func\nself.description = description\nself.help = help\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\nparser.add_argument(\n\"-c\",\n\"--input-config\",\nrequired=True,\nhelp=\"specify the config file name\",\n)\nparser.add_argument(\n\"-cp\",\n\"--custom-pipeline\",\naction=\"store_true\",\nhelp=\"infer a custom pipeline running order of modules from the config\",\n)\nfor module_name in VALID_MODULES:\ngroup = parser.add_argument_group(title=f\"{module_name} overrides\")\nMODULE_MAP[module_name].add_args_func(group, override=True)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> for each module in the pipeline.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` for each module in the pipeline.\"\"\"\nfor module_name in PIPELINE:\ngroup = parser.add_argument_group(title=module_name)\nMODULE_MAP[module_name].add_args_func(group)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>config</code> <code>ModuleConfig</code> <p>A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def add_subparser(\nsubparsers: argparse._SubParsersAction,\nname: str,\nconfig: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        config: A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.\n    Returns:\n        The newly created subparser.\n    \"\"\"\nparser = subparsers.add_parser(\nname=name,\ndescription=config.description,\nhelp=config.help,\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\nconfig.add_args_func(parser)\nparser.set_defaults(func=config.func)\nreturn parser\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\nprint(\"Running full pipeline...\")\nargs.modules_to_run = PIPELINE\nfor module_name in PIPELINE:\nargs = MODULE_MAP[module_name].func(args)\n</code></pre>"},{"location":"reference/cli/run/","title":"run","text":""},{"location":"reference/cli/run/#nhssynth.cli.run.run","title":"<code>run()</code>","text":"<p>CLI for preparing, training and evaluating a synthetic data generator.</p> Source code in <code>cli/run.py</code> <pre><code>def run() -&gt; None:\n\"\"\"CLI for preparing, training and evaluating a synthetic data generator.\"\"\"\nparser = argparse.ArgumentParser(\nprog=\"nhssynth\", description=\"CLI for preparing, training and evaluating a synthetic data generator.\"\n)\nadd_top_level_args(parser)\n# Below we instantiate one subparser for each module + one for running with a config file and one for\n# doing a full pipeline run with CLI-specified config\nsubparsers = parser.add_subparsers()\nall_subparsers = {\nname: add_subparser(subparsers, name, option_config) for name, option_config in MODULE_MAP.items()\n}\nargs = parser.parse_args()\n# Use get to return None when no function has been set, i.e. user made no running choice\nexecutor = vars(args).get(\"func\")\nif executor:\nargs.modules_to_run = get_modules_to_run(executor)\nexecutor(args)\nelif hasattr(args, \"input_config\"):\nargs = read_config(args, parser, all_subparsers)\nelse:\nparser.print_help()\n# Whenever either are specified, we want to dump the configuration to allow for this run to be replicated\nif args.save_config or args.save_config_path:\nwrite_config(args, all_subparsers)\nprint(\"Complete!\")\n</code></pre>"},{"location":"reference/common/","title":"common","text":""},{"location":"reference/common/#nhssynth.common.common","title":"<code>common</code>","text":""},{"location":"reference/common/#nhssynth.common.common.set_seed","title":"<code>set_seed(seed=None)</code>","text":"<p>Set the seed for numpy and torch.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>None | int</code> <p>The seed to set.</p> <code>None</code> Source code in <code>common/common.py</code> <pre><code>def set_seed(seed: None | int = None) -&gt; None:\n\"\"\"\n    Set the seed for numpy and torch.\n    Args:\n        seed: The seed to set.\n    \"\"\"\nif seed:\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n</code></pre>"},{"location":"reference/common/#nhssynth.common.dicts","title":"<code>dicts</code>","text":""},{"location":"reference/common/#nhssynth.common.dicts.filter_dict","title":"<code>filter_dict(d, filter_keys, include=False)</code>","text":"<p>Given a dictionary, return a new dictionary either including or excluding keys in a given <code>filter</code> set.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to filter.</p> required <code>filter_keys</code> <code>set | list</code> <p>A list or set of keys to either include or exclude.</p> required <code>include</code> <code>bool</code> <p>Determine whether to return a dictionary including or excluding keys in <code>filter</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A filtered dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n{'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n{'a': 1, 'b': 2}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def filter_dict(d: dict, filter_keys: set | list, include: bool = False) -&gt; dict:\n\"\"\"\n    Given a dictionary, return a new dictionary either including or excluding keys in a given `filter` set.\n    Args:\n        d: A dictionary to filter.\n        filter_keys: A list or set of keys to either include or exclude.\n        include: Determine whether to return a dictionary including or excluding keys in `filter`.\n    Returns:\n        A filtered dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n        {'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n        {'a': 1, 'b': 2}\n    \"\"\"\nif include:\nfiltered_keys = set(filter_keys) &amp; set(d.keys())\nelse:\nfiltered_keys = set(d.keys()) - set(filter_keys)\nreturn {k: v for k, v in d.items() if k in filtered_keys}\n</code></pre>"},{"location":"reference/common/#nhssynth.common.dicts.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary with possibly nested keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def flatten_dict(d: dict[str, Any]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n    Args:\n        d: A dictionary with possibly nested keys.\n    Returns:\n        A flattened dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\nitems = []\nfor k, v in d.items():\nif isinstance(v, dict):\nitems.extend(flatten_dict(v).items())\nelse:\nitems.append((k, v))\nreturn dict(items)\n</code></pre>"},{"location":"reference/common/#nhssynth.common.dicts.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def get_key_by_value(d: dict, value) -&gt; Any | None:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n    \"\"\"\nfor key, val in d.items():\nif val == value:\nreturn key\nreturn None\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io","title":"<code>io</code>","text":""},{"location":"reference/common/#nhssynth.common.io.check_exists","title":"<code>check_exists(fns, dir)</code>","text":"<p>Checks if the files in <code>fns</code> exist in <code>dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir_experiment</code> <p>The directory the files should exist in.</p> required <p>Exceptions:</p> Type Description <code>FileNotFoundError</code> <p>If any of the files in <code>fns</code> do not exist in <code>dir_experiment</code>.</p> Source code in <code>common/io.py</code> <pre><code>def check_exists(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Checks if the files in `fns` exist in `dir`.\n    Args:\n        fns: The list of files to check.\n        dir_experiment: The directory the files should exist in.\n    Raises:\n        FileNotFoundError: If any of the files in `fns` do not exist in `dir_experiment`.\n    \"\"\"\nfor fn in fns:\nif not (dir / fn).exists():\nraise FileNotFoundError(f\"File {fn} does not exist at {dir}.\")\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.consistent_ending","title":"<code>consistent_ending(fn, ending='.pkl')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, removes any existing ending and appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".pkl\".</p> <code>'.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>common/io.py</code> <pre><code>def consistent_ending(fn: str, ending: str = \".pkl\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, removes any existing ending and appends `ending`.\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".pkl\".\n    Returns:\n        The filename with the correct ending.\n    \"\"\"\npath_fn = Path(fn)\nif path_fn.suffix == ending:\nreturn fn\nelse:\nreturn str(path_fn.parent / path_fn.stem) + ending\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.experiment_io","title":"<code>experiment_io(experiment_name, dir_experiments='experiments')</code>","text":"<p>Create an experiment's directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <code>dir_experiments</code> <code>str</code> <p>The name of the directory containing all experiments.</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the experiment directory.</p> Source code in <code>common/io.py</code> <pre><code>def experiment_io(experiment_name: str, dir_experiments: str = \"experiments\") -&gt; str:\n\"\"\"\n    Create an experiment's directory and return the path.\n    Args:\n        experiment_name: The name of the experiment.\n        dir_experiments: The name of the directory containing all experiments.\n    Returns:\n        The path to the experiment directory.\n    \"\"\"\ndir_experiment = Path(dir_experiments) / experiment_name\ndir_experiment.mkdir(parents=True, exist_ok=True)\nreturn dir_experiment\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.potential_suffix","title":"<code>potential_suffix(fn, fn_base)</code>","text":"<p>Checks if <code>fn</code> is a suffix (starts with an underscore) to append to <code>fn_base</code>, or a filename in its own right.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename / potential suffix to append to <code>fn_base</code>.</p> required <code>fn_base</code> <code>str</code> <p>The name of the file the suffix would attach to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The appropriately processed <code>fn</code></p> Source code in <code>common/io.py</code> <pre><code>def potential_suffix(fn: str, fn_base: str) -&gt; str:\n\"\"\"\n    Checks if `fn` is a suffix (starts with an underscore) to append to `fn_base`, or a filename in its own right.\n    Args:\n        fn: The filename / potential suffix to append to `fn_base`.\n        fn_base: The name of the file the suffix would attach to.\n    Returns:\n        The appropriately processed `fn`\n    \"\"\"\nfn_base = Path(fn_base).stem\nif fn[0] == \"_\":\nreturn fn_base + fn\nelse:\nreturn fn\n</code></pre>"},{"location":"reference/common/#nhssynth.common.io.warn_if_path_supplied","title":"<code>warn_if_path_supplied(fns, dir)</code>","text":"<p>Warns if the files in <code>fns</code> include directory separators.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir</code> <code>Path</code> <p>The directory the files should exist in.</p> required <p>!!! warnings     Raises a UserWarning when the path to any of the files in <code>fns</code> includes directory separators, as this may not work as intended.</p> Source code in <code>common/io.py</code> <pre><code>def warn_if_path_supplied(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Warns if the files in `fns` include directory separators.\n    Args:\n        fns: The list of files to check.\n        dir: The directory the files should exist in.\n    Warnings:\n        Raises a UserWarning when the path to any of the files in `fns` includes directory separators, as this may not work as intended.\n    \"\"\"\nfor fn in fns:\nif \"/\" in fn:\nwarnings.warn(\nf\"Using the path supplied appended to {dir}, i.e. attempting to read data from {dir / fn}\",\nUserWarning,\n)\n</code></pre>"},{"location":"reference/common/common/","title":"common","text":""},{"location":"reference/common/common/#nhssynth.common.common.set_seed","title":"<code>set_seed(seed=None)</code>","text":"<p>Set the seed for numpy and torch.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>None | int</code> <p>The seed to set.</p> <code>None</code> Source code in <code>common/common.py</code> <pre><code>def set_seed(seed: None | int = None) -&gt; None:\n\"\"\"\n    Set the seed for numpy and torch.\n    Args:\n        seed: The seed to set.\n    \"\"\"\nif seed:\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n</code></pre>"},{"location":"reference/common/constants/","title":"constants","text":""},{"location":"reference/common/dicts/","title":"dicts","text":""},{"location":"reference/common/dicts/#nhssynth.common.dicts.filter_dict","title":"<code>filter_dict(d, filter_keys, include=False)</code>","text":"<p>Given a dictionary, return a new dictionary either including or excluding keys in a given <code>filter</code> set.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to filter.</p> required <code>filter_keys</code> <code>set | list</code> <p>A list or set of keys to either include or exclude.</p> required <code>include</code> <code>bool</code> <p>Determine whether to return a dictionary including or excluding keys in <code>filter</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A filtered dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n{'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n{'a': 1, 'b': 2}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def filter_dict(d: dict, filter_keys: set | list, include: bool = False) -&gt; dict:\n\"\"\"\n    Given a dictionary, return a new dictionary either including or excluding keys in a given `filter` set.\n    Args:\n        d: A dictionary to filter.\n        filter_keys: A list or set of keys to either include or exclude.\n        include: Determine whether to return a dictionary including or excluding keys in `filter`.\n    Returns:\n        A filtered dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n        {'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n        {'a': 1, 'b': 2}\n    \"\"\"\nif include:\nfiltered_keys = set(filter_keys) &amp; set(d.keys())\nelse:\nfiltered_keys = set(d.keys()) - set(filter_keys)\nreturn {k: v for k, v in d.items() if k in filtered_keys}\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary with possibly nested keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def flatten_dict(d: dict[str, Any]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n    Args:\n        d: A dictionary with possibly nested keys.\n    Returns:\n        A flattened dictionary.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\nitems = []\nfor k, v in d.items():\nif isinstance(v, dict):\nitems.extend(flatten_dict(v).items())\nelse:\nitems.append((k, v))\nreturn dict(items)\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>common/dicts.py</code> <pre><code>def get_key_by_value(d: dict, value) -&gt; Any | None:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n    \"\"\"\nfor key, val in d.items():\nif val == value:\nreturn key\nreturn None\n</code></pre>"},{"location":"reference/common/io/","title":"io","text":""},{"location":"reference/common/io/#nhssynth.common.io.check_exists","title":"<code>check_exists(fns, dir)</code>","text":"<p>Checks if the files in <code>fns</code> exist in <code>dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir_experiment</code> <p>The directory the files should exist in.</p> required <p>Exceptions:</p> Type Description <code>FileNotFoundError</code> <p>If any of the files in <code>fns</code> do not exist in <code>dir_experiment</code>.</p> Source code in <code>common/io.py</code> <pre><code>def check_exists(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Checks if the files in `fns` exist in `dir`.\n    Args:\n        fns: The list of files to check.\n        dir_experiment: The directory the files should exist in.\n    Raises:\n        FileNotFoundError: If any of the files in `fns` do not exist in `dir_experiment`.\n    \"\"\"\nfor fn in fns:\nif not (dir / fn).exists():\nraise FileNotFoundError(f\"File {fn} does not exist at {dir}.\")\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.consistent_ending","title":"<code>consistent_ending(fn, ending='.pkl')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, removes any existing ending and appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".pkl\".</p> <code>'.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>common/io.py</code> <pre><code>def consistent_ending(fn: str, ending: str = \".pkl\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, removes any existing ending and appends `ending`.\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".pkl\".\n    Returns:\n        The filename with the correct ending.\n    \"\"\"\npath_fn = Path(fn)\nif path_fn.suffix == ending:\nreturn fn\nelse:\nreturn str(path_fn.parent / path_fn.stem) + ending\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.experiment_io","title":"<code>experiment_io(experiment_name, dir_experiments='experiments')</code>","text":"<p>Create an experiment's directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <code>dir_experiments</code> <code>str</code> <p>The name of the directory containing all experiments.</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the experiment directory.</p> Source code in <code>common/io.py</code> <pre><code>def experiment_io(experiment_name: str, dir_experiments: str = \"experiments\") -&gt; str:\n\"\"\"\n    Create an experiment's directory and return the path.\n    Args:\n        experiment_name: The name of the experiment.\n        dir_experiments: The name of the directory containing all experiments.\n    Returns:\n        The path to the experiment directory.\n    \"\"\"\ndir_experiment = Path(dir_experiments) / experiment_name\ndir_experiment.mkdir(parents=True, exist_ok=True)\nreturn dir_experiment\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.potential_suffix","title":"<code>potential_suffix(fn, fn_base)</code>","text":"<p>Checks if <code>fn</code> is a suffix (starts with an underscore) to append to <code>fn_base</code>, or a filename in its own right.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename / potential suffix to append to <code>fn_base</code>.</p> required <code>fn_base</code> <code>str</code> <p>The name of the file the suffix would attach to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The appropriately processed <code>fn</code></p> Source code in <code>common/io.py</code> <pre><code>def potential_suffix(fn: str, fn_base: str) -&gt; str:\n\"\"\"\n    Checks if `fn` is a suffix (starts with an underscore) to append to `fn_base`, or a filename in its own right.\n    Args:\n        fn: The filename / potential suffix to append to `fn_base`.\n        fn_base: The name of the file the suffix would attach to.\n    Returns:\n        The appropriately processed `fn`\n    \"\"\"\nfn_base = Path(fn_base).stem\nif fn[0] == \"_\":\nreturn fn_base + fn\nelse:\nreturn fn\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.warn_if_path_supplied","title":"<code>warn_if_path_supplied(fns, dir)</code>","text":"<p>Warns if the files in <code>fns</code> include directory separators.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list</code> <p>The list of files to check.</p> required <code>dir</code> <code>Path</code> <p>The directory the files should exist in.</p> required <p>!!! warnings     Raises a UserWarning when the path to any of the files in <code>fns</code> includes directory separators, as this may not work as intended.</p> Source code in <code>common/io.py</code> <pre><code>def warn_if_path_supplied(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Warns if the files in `fns` include directory separators.\n    Args:\n        fns: The list of files to check.\n        dir: The directory the files should exist in.\n    Warnings:\n        Raises a UserWarning when the path to any of the files in `fns` includes directory separators, as this may not work as intended.\n    \"\"\"\nfor fn in fns:\nif \"/\" in fn:\nwarnings.warn(\nf\"Using the path supplied appended to {dir}, i.e. attempting to read data from {dir / fn}\",\nUserWarning,\n)\n</code></pre>"},{"location":"reference/modules/","title":"modules","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader","title":"<code>dataloader</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.io","title":"<code>io</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_input, fn_metadata, dir_data)</code>","text":"<p>Formats the input filenames and directory for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_input</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_input</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_input_paths(\nfn_input: str,\nfn_metadata: str,\ndir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input filenames and directory for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_input`.\n        dir_data: The directory that should contain both of the above.\n    Returns:\n        A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).\n    Warnings:\n        Raises a UserWarning when the path to `fn_input` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_input, fn_metadata = consistent_ending(fn_input, \".csv\"), consistent_ending(fn_metadata, \".yaml\")\ndir_data = Path(dir_data)\nfn_metadata = potential_suffix(fn_metadata, fn_input)\nwarn_if_path_supplied([fn_input, fn_metadata], dir_data)\ncheck_exists([fn_input], dir_data)\nreturn dir_data, fn_input, fn_metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_input, fn_output, fn_transformer, dir_experiment)</code>","text":"<p>Formats the output filenames for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_output</code> <code>str</code> <p>The output data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_transformer</code> <code>str</code> <p>The transformer filename/suffix to append to <code>fn_input</code>.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the formatted output filenames.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_output</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_transformer</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_output_paths(\nfn_input: str,\nfn_output: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; tuple[str, str]:\n\"\"\"\n    Formats the output filenames for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_output: The output data filename/suffix to append to `fn_input`.\n        fn_transformer: The transformer filename/suffix to append to `fn_input`.\n        dir_experiment: The experiment directory to write the outputs to.\n    Returns:\n        A tuple containing the formatted output filenames.\n    Warnings:\n        Raises a UserWarning when the path to `fn_output` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_transformer` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_output, fn_transformer = consistent_ending(fn_output), consistent_ending(fn_transformer)\nfn_output, fn_transformer = potential_suffix(fn_output, fn_input), potential_suffix(fn_transformer, fn_input)\nwarn_if_path_supplied([fn_output, fn_transformer], dir_experiment)\nreturn fn_output, fn_transformer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(transformed_input, metatransformer, fn_output, fn_transformer, dir_experiment)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>transformed_input</code> <code>DataFrame</code> <p>The prepared version of the input data.</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its prepared state.</p> required <code>fn_output</code> <code>str</code> <p>The filename to dump the prepared data to.</p> required <code>fn_transformer</code> <code>str</code> <p>The filename to dump the metatransformer to.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required Source code in <code>modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\ntransformed_input: pd.DataFrame,\nmetatransformer: MetaTransformer,\nfn_output: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n    Args:\n        transformed_input: The prepared version of the input data.\n        metatransformer: The metatransformer used to transform the data into its prepared state.\n        fn_output: The filename to dump the prepared data to.\n        fn_transformer: The filename to dump the metatransformer to.\n        dir_experiment: The experiment directory to write the outputs to.\n    \"\"\"\ntransformed_input.to_pickle(dir_experiment / fn_output)\ntransformed_input.to_csv(dir_experiment / (fn_output[:-3] + \"csv\"), index=False)\nwith open(dir_experiment / fn_transformer, \"wb\") as f:\npickle.dump(metatransformer, f)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata","title":"<code>metadata</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the <code>metadata</code> correspond to valid columns in the <code>data</code>. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the passed <code>data</code>.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns that are in metadata are not present in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict[str, Any]], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the `metadata` correspond to valid columns in the `data`.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n    Args:\n        metadata: A dictionary containing metadata for the columns in the passed `data`.\n        data: The DataFrame to check against the metadata.\n    Raises:\n        AssertionError: If any columns that *are* in metadata are *not* present in the `data`.\n    \"\"\"\nassert all([k in data.columns for k in metadata.keys()])\nmetadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # one entry for each column that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # one entry for each column that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\nc_index = 1\ncolumn_types = {}\nt_index = 1\ntransformers = {}\nfor cn, cd in metadata.items():\nif cd not in column_types.values():\ncolumn_types[c_index] = cd.copy()\nmetadata[cn] = column_types[c_index]\nc_index += 1\nelse:\ncix = get_key_by_value(column_types, cd)\nmetadata[cn] = column_types[cix]\nif cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\ntransformers[t_index] = cd[\"transformer\"].copy()\nmetadata[cn][\"transformer\"] = transformers[t_index]\nt_index += 1\nelif cd[\"transformer\"]:\ntix = get_key_by_value(transformers, cd[\"transformer\"])\nmetadata[cn][\"transformer\"] = transformers[tix]\nreturn {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame in question.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n    Args:\n        data: The DataFrame in question.\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\nreturn {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data`.\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n    Returns:\n        A metadata dictionary containing information about the columns in the `data`.\n    \"\"\"\nif in_path.exists():\nwith open(in_path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out expanded alias/anchor groups\nmetadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\ncheck_metadata_columns(metadata, data)\nelse:\nmetadata = create_empty_metadata(data)\nreturn metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, metadata, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be written.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\nout_path: pathlib.Path,\nmetadata: dict[str, dict[str, Any]],\ncollapse_yaml: bool,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        metadata: The metadata dictionary to be written.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\nif collapse_yaml:\nmetadata = collapse(metadata)\nwith open(out_path, \"w\") as yaml_file:\nyaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer","title":"<code>metatransformer</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code> MetaTransformer        </code>","text":"<p>A metatransformer object that can wrap either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back to the original format of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>A dictionary mapping column names to their metadata.</p> required <code>sdv_workflow</code> <p>A flag indicating whether or not to use the SDV workflow.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> required <code>synthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use within the SDV workflow.</p> required <p>Once instantiated via <code>mt = MetaTransformer(&lt;parameters&gt;)</code>, the following attributes will be available:</p> <p>Attributes:</p> Name Type Description <code>sdv_workflow</code> <code>bool</code> <p>A flag indicating whether or not to use the SDV workflow.</p> <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> <code>Synthesizer</code> <code>BaseSingleTableSynthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use within the SDV workflow.</p> <code>dtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).</p> <code>sdtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to the appropriate SDV-specific data type.</p> <code>transformers</code> <code>dict[str, BaseTransformer | None]</code> <p>A dictionary mapping each column to their assigned (if any) transformer.</p> <p>After preparing some data with the MetaTransformer, i.e. <code>prepared_data = mt.apply(data)</code>, the following attributes and methods will be available:</p> <p>Attributes:</p> Name Type Description <code>metatransformer</code> <code>HyperTransformer | self.Synthesizer</code> <p>An instanatiated <code>HyperTransformer</code> or <code>self.Synthesizer</code> object, ready to use on data.</p> <code>assembled_metadata</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the formatted and complete metadata for the MetaTransformer.</p> <code>onehots</code> <code>list[list[int]]</code> <p>The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).</p> <code>singles</code> <code>list[int]</code> <p>The indices of non-one-hotted columns.</p> <p>Methods:</p> <ul> <li><code>get_assembled_metadata()</code>: Returns the assembled metadata.</li> <li><code>get_onehots_and_singles()</code>: Returns the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</li> <li><code>inverse_apply(synthetic_data)</code>: Apply the inverse of the MetaTransformer to the given data.</li> </ul> <p>Note that <code>mt.apply</code> is a helper function that runs <code>mt.apply_dtypes</code>, <code>mt.instaniate</code>, <code>mt.assemble</code>, <code>mt.prepare</code> and finally <code>mt.count_onehots_and_singles</code> in sequence on a given raw dataset. Along the way it assigns the attributes listed above.</p> <p>This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    A metatransformer object that can wrap either a [`HyperTransformer`](https://docs.sdv.dev/rdt/usage/hypertransformer) from RDT or a\n    [`BaseSingleTableSynthesizer`](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers) from SDV. The metatransformer is\n    responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back\n    to the original format of the input data.\n    Args:\n        metadata: A dictionary mapping column names to their metadata.\n        sdv_workflow: A flag indicating whether or not to use the SDV workflow.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        synthesizer: The `BaseSingleTableSynthesizer` class to use within the SDV workflow.\n    Once instantiated via `mt = MetaTransformer(&lt;parameters&gt;)`, the following attributes will be available:\n    Attributes:\n        sdv_workflow: A flag indicating whether or not to use the SDV workflow.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        Synthesizer: The `BaseSingleTableSynthesizer` class to use within the SDV workflow.\n        dtypes: A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).\n        sdtypes: A dictionary mapping each column to the appropriate SDV-specific data type.\n        transformers: A dictionary mapping each column to their assigned (if any) transformer.\n    After preparing some data with the MetaTransformer, i.e. `prepared_data = mt.apply(data)`, the following attributes and methods will be available:\n    Attributes:\n        metatransformer (HyperTransformer | self.Synthesizer): An instanatiated `HyperTransformer` or `self.Synthesizer` object, ready to use on data.\n        assembled_metadata (dict[str, dict[str, Any]]): A dictionary containing the formatted and complete metadata for the MetaTransformer.\n        onehots (list[list[int]]): The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).\n        singles (list[int]): The indices of non-one-hotted columns.\n    **Methods:**\n    - `get_assembled_metadata()`: Returns the assembled metadata.\n    - `get_onehots_and_singles()`: Returns the values of the MetaTransformer's `onehots` and `singles` attributes.\n    - `inverse_apply(synthetic_data)`: Apply the inverse of the MetaTransformer to the given data.\n    Note that `mt.apply` is a helper function that runs `mt.apply_dtypes`, `mt.instaniate`, `mt.assemble`, `mt.prepare` and finally\n    `mt.count_onehots_and_singles` in sequence on a given raw dataset. Along the way it assigns the attributes listed above.\n    This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.\n    \"\"\"\ndef __init__(self, metadata, sdv_workflow, allow_null_transformers, synthesizer) -&gt; None:\nself.sdv_workflow: bool = sdv_workflow\nself.allow_null_transformers: bool = allow_null_transformers\nself.Synthesizer: BaseSingleTableSynthesizer = SDV_SYNTHESIZER_CHOICES[synthesizer]\n# TODO think about whether these belong here\nself.dtypes: dict[str, dict[str, Any]] = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\nself.sdtypes: dict[str, dict[str, Any]] = {\ncn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()\n}\nself.transformers: dict[str, BaseTransformer | None] = {cn: get_transformer(cd) for cn, cd in metadata.items()}\ndef apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n        Args:\n            data: The raw input DataFrame.\n        Returns:\n            The data with the dtypes applied.\n        \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\ndef _instantiate_ohe_component_transformers(\nself, transformers: dict[str, BaseTransformer | None]\n) -&gt; dict[str, BaseTransformer]:\n\"\"\"\n        Instantiates a OneHotEncoder for each resulting `*.component` column that arises from a ClusterBasedNormalizer.\n        Args:\n            transformers: A dictionary mapping column names to their assigned transformers.\n        Returns:\n            A dictionary mapping each `*.component` column to a OneHotEncoder.\n        \"\"\"\nreturn {\nf\"{cn}.component\": OneHotEncoder()\nfor cn, transformer in transformers.items()\nif transformer.get_name() == \"ClusterBasedNormalizer\"\n}\ndef _instantiate_synthesizer(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n        Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete and `self.allow_null_transformers` is `False`.\n        \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\ndef _instantiate_hypertransformer(self, data: pd.DataFrame) -&gt; HyperTransformer:\n\"\"\"\n        Instantiates a `HyperTransformer` object from the metadata and given data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `HyperTransformer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete.\n        \"\"\"\nht = HyperTransformer()\nif all(self.sdtypes.values()) and (all(self.transformers.values()) or self.allow_null_transformers):\nht.set_config(\nconfig={\n\"sdtypes\": {k: v[\"sdtype\"] for k, v in self.sdtypes.items()},\n\"transformers\": self.transformers,\n}\n)\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in self.sdtypes.items() if not v])) if not all(self.sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in self.transformers.items() if not v])) if not all(self.transformers.values()) and not self.allow_null_transformers else ''} automatically...\",\nUserWarning,\n)\nht.detect_initial_config(data)\nht.update_sdtypes({k: v[\"sdtype\"] for k, v in self.sdtypes.items() if v})\nht.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(ht.get_config()[\"transformers\"])\nreturn ht, component_transformer\ndef instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n        Calls the appropriate instantiation method based on the value of `self.sdv_workflow`.\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` or `HyperTransformer` object.\n        \"\"\"\nif self.sdv_workflow:\nreturn self._instantiate_synthesizer(data)\nelse:\nreturn self._instantiate_hypertransformer(data)\ndef _get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\nreturn self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\ndef assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of `self,sdv_workflow`\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nsdmetadata = self.metatransformer.metadata\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in sdmetadata.columns.items()\n}\nelse:\nconfig = self.metatransformer.get_config()\nreturn {\ncn: {\n\"sdtype\": cd,\n\"transformer\": make_transformer_dict(config[\"transformers\"][cn])\nif config[\"transformers\"][cn]\nelse None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in config[\"sdtypes\"].items()\n}\ndef prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the data by processing it via the metatransformer.\n        Args:\n            data: The data to fit and apply the transformer to.\n        Returns:\n            The transformed data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nprepared_data = self.metatransformer.preprocess(data)\nelse:\nprepared_data = self.metatransformer.fit_transform(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\ndef count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n        Also records the indices of non-one-hotted columns in a separate list.\n        Args:\n            data: The data to extract column indices from.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn).columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelse:\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\ndef apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n        Args:\n            data: The DataFrame to transform.\n        Returns:\n            The transformed data.\n        \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn prepared_data\ndef get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Returns the assembled metadata for the transformer.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not self.assembled_metadata:\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\ndef get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        Raises:\n            ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n        \"\"\"\nif not self.onehots or not self.singles:\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\ndef inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n        Args:\n            data: The transformed data.\n        Returns:\n            The original data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nif self.sdv_workflow:\nreturn self.metatransformer._data_processor.reverse_transform(data)\nelse:\nreturn self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply(self, data)</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n    Args:\n        data: The DataFrame to transform.\n    Returns:\n        The transformed data.\n    \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(self, data)</code>","text":"<p>Applies dtypes from the metadata to <code>data</code> and infers missing dtypes by reading pandas defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the dtypes applied.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n    Args:\n        data: The raw input DataFrame.\n    Returns:\n        The data with the dtypes applied.\n    \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.assemble","title":"<code>assemble(self)</code>","text":"<p>Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of <code>self,sdv_workflow</code></p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of `self,sdv_workflow`\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nsdmetadata = self.metatransformer.metadata\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in sdmetadata.columns.items()\n}\nelse:\nconfig = self.metatransformer.get_config()\nreturn {\ncn: {\n\"sdtype\": cd,\n\"transformer\": make_transformer_dict(config[\"transformers\"][cn])\nif config[\"transformers\"][cn]\nelse None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in config[\"sdtypes\"].items()\n}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.count_onehots_and_singles","title":"<code>count_onehots_and_singles(self, data)</code>","text":"<p>Uses the assembled metadata to identify and record the indices of one-hotted column groups. Also records the indices of non-one-hotted columns in a separate list.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to extract column indices from.</p> required <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n    Also records the indices of non-one-hotted columns in a separate list.\n    Args:\n        data: The data to extract column indices from.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn).columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelse:\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_assembled_metadata","title":"<code>get_assembled_metadata(self)</code>","text":"<p>Returns the assembled metadata for the transformer.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Returns the assembled metadata for the transformer.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not self.assembled_metadata:\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_onehots_and_singles","title":"<code>get_onehots_and_singles(self)</code>","text":"<p>Get the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</p> <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>self.onehots</code> and <code>self.singles</code> have yet to be counted.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    Raises:\n        ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n    \"\"\"\nif not self.onehots or not self.singles:\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate","title":"<code>instantiate(self, data)</code>","text":"<p>Calls the appropriate instantiation method based on the value of <code>self.sdv_workflow</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Type Description <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A fully instantiated <code>self.Synthesizer</code> or <code>HyperTransformer</code> object.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n    Calls the appropriate instantiation method based on the value of `self.sdv_workflow`.\n    Args:\n        data: The input DataFrame.\n    Returns:\n        A fully instantiated `self.Synthesizer` or `HyperTransformer` object.\n    \"\"\"\nif self.sdv_workflow:\nreturn self._instantiate_synthesizer(data)\nelse:\nreturn self._instantiate_hypertransformer(data)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(self, data)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The transformed data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n    Args:\n        data: The transformed data.\n    Returns:\n        The original data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nif self.sdv_workflow:\nreturn self.metatransformer._data_processor.reverse_transform(data)\nelse:\nreturn self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.prepare","title":"<code>prepare(self, data)</code>","text":"<p>Prepares the data by processing it via the metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to fit and apply the transformer to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the data by processing it via the metatransformer.\n    Args:\n        data: The data to fit and apply the transformer to.\n    Returns:\n        The transformed data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nprepared_data = self.metatransformer.preprocess(data)\nelse:\nprepared_data = self.metatransformer.fit_transform(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object constructed from data in the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>rdt.transformers.base.BaseTransformer | None</code> <p>An instantiated <code>BaseTransformer</code> if the dictionary contains valid transformer data, else None.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_transformer(d: dict) -&gt; BaseTransformer | None:\n\"\"\"\n    Return a callable transformer object constructed from data in the given dictionary.\n    Args:\n        d: A dictionary containing the transformer data.\n    Returns:\n        An instantiated `BaseTransformer` if the dictionary contains valid transformer data, else None.\n    \"\"\"\ntransformer_data = d.get(\"transformer\", None)\nif isinstance(transformer_data, dict) and \"name\" in transformer_data:\n# Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\ntransformer_data = transformer_data.copy()\ntransformer_name = transformer_data.pop(\"name\")\nreturn eval(transformer_name)(**transformer_data)\nelif isinstance(transformer_data, str):\nreturn eval(transformer_data)()\nelse:\nreturn None\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.metatransformer.make_transformer_dict","title":"<code>make_transformer_dict(transformer)</code>","text":"<p>Deconstruct a <code>transformer</code> into a dictionary of config.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>BaseTransformer</code> <p>A BaseTransformer object from RDT (SDV).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the transformer's name and arguments.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def make_transformer_dict(transformer: BaseTransformer) -&gt; dict[str, Any]:\n\"\"\"\n    Deconstruct a `transformer` into a dictionary of config.\n    Args:\n        transformer: A BaseTransformer object from RDT (SDV).\n    Returns:\n        A dictionary containing the transformer's name and arguments.\n    \"\"\"\nreturn {\n\"name\": type(transformer).__name__,\n**filter_dict(\ntransformer.__dict__,\n{\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n),\n}\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.dataloader.run","title":"<code>run</code>","text":""},{"location":"reference/modules/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforming the input data and writing the output and transformer used to file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforming the input data and writing the output and transformer used to file.\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n    \"\"\"\nprint(\"Running dataloader module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\ndir_input, fn_input_data, fn_metadata = check_input_paths(args.input, args.metadata, args.data_dir)\n# Load the dataset and accompanying metadata\ninput = pd.read_csv(dir_input / fn_input_data, index_col=args.index_col)\nmetadata = load_metadata(dir_input / fn_metadata, input)\nmt = MetaTransformer(metadata, args.sdv_workflow, args.allow_null_transformers, args.synthesizer)\ntransformed_input = mt.apply(input)\n# Output the metadata corresponding to `transformed_input`, for reproducibility\nif not args.discard_metadata:\noutput_metadata(dir_experiment / fn_metadata, mt.get_assembled_metadata(), args.collapse_yaml)\n# Write the transformed input to the appropriate file\nif not args.modules_to_run or args.modules_to_run == [\"dataloader\"] or args.write_all:\nfn_output_data, fn_transformer = check_output_paths(\nfn_input_data, args.output, args.metatransformer, dir_experiment\n)\nwrite_data_outputs(transformed_input, mt, fn_output_data, fn_transformer, dir_experiment)\nif \"model\" in args.modules_to_run:\nargs.dataloader_output = {\n\"fn_base\": fn_input_data,\n\"data\": transformed_input,\n\"metatransformer\": mt,\n}\nreturn args\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model","title":"<code>model</code>  <code>special</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.DPVAE","title":"<code>DPVAE</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\ndef __init__(\nself,\nlatent_dim,\nonehots=[[]],\nsingles=[],\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = len(singles) + sum([len(x) for x in onehots])\nself.singles = singles\nself.onehots = onehots\nself.net = nn.Sequential(\nnn.Linear(latent_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nlatent_dim,\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = 2 * latent_dim\nself.latent_dim = latent_dim\nself.net = nn.Sequential(\nnn.Linear(input_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\ndef __init__(self, num_singles):\nsuper().__init__()\nself.output_logsigma_fn = nn.Linear(num_singles, num_singles, bias=True)\ntorch.nn.init.zeros_(self.output_logsigma_fn.weight)\ntorch.nn.init.zeros_(self.output_logsigma_fn.bias)\nself.output_logsigma_fn.weight.requires_grad = False\ndef forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder.to(encoder.device)\nself.decoder = decoder.to(decoder.device)\nself.device = encoder.device\nself.onehots = self.decoder.onehots\nself.singles = self.decoder.singles\nself.noiser = Noiser(len(self.singles)).to(decoder.device)\ndef reconstruct(self, X):\nmu_z, logsigma_z = self.encoder(X)\nx_recon = self.decoder(mu_z)\nreturn x_recon\ndef generate(self, N):\nz_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\nx_gen = self.decoder(z_samples)\nx_gen_ = torch.ones_like(x_gen, device=self.device)\nfor cat_idxs in self.onehots:\nx_gen_[:, cat_idxs] = torch.distributions.one_hot_categorical.OneHotCategorical(\nlogits=x_gen[:, cat_idxs]\n).sample()\nx_gen_[:, self.singles] = x_gen[:, self.singles] + torch.exp(\nself.noiser(x_gen[:, self.singles])\n) * torch.randn_like(x_gen[:, self.singles])\nif torch.cuda.is_available():\nx_gen_ = x_gen_.cpu()\nreturn x_gen_.detach()\ndef loss(self, X):\nmu_z, logsigma_z = self.encoder(X)\np = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\nq = Normal(mu_z, torch.exp(logsigma_z))\nkld = torch.sum(torch.distributions.kl_divergence(q, p))\ns = torch.randn_like(mu_z)\nz_samples = mu_z + s * torch.exp(logsigma_z)\nx_recon = self.decoder(z_samples)\ncategoric_loglik = 0\nif len(self.onehots):\nfor cat_idxs in self.onehots:\ncategoric_loglik += -torch.nn.functional.cross_entropy(\nx_recon[:, cat_idxs],\ntorch.max(X[:, cat_idxs], 1)[1],\n).sum()\ngauss_loglik = 0\nif len(self.singles):\ngauss_loglik = (\nNormal(\nloc=x_recon[:, self.singles],\nscale=torch.exp(self.noiser(x_recon[:, self.singles])),\n)\n.log_prob(X[:, self.singles])\n.sum()\n)\nreconstruction_loss = -(categoric_loglik + gauss_loglik)\nelbo = kld + reconstruction_loss\nreturn (elbo, reconstruction_loss, kld, categoric_loglik, gauss_loglik)\ndef train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\n# EARLY STOPPING #\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nlog_elbo = []\nlog_reconstruct = []\nlog_divergence = []\nlog_cat_loss = []\nlog_num_loss = []\nstats_bar_1 = tqdm(total=0, desc=\"\", position=0, bar_format=\"{desc}\", leave=True)\nstats_bar_2 = tqdm(total=0, desc=\"\", position=1, bar_format=\"{desc}\", leave=True)\nstats_bar_3 = tqdm(total=0, desc=\"\", position=2, bar_format=\"{desc}\", leave=True)\nstats_bar_4 = tqdm(total=0, desc=\"\", position=3, bar_format=\"{desc}\", leave=True)\nstats_bar_5 = tqdm(total=0, desc=\"\", position=4, bar_format=\"{desc}\", leave=True)\nposition = 5\nif self.privacy_engine is not None:\nepsilon = []\nstats_bar_6 = tqdm(total=0, desc=\"\", position=5, bar_format=\"{desc}\", leave=True)\nposition += 1\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=position, leave=False):\nelbo_e = 0.0\nkld_e = 0.0\nreconstruction_e = 0.0\ncategorical_e = 0.0\nnumerical_e = 0.0\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=position + 1, leave=False):\nself.optimizer.zero_grad()\n(\nelbo,\nreconstruction_loss,\nkld,\ncategorical_loss,\nnumerical_loss,\n) = self.loss(Y_subset.to(self.encoder.device))\nelbo.backward()\nself.optimizer.step()\nelbo_e += elbo.item()\nkld_e += kld.item()\nreconstruction_e += reconstruction_loss.item()\ncategorical_e += categorical_loss.item()\nnumerical_e += numerical_loss.item()\nstats_bar_1.set_description_str(f\"ELBO: \\t\\t\\t{elbo_e:.2f}\")\nstats_bar_2.set_description_str(f\"KLD: \\t\\t\\t{kld_e:.2f}\")\nstats_bar_3.set_description_str(f\"Reconstruction Loss: \\t{reconstruction_e:.2f}\")\nstats_bar_4.set_description_str(f\"Categorical Loss: \\t{categorical_e:.2f}\")\nstats_bar_5.set_description_str(f\"Numerical Loss: \\t{numerical_e:.2f}\")\nif self.privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar_6.set_description_str(f\"Epsilon and Best Alpha: \\t{epsilon_e[0]:.2f}\\t{epsilon_e[1]:.2f}\")\nepsilon.append(epsilon_e)\nlog_elbo.append(elbo_e)\nlog_reconstruct.append(reconstruction_e)\nlog_divergence.append(kld_e)\nlog_cat_loss.append(categorical_e)\nlog_num_loss.append(numerical_e)\nif epoch == 0:\nmin_elbo = elbo_e\nif elbo_e &lt; (min_elbo - delta):\nmin_elbo = elbo_e\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nstats_bar_1.close()\nstats_bar_2.close()\nstats_bar_3.close()\nstats_bar_4.close()\nstats_bar_5.close()\nif privacy_engine is not None:\nstats_bar_6.close()\nreturn (\nnum_epochs,\nlog_elbo,\nlog_reconstruct,\nlog_divergence,\nlog_cat_loss,\nlog_num_loss,\n)\ndef get_privacy_spent(self, delta):\nif hasattr(self, \"privacy_engine\"):\nreturn self.privacy_engine.get_privacy_spent(delta)\nelse:\nprint(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n)\ndef save(self, filename):\ntorch.save(self.state_dict(), filename)\ndef load(self, filename):\nself.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, num_epochs, privacy_engine=None, patience=5, delta=10)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\n# EARLY STOPPING #\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nlog_elbo = []\nlog_reconstruct = []\nlog_divergence = []\nlog_cat_loss = []\nlog_num_loss = []\nstats_bar_1 = tqdm(total=0, desc=\"\", position=0, bar_format=\"{desc}\", leave=True)\nstats_bar_2 = tqdm(total=0, desc=\"\", position=1, bar_format=\"{desc}\", leave=True)\nstats_bar_3 = tqdm(total=0, desc=\"\", position=2, bar_format=\"{desc}\", leave=True)\nstats_bar_4 = tqdm(total=0, desc=\"\", position=3, bar_format=\"{desc}\", leave=True)\nstats_bar_5 = tqdm(total=0, desc=\"\", position=4, bar_format=\"{desc}\", leave=True)\nposition = 5\nif self.privacy_engine is not None:\nepsilon = []\nstats_bar_6 = tqdm(total=0, desc=\"\", position=5, bar_format=\"{desc}\", leave=True)\nposition += 1\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=position, leave=False):\nelbo_e = 0.0\nkld_e = 0.0\nreconstruction_e = 0.0\ncategorical_e = 0.0\nnumerical_e = 0.0\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=position + 1, leave=False):\nself.optimizer.zero_grad()\n(\nelbo,\nreconstruction_loss,\nkld,\ncategorical_loss,\nnumerical_loss,\n) = self.loss(Y_subset.to(self.encoder.device))\nelbo.backward()\nself.optimizer.step()\nelbo_e += elbo.item()\nkld_e += kld.item()\nreconstruction_e += reconstruction_loss.item()\ncategorical_e += categorical_loss.item()\nnumerical_e += numerical_loss.item()\nstats_bar_1.set_description_str(f\"ELBO: \\t\\t\\t{elbo_e:.2f}\")\nstats_bar_2.set_description_str(f\"KLD: \\t\\t\\t{kld_e:.2f}\")\nstats_bar_3.set_description_str(f\"Reconstruction Loss: \\t{reconstruction_e:.2f}\")\nstats_bar_4.set_description_str(f\"Categorical Loss: \\t{categorical_e:.2f}\")\nstats_bar_5.set_description_str(f\"Numerical Loss: \\t{numerical_e:.2f}\")\nif self.privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar_6.set_description_str(f\"Epsilon and Best Alpha: \\t{epsilon_e[0]:.2f}\\t{epsilon_e[1]:.2f}\")\nepsilon.append(epsilon_e)\nlog_elbo.append(elbo_e)\nlog_reconstruct.append(reconstruction_e)\nlog_divergence.append(kld_e)\nlog_cat_loss.append(categorical_e)\nlog_num_loss.append(numerical_e)\nif epoch == 0:\nmin_elbo = elbo_e\nif elbo_e &lt; (min_elbo - delta):\nmin_elbo = elbo_e\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nstats_bar_1.close()\nstats_bar_2.close()\nstats_bar_3.close()\nstats_bar_4.close()\nstats_bar_5.close()\nif privacy_engine is not None:\nstats_bar_6.close()\nreturn (\nnum_epochs,\nlog_elbo,\nlog_reconstruct,\nlog_divergence,\nlog_cat_loss,\nlog_num_loss,\n)\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.io","title":"<code>io</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_base, fn_prepared, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_data</code> <p>The name of the data file.</p> required <code>fn_metadata</code> <p>The name of the metadata file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_input_paths(fn_base: str, fn_prepared: str, fn_metatransformer: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_data: The name of the data file.\n        fn_metadata: The name of the metadata file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_base, fn_prepared, fn_metatransformer = (\nconsistent_ending(fn_base),\nconsistent_ending(fn_prepared),\nconsistent_ending(fn_metatransformer),\n)\nfn_prepared, fn_metatransformer = (\npotential_suffix(fn_prepared, fn_base),\npotential_suffix(fn_metatransformer, fn_base),\n)\nwarn_if_path_supplied([fn_base, fn_prepared, fn_metatransformer], dir_experiment)\ncheck_exists([fn_prepared, fn_metatransformer], dir_experiment)\nreturn fn_base, fn_prepared, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.io.check_output_paths","title":"<code>check_output_paths(fn_base, fn_out, fn_model, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_model</code> <code>str</code> <p>The name of the model file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The path to output the model.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_output_paths(fn_base: Path, fn_out: str, fn_model: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_model: The name of the model file.\n        dir_experiment: The path to the experiment output directory.\n    Returns:\n        The path to output the model.\n    \"\"\"\nfn_out, fn_model = consistent_ending(fn_out, \".csv\"), consistent_ending(fn_model, \".pt\")\nfn_out, fn_model = potential_suffix(fn_out, fn_base), potential_suffix(fn_model, fn_base)\nwarn_if_path_supplied([fn_out, fn_model], dir_experiment)\nreturn fn_out, fn_model\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/model/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif getattr(args, \"dataloader_output\", None):\nreturn (\nargs.dataloader_output[\"fn_base\"],\nargs.dataloader_output[\"data\"],\nargs.dataloader_output[\"metatransformer\"],\n)\nelse:\nif not args.real_data:\nraise ValueError(\n\"You must provide `--real-data` when running this module on its own, please provide this (a prepared version and corresponding MetaTransformer must also exist in {dir_experiment})\"\n)\nfn_base, fn_prepared_data, fn_metatransformer = check_input_paths(\nargs.real_data, args.prepared_data, args.real_metatransformer, dir_experiment\n)\nwith open(dir_experiment / fn_prepared_data, \"rb\") as f:\ndata = pickle.load(f)\nwith open(dir_experiment / fn_metatransformer, \"rb\") as f:\nmt = pickle.load(f)\nreturn fn_base, data, mt\n</code></pre>"},{"location":"reference/modules/#nhssynth.modules.model.run","title":"<code>run</code>","text":""},{"location":"reference/modules/#nhssynth.modules.model.run.run","title":"<code>run(args)</code>","text":"<p>Run the model architecture module.</p> Source code in <code>modules/model/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"Run the model architecture module.\"\"\"\nprint(\"Running model architecture module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\nfn_base, data, mt = load_required_data(args, dir_experiment)\nonehots, singles = mt.get_onehots_and_singles()\nprint(onehots)\nprint(singles)\nprint(data.shape)\nnrows, ncols = data.shape\n# Should the data also all be turned into floats?\ntorch_data = TensorDataset(torch.Tensor(data.to_numpy()))\nsample_rate = args.batch_size / nrows\nmodel = VAE(\nEncoder(input_dim=ncols, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim, use_gpu=args.use_gpu),\nDecoder(args.latent_dim, onehots=onehots, singles=singles, use_gpu=args.use_gpu),\n)\nmodel.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\ndata_loader = DataLoader(\ntorch_data,\nbatch_sampler=UniformWithReplacementSampler(num_samples=nrows, sample_rate=sample_rate),\npin_memory=True,\n# batch_size=args.batch_size,\n)\nif not args.non_private_training:\nprivacy_engine = PrivacyEngine(\n# secure_rng=args.secure_rng,\nmodule=model,\nsample_rate=sample_rate,\nalphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\ntarget_epsilon=args.target_epsilon,\ntarget_delta=args.target_delta,\nepochs=args.num_epochs,\nmax_grad_norm=args.max_grad_norm,\n)\n# model.privacy_engine = PrivacyEngine(secure_mode=args.secure_rng)\n# model, optimizer, data_loader = model.privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=data_loader,\n#     epochs=args.num_epochs,\n#     target_epsilon=args.target_epsilon,\n#     target_delta=args.target_delta,\n#     max_grad_norm=args.max_grad_norm,\n# )\n# print(model)\n# print(f\"Using sigma={optimizer.noise_multiplier} and C={args.max_grad_norm}\")\nresults = model.train(data_loader, args.num_epochs, privacy_engine=privacy_engine)\nelse:\nresults = model.train(data_loader, args.num_epochs)\nsynthetic_data = pd.DataFrame(model.generate(nrows), columns=data.columns)\nfn_output, fn_model = check_output_paths(fn_base, args.synthetic_data, args.model_file, dir_experiment)\nif not args.discard_synthetic:\nsynthetic_data = mt.inverse_apply(synthetic_data)\nsynthetic_data.to_csv(dir_experiment / fn_output, index=False)\nif not args.discard_model:\nmodel.save(dir_experiment / fn_model)\nif args.modules_to_run and \"evaluation\" in args.modules_to_run:\nargs.model_output = {\"results\": results}\nreturn args\n</code></pre>"},{"location":"reference/modules/dataloader/","title":"dataloader","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io","title":"<code>io</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_input, fn_metadata, dir_data)</code>","text":"<p>Formats the input filenames and directory for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_input</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_input</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_input_paths(\nfn_input: str,\nfn_metadata: str,\ndir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input filenames and directory for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_input`.\n        dir_data: The directory that should contain both of the above.\n    Returns:\n        A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).\n    Warnings:\n        Raises a UserWarning when the path to `fn_input` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_input, fn_metadata = consistent_ending(fn_input, \".csv\"), consistent_ending(fn_metadata, \".yaml\")\ndir_data = Path(dir_data)\nfn_metadata = potential_suffix(fn_metadata, fn_input)\nwarn_if_path_supplied([fn_input, fn_metadata], dir_data)\ncheck_exists([fn_input], dir_data)\nreturn dir_data, fn_input, fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_input, fn_output, fn_transformer, dir_experiment)</code>","text":"<p>Formats the output filenames for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_output</code> <code>str</code> <p>The output data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_transformer</code> <code>str</code> <p>The transformer filename/suffix to append to <code>fn_input</code>.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the formatted output filenames.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_output</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_transformer</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_output_paths(\nfn_input: str,\nfn_output: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; tuple[str, str]:\n\"\"\"\n    Formats the output filenames for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_output: The output data filename/suffix to append to `fn_input`.\n        fn_transformer: The transformer filename/suffix to append to `fn_input`.\n        dir_experiment: The experiment directory to write the outputs to.\n    Returns:\n        A tuple containing the formatted output filenames.\n    Warnings:\n        Raises a UserWarning when the path to `fn_output` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_transformer` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_output, fn_transformer = consistent_ending(fn_output), consistent_ending(fn_transformer)\nfn_output, fn_transformer = potential_suffix(fn_output, fn_input), potential_suffix(fn_transformer, fn_input)\nwarn_if_path_supplied([fn_output, fn_transformer], dir_experiment)\nreturn fn_output, fn_transformer\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(transformed_input, metatransformer, fn_output, fn_transformer, dir_experiment)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>transformed_input</code> <code>DataFrame</code> <p>The prepared version of the input data.</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its prepared state.</p> required <code>fn_output</code> <code>str</code> <p>The filename to dump the prepared data to.</p> required <code>fn_transformer</code> <code>str</code> <p>The filename to dump the metatransformer to.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required Source code in <code>modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\ntransformed_input: pd.DataFrame,\nmetatransformer: MetaTransformer,\nfn_output: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n    Args:\n        transformed_input: The prepared version of the input data.\n        metatransformer: The metatransformer used to transform the data into its prepared state.\n        fn_output: The filename to dump the prepared data to.\n        fn_transformer: The filename to dump the metatransformer to.\n        dir_experiment: The experiment directory to write the outputs to.\n    \"\"\"\ntransformed_input.to_pickle(dir_experiment / fn_output)\ntransformed_input.to_csv(dir_experiment / (fn_output[:-3] + \"csv\"), index=False)\nwith open(dir_experiment / fn_transformer, \"wb\") as f:\npickle.dump(metatransformer, f)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata","title":"<code>metadata</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the <code>metadata</code> correspond to valid columns in the <code>data</code>. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the passed <code>data</code>.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns that are in metadata are not present in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict[str, Any]], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the `metadata` correspond to valid columns in the `data`.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n    Args:\n        metadata: A dictionary containing metadata for the columns in the passed `data`.\n        data: The DataFrame to check against the metadata.\n    Raises:\n        AssertionError: If any columns that *are* in metadata are *not* present in the `data`.\n    \"\"\"\nassert all([k in data.columns for k in metadata.keys()])\nmetadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # one entry for each column that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # one entry for each column that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\nc_index = 1\ncolumn_types = {}\nt_index = 1\ntransformers = {}\nfor cn, cd in metadata.items():\nif cd not in column_types.values():\ncolumn_types[c_index] = cd.copy()\nmetadata[cn] = column_types[c_index]\nc_index += 1\nelse:\ncix = get_key_by_value(column_types, cd)\nmetadata[cn] = column_types[cix]\nif cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\ntransformers[t_index] = cd[\"transformer\"].copy()\nmetadata[cn][\"transformer\"] = transformers[t_index]\nt_index += 1\nelif cd[\"transformer\"]:\ntix = get_key_by_value(transformers, cd[\"transformer\"])\nmetadata[cn][\"transformer\"] = transformers[tix]\nreturn {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame in question.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n    Args:\n        data: The DataFrame in question.\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\nreturn {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data`.\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n    Returns:\n        A metadata dictionary containing information about the columns in the `data`.\n    \"\"\"\nif in_path.exists():\nwith open(in_path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out expanded alias/anchor groups\nmetadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\ncheck_metadata_columns(metadata, data)\nelse:\nmetadata = create_empty_metadata(data)\nreturn metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, metadata, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be written.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\nout_path: pathlib.Path,\nmetadata: dict[str, dict[str, Any]],\ncollapse_yaml: bool,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        metadata: The metadata dictionary to be written.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\nif collapse_yaml:\nmetadata = collapse(metadata)\nwith open(out_path, \"w\") as yaml_file:\nyaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer","title":"<code>metatransformer</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code> MetaTransformer        </code>","text":"<p>A metatransformer object that can wrap either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back to the original format of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>A dictionary mapping column names to their metadata.</p> required <code>sdv_workflow</code> <p>A flag indicating whether or not to use the SDV workflow.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> required <code>synthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use within the SDV workflow.</p> required <p>Once instantiated via <code>mt = MetaTransformer(&lt;parameters&gt;)</code>, the following attributes will be available:</p> <p>Attributes:</p> Name Type Description <code>sdv_workflow</code> <code>bool</code> <p>A flag indicating whether or not to use the SDV workflow.</p> <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> <code>Synthesizer</code> <code>BaseSingleTableSynthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use within the SDV workflow.</p> <code>dtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).</p> <code>sdtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to the appropriate SDV-specific data type.</p> <code>transformers</code> <code>dict[str, BaseTransformer | None]</code> <p>A dictionary mapping each column to their assigned (if any) transformer.</p> <p>After preparing some data with the MetaTransformer, i.e. <code>prepared_data = mt.apply(data)</code>, the following attributes and methods will be available:</p> <p>Attributes:</p> Name Type Description <code>metatransformer</code> <code>HyperTransformer | self.Synthesizer</code> <p>An instanatiated <code>HyperTransformer</code> or <code>self.Synthesizer</code> object, ready to use on data.</p> <code>assembled_metadata</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the formatted and complete metadata for the MetaTransformer.</p> <code>onehots</code> <code>list[list[int]]</code> <p>The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).</p> <code>singles</code> <code>list[int]</code> <p>The indices of non-one-hotted columns.</p> <p>Methods:</p> <ul> <li><code>get_assembled_metadata()</code>: Returns the assembled metadata.</li> <li><code>get_onehots_and_singles()</code>: Returns the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</li> <li><code>inverse_apply(synthetic_data)</code>: Apply the inverse of the MetaTransformer to the given data.</li> </ul> <p>Note that <code>mt.apply</code> is a helper function that runs <code>mt.apply_dtypes</code>, <code>mt.instaniate</code>, <code>mt.assemble</code>, <code>mt.prepare</code> and finally <code>mt.count_onehots_and_singles</code> in sequence on a given raw dataset. Along the way it assigns the attributes listed above.</p> <p>This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    A metatransformer object that can wrap either a [`HyperTransformer`](https://docs.sdv.dev/rdt/usage/hypertransformer) from RDT or a\n    [`BaseSingleTableSynthesizer`](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers) from SDV. The metatransformer is\n    responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back\n    to the original format of the input data.\n    Args:\n        metadata: A dictionary mapping column names to their metadata.\n        sdv_workflow: A flag indicating whether or not to use the SDV workflow.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        synthesizer: The `BaseSingleTableSynthesizer` class to use within the SDV workflow.\n    Once instantiated via `mt = MetaTransformer(&lt;parameters&gt;)`, the following attributes will be available:\n    Attributes:\n        sdv_workflow: A flag indicating whether or not to use the SDV workflow.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        Synthesizer: The `BaseSingleTableSynthesizer` class to use within the SDV workflow.\n        dtypes: A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).\n        sdtypes: A dictionary mapping each column to the appropriate SDV-specific data type.\n        transformers: A dictionary mapping each column to their assigned (if any) transformer.\n    After preparing some data with the MetaTransformer, i.e. `prepared_data = mt.apply(data)`, the following attributes and methods will be available:\n    Attributes:\n        metatransformer (HyperTransformer | self.Synthesizer): An instanatiated `HyperTransformer` or `self.Synthesizer` object, ready to use on data.\n        assembled_metadata (dict[str, dict[str, Any]]): A dictionary containing the formatted and complete metadata for the MetaTransformer.\n        onehots (list[list[int]]): The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).\n        singles (list[int]): The indices of non-one-hotted columns.\n    **Methods:**\n    - `get_assembled_metadata()`: Returns the assembled metadata.\n    - `get_onehots_and_singles()`: Returns the values of the MetaTransformer's `onehots` and `singles` attributes.\n    - `inverse_apply(synthetic_data)`: Apply the inverse of the MetaTransformer to the given data.\n    Note that `mt.apply` is a helper function that runs `mt.apply_dtypes`, `mt.instaniate`, `mt.assemble`, `mt.prepare` and finally\n    `mt.count_onehots_and_singles` in sequence on a given raw dataset. Along the way it assigns the attributes listed above.\n    This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.\n    \"\"\"\ndef __init__(self, metadata, sdv_workflow, allow_null_transformers, synthesizer) -&gt; None:\nself.sdv_workflow: bool = sdv_workflow\nself.allow_null_transformers: bool = allow_null_transformers\nself.Synthesizer: BaseSingleTableSynthesizer = SDV_SYNTHESIZER_CHOICES[synthesizer]\n# TODO think about whether these belong here\nself.dtypes: dict[str, dict[str, Any]] = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\nself.sdtypes: dict[str, dict[str, Any]] = {\ncn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()\n}\nself.transformers: dict[str, BaseTransformer | None] = {cn: get_transformer(cd) for cn, cd in metadata.items()}\ndef apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n        Args:\n            data: The raw input DataFrame.\n        Returns:\n            The data with the dtypes applied.\n        \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\ndef _instantiate_ohe_component_transformers(\nself, transformers: dict[str, BaseTransformer | None]\n) -&gt; dict[str, BaseTransformer]:\n\"\"\"\n        Instantiates a OneHotEncoder for each resulting `*.component` column that arises from a ClusterBasedNormalizer.\n        Args:\n            transformers: A dictionary mapping column names to their assigned transformers.\n        Returns:\n            A dictionary mapping each `*.component` column to a OneHotEncoder.\n        \"\"\"\nreturn {\nf\"{cn}.component\": OneHotEncoder()\nfor cn, transformer in transformers.items()\nif transformer.get_name() == \"ClusterBasedNormalizer\"\n}\ndef _instantiate_synthesizer(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n        Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete and `self.allow_null_transformers` is `False`.\n        \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\ndef _instantiate_hypertransformer(self, data: pd.DataFrame) -&gt; HyperTransformer:\n\"\"\"\n        Instantiates a `HyperTransformer` object from the metadata and given data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `HyperTransformer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete.\n        \"\"\"\nht = HyperTransformer()\nif all(self.sdtypes.values()) and (all(self.transformers.values()) or self.allow_null_transformers):\nht.set_config(\nconfig={\n\"sdtypes\": {k: v[\"sdtype\"] for k, v in self.sdtypes.items()},\n\"transformers\": self.transformers,\n}\n)\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in self.sdtypes.items() if not v])) if not all(self.sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in self.transformers.items() if not v])) if not all(self.transformers.values()) and not self.allow_null_transformers else ''} automatically...\",\nUserWarning,\n)\nht.detect_initial_config(data)\nht.update_sdtypes({k: v[\"sdtype\"] for k, v in self.sdtypes.items() if v})\nht.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(ht.get_config()[\"transformers\"])\nreturn ht, component_transformer\ndef instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n        Calls the appropriate instantiation method based on the value of `self.sdv_workflow`.\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` or `HyperTransformer` object.\n        \"\"\"\nif self.sdv_workflow:\nreturn self._instantiate_synthesizer(data)\nelse:\nreturn self._instantiate_hypertransformer(data)\ndef _get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\nreturn self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\ndef assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of `self,sdv_workflow`\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nsdmetadata = self.metatransformer.metadata\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in sdmetadata.columns.items()\n}\nelse:\nconfig = self.metatransformer.get_config()\nreturn {\ncn: {\n\"sdtype\": cd,\n\"transformer\": make_transformer_dict(config[\"transformers\"][cn])\nif config[\"transformers\"][cn]\nelse None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in config[\"sdtypes\"].items()\n}\ndef prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the data by processing it via the metatransformer.\n        Args:\n            data: The data to fit and apply the transformer to.\n        Returns:\n            The transformed data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nprepared_data = self.metatransformer.preprocess(data)\nelse:\nprepared_data = self.metatransformer.fit_transform(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\ndef count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n        Also records the indices of non-one-hotted columns in a separate list.\n        Args:\n            data: The data to extract column indices from.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn).columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelse:\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\ndef apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n        Args:\n            data: The DataFrame to transform.\n        Returns:\n            The transformed data.\n        \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn prepared_data\ndef get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Returns the assembled metadata for the transformer.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not self.assembled_metadata:\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\ndef get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        Raises:\n            ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n        \"\"\"\nif not self.onehots or not self.singles:\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\ndef inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n        Args:\n            data: The transformed data.\n        Returns:\n            The original data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nif self.sdv_workflow:\nreturn self.metatransformer._data_processor.reverse_transform(data)\nelse:\nreturn self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply(self, data)</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n    Args:\n        data: The DataFrame to transform.\n    Returns:\n        The transformed data.\n    \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(self, data)</code>","text":"<p>Applies dtypes from the metadata to <code>data</code> and infers missing dtypes by reading pandas defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the dtypes applied.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n    Args:\n        data: The raw input DataFrame.\n    Returns:\n        The data with the dtypes applied.\n    \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.assemble","title":"<code>assemble(self)</code>","text":"<p>Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of <code>self,sdv_workflow</code></p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of `self,sdv_workflow`\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nsdmetadata = self.metatransformer.metadata\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in sdmetadata.columns.items()\n}\nelse:\nconfig = self.metatransformer.get_config()\nreturn {\ncn: {\n\"sdtype\": cd,\n\"transformer\": make_transformer_dict(config[\"transformers\"][cn])\nif config[\"transformers\"][cn]\nelse None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in config[\"sdtypes\"].items()\n}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.count_onehots_and_singles","title":"<code>count_onehots_and_singles(self, data)</code>","text":"<p>Uses the assembled metadata to identify and record the indices of one-hotted column groups. Also records the indices of non-one-hotted columns in a separate list.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to extract column indices from.</p> required <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n    Also records the indices of non-one-hotted columns in a separate list.\n    Args:\n        data: The data to extract column indices from.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn).columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelse:\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_assembled_metadata","title":"<code>get_assembled_metadata(self)</code>","text":"<p>Returns the assembled metadata for the transformer.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Returns the assembled metadata for the transformer.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not self.assembled_metadata:\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_onehots_and_singles","title":"<code>get_onehots_and_singles(self)</code>","text":"<p>Get the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</p> <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>self.onehots</code> and <code>self.singles</code> have yet to be counted.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    Raises:\n        ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n    \"\"\"\nif not self.onehots or not self.singles:\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate","title":"<code>instantiate(self, data)</code>","text":"<p>Calls the appropriate instantiation method based on the value of <code>self.sdv_workflow</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Type Description <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A fully instantiated <code>self.Synthesizer</code> or <code>HyperTransformer</code> object.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n    Calls the appropriate instantiation method based on the value of `self.sdv_workflow`.\n    Args:\n        data: The input DataFrame.\n    Returns:\n        A fully instantiated `self.Synthesizer` or `HyperTransformer` object.\n    \"\"\"\nif self.sdv_workflow:\nreturn self._instantiate_synthesizer(data)\nelse:\nreturn self._instantiate_hypertransformer(data)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(self, data)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The transformed data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n    Args:\n        data: The transformed data.\n    Returns:\n        The original data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nif self.sdv_workflow:\nreturn self.metatransformer._data_processor.reverse_transform(data)\nelse:\nreturn self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.prepare","title":"<code>prepare(self, data)</code>","text":"<p>Prepares the data by processing it via the metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to fit and apply the transformer to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the data by processing it via the metatransformer.\n    Args:\n        data: The data to fit and apply the transformer to.\n    Returns:\n        The transformed data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nprepared_data = self.metatransformer.preprocess(data)\nelse:\nprepared_data = self.metatransformer.fit_transform(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object constructed from data in the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>rdt.transformers.base.BaseTransformer | None</code> <p>An instantiated <code>BaseTransformer</code> if the dictionary contains valid transformer data, else None.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_transformer(d: dict) -&gt; BaseTransformer | None:\n\"\"\"\n    Return a callable transformer object constructed from data in the given dictionary.\n    Args:\n        d: A dictionary containing the transformer data.\n    Returns:\n        An instantiated `BaseTransformer` if the dictionary contains valid transformer data, else None.\n    \"\"\"\ntransformer_data = d.get(\"transformer\", None)\nif isinstance(transformer_data, dict) and \"name\" in transformer_data:\n# Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\ntransformer_data = transformer_data.copy()\ntransformer_name = transformer_data.pop(\"name\")\nreturn eval(transformer_name)(**transformer_data)\nelif isinstance(transformer_data, str):\nreturn eval(transformer_data)()\nelse:\nreturn None\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.metatransformer.make_transformer_dict","title":"<code>make_transformer_dict(transformer)</code>","text":"<p>Deconstruct a <code>transformer</code> into a dictionary of config.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>BaseTransformer</code> <p>A BaseTransformer object from RDT (SDV).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the transformer's name and arguments.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def make_transformer_dict(transformer: BaseTransformer) -&gt; dict[str, Any]:\n\"\"\"\n    Deconstruct a `transformer` into a dictionary of config.\n    Args:\n        transformer: A BaseTransformer object from RDT (SDV).\n    Returns:\n        A dictionary containing the transformer's name and arguments.\n    \"\"\"\nreturn {\n\"name\": type(transformer).__name__,\n**filter_dict(\ntransformer.__dict__,\n{\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n),\n}\n</code></pre>"},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.run","title":"<code>run</code>","text":""},{"location":"reference/modules/dataloader/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforming the input data and writing the output and transformer used to file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforming the input data and writing the output and transformer used to file.\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n    \"\"\"\nprint(\"Running dataloader module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\ndir_input, fn_input_data, fn_metadata = check_input_paths(args.input, args.metadata, args.data_dir)\n# Load the dataset and accompanying metadata\ninput = pd.read_csv(dir_input / fn_input_data, index_col=args.index_col)\nmetadata = load_metadata(dir_input / fn_metadata, input)\nmt = MetaTransformer(metadata, args.sdv_workflow, args.allow_null_transformers, args.synthesizer)\ntransformed_input = mt.apply(input)\n# Output the metadata corresponding to `transformed_input`, for reproducibility\nif not args.discard_metadata:\noutput_metadata(dir_experiment / fn_metadata, mt.get_assembled_metadata(), args.collapse_yaml)\n# Write the transformed input to the appropriate file\nif not args.modules_to_run or args.modules_to_run == [\"dataloader\"] or args.write_all:\nfn_output_data, fn_transformer = check_output_paths(\nfn_input_data, args.output, args.metatransformer, dir_experiment\n)\nwrite_data_outputs(transformed_input, mt, fn_output_data, fn_transformer, dir_experiment)\nif \"model\" in args.modules_to_run:\nargs.dataloader_output = {\n\"fn_base\": fn_input_data,\n\"data\": transformed_input,\n\"metatransformer\": mt,\n}\nreturn args\n</code></pre>"},{"location":"reference/modules/dataloader/io/","title":"io","text":""},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_input, fn_metadata, dir_data)</code>","text":"<p>Formats the input filenames and directory for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_input</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_input</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_input_paths(\nfn_input: str,\nfn_metadata: str,\ndir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input filenames and directory for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_input`.\n        dir_data: The directory that should contain both of the above.\n    Returns:\n        A tuple containing the correct directory path, input data filename and metadata filename (used for both in and out).\n    Warnings:\n        Raises a UserWarning when the path to `fn_input` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_input, fn_metadata = consistent_ending(fn_input, \".csv\"), consistent_ending(fn_metadata, \".yaml\")\ndir_data = Path(dir_data)\nfn_metadata = potential_suffix(fn_metadata, fn_input)\nwarn_if_path_supplied([fn_input, fn_metadata], dir_data)\ncheck_exists([fn_input], dir_data)\nreturn dir_data, fn_input, fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_input, fn_output, fn_transformer, dir_experiment)</code>","text":"<p>Formats the output filenames for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_input</code> <code>str</code> <p>The input data filename.</p> required <code>fn_output</code> <code>str</code> <p>The output data filename/suffix to append to <code>fn_input</code>.</p> required <code>fn_transformer</code> <code>str</code> <p>The transformer filename/suffix to append to <code>fn_input</code>.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the formatted output filenames.</p> <p>!!! warnings     Raises a UserWarning when the path to <code>fn_output</code> includes directory separators, as this is not supported and may not work as intended.     Raises a UserWarning when the path to <code>fn_transformer</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>modules/dataloader/io.py</code> <pre><code>def check_output_paths(\nfn_input: str,\nfn_output: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; tuple[str, str]:\n\"\"\"\n    Formats the output filenames for an experiment.\n    Args:\n        fn_input: The input data filename.\n        fn_output: The output data filename/suffix to append to `fn_input`.\n        fn_transformer: The transformer filename/suffix to append to `fn_input`.\n        dir_experiment: The experiment directory to write the outputs to.\n    Returns:\n        A tuple containing the formatted output filenames.\n    Warnings:\n        Raises a UserWarning when the path to `fn_output` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_transformer` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\nfn_output, fn_transformer = consistent_ending(fn_output), consistent_ending(fn_transformer)\nfn_output, fn_transformer = potential_suffix(fn_output, fn_input), potential_suffix(fn_transformer, fn_input)\nwarn_if_path_supplied([fn_output, fn_transformer], dir_experiment)\nreturn fn_output, fn_transformer\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(transformed_input, metatransformer, fn_output, fn_transformer, dir_experiment)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>transformed_input</code> <code>DataFrame</code> <p>The prepared version of the input data.</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its prepared state.</p> required <code>fn_output</code> <code>str</code> <p>The filename to dump the prepared data to.</p> required <code>fn_transformer</code> <code>str</code> <p>The filename to dump the metatransformer to.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required Source code in <code>modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\ntransformed_input: pd.DataFrame,\nmetatransformer: MetaTransformer,\nfn_output: str,\nfn_transformer: str,\ndir_experiment: Path,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n    Args:\n        transformed_input: The prepared version of the input data.\n        metatransformer: The metatransformer used to transform the data into its prepared state.\n        fn_output: The filename to dump the prepared data to.\n        fn_transformer: The filename to dump the metatransformer to.\n        dir_experiment: The experiment directory to write the outputs to.\n    \"\"\"\ntransformed_input.to_pickle(dir_experiment / fn_output)\ntransformed_input.to_csv(dir_experiment / (fn_output[:-3] + \"csv\"), index=False)\nwith open(dir_experiment / fn_transformer, \"wb\") as f:\npickle.dump(metatransformer, f)\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/","title":"metadata","text":""},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the <code>metadata</code> correspond to valid columns in the <code>data</code>. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>A dictionary containing metadata for the columns in the passed <code>data</code>.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Exceptions:</p> Type Description <code>AssertionError</code> <p>If any columns that are in metadata are not present in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict[str, Any]], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the `metadata` correspond to valid columns in the `data`.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n    Args:\n        metadata: A dictionary containing metadata for the columns in the passed `data`.\n        data: The DataFrame to check against the metadata.\n    Raises:\n        AssertionError: If any columns that *are* in metadata are *not* present in the `data`.\n    \"\"\"\nassert all([k in data.columns for k in metadata.keys()])\nmetadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers.     The returned dictionary has the following structure:     {         \"transformers\": dict,         \"column_types\": dict,         metadata  # one entry for each column that now reference the dicts above     }     - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.     - \"column_types\" is a dictionary mapping column type indices to column type configurations.     - \"metadata\" contains the original metadata dictionary, with column types and transformers       rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrite to collapse duplicate column types and transformers in order to leverage YAML anchors\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # one entry for each column that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\nc_index = 1\ncolumn_types = {}\nt_index = 1\ntransformers = {}\nfor cn, cd in metadata.items():\nif cd not in column_types.values():\ncolumn_types[c_index] = cd.copy()\nmetadata[cn] = column_types[c_index]\nc_index += 1\nelse:\ncix = get_key_by_value(column_types, cd)\nmetadata[cn] = column_types[cix]\nif cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\ntransformers[t_index] = cd[\"transformer\"].copy()\nmetadata[cn][\"transformer\"] = transformers[t_index]\nt_index += 1\nelif cd[\"transformer\"]:\ntix = get_key_by_value(transformers, cd[\"transformer\"])\nmetadata[cn][\"transformer\"] = transformers[tix]\nreturn {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame in question.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n    Args:\n        data: The DataFrame in question.\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\nreturn {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A metadata dictionary containing information about the columns in the <code>data</code>.</p> Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data`.\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n    Returns:\n        A metadata dictionary containing information about the columns in the `data`.\n    \"\"\"\nif in_path.exists():\nwith open(in_path) as stream:\nmetadata = yaml.safe_load(stream)\n# Filter out expanded alias/anchor groups\nmetadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\ncheck_metadata_columns(metadata, data)\nelse:\nmetadata = create_empty_metadata(data)\nreturn metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, metadata, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be written.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\nout_path: pathlib.Path,\nmetadata: dict[str, dict[str, Any]],\ncollapse_yaml: bool,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        metadata: The metadata dictionary to be written.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\nif collapse_yaml:\nmetadata = collapse(metadata)\nwith open(out_path, \"w\") as yaml_file:\nyaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/","title":"metatransformer","text":""},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code> MetaTransformer        </code>","text":"<p>A metatransformer object that can wrap either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV. The metatransformer is responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back to the original format of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <p>A dictionary mapping column names to their metadata.</p> required <code>sdv_workflow</code> <p>A flag indicating whether or not to use the SDV workflow.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> required <code>synthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use within the SDV workflow.</p> required <p>Once instantiated via <code>mt = MetaTransformer(&lt;parameters&gt;)</code>, the following attributes will be available:</p> <p>Attributes:</p> Name Type Description <code>sdv_workflow</code> <code>bool</code> <p>A flag indicating whether or not to use the SDV workflow.</p> <code>allow_null_transformers</code> <code>bool</code> <p>A flag indicating whether or not to allow null transformers on some / all columns.</p> <code>Synthesizer</code> <code>BaseSingleTableSynthesizer</code> <p>The <code>BaseSingleTableSynthesizer</code> class to use within the SDV workflow.</p> <code>dtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).</p> <code>sdtypes</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping each column to the appropriate SDV-specific data type.</p> <code>transformers</code> <code>dict[str, BaseTransformer | None]</code> <p>A dictionary mapping each column to their assigned (if any) transformer.</p> <p>After preparing some data with the MetaTransformer, i.e. <code>prepared_data = mt.apply(data)</code>, the following attributes and methods will be available:</p> <p>Attributes:</p> Name Type Description <code>metatransformer</code> <code>HyperTransformer | self.Synthesizer</code> <p>An instanatiated <code>HyperTransformer</code> or <code>self.Synthesizer</code> object, ready to use on data.</p> <code>assembled_metadata</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the formatted and complete metadata for the MetaTransformer.</p> <code>onehots</code> <code>list[list[int]]</code> <p>The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).</p> <code>singles</code> <code>list[int]</code> <p>The indices of non-one-hotted columns.</p> <p>Methods:</p> <ul> <li><code>get_assembled_metadata()</code>: Returns the assembled metadata.</li> <li><code>get_onehots_and_singles()</code>: Returns the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</li> <li><code>inverse_apply(synthetic_data)</code>: Apply the inverse of the MetaTransformer to the given data.</li> </ul> <p>Note that <code>mt.apply</code> is a helper function that runs <code>mt.apply_dtypes</code>, <code>mt.instaniate</code>, <code>mt.assemble</code>, <code>mt.prepare</code> and finally <code>mt.count_onehots_and_singles</code> in sequence on a given raw dataset. Along the way it assigns the attributes listed above.</p> <p>This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    A metatransformer object that can wrap either a [`HyperTransformer`](https://docs.sdv.dev/rdt/usage/hypertransformer) from RDT or a\n    [`BaseSingleTableSynthesizer`](https://docs.sdv.dev/sdv/single-table-data/modeling/synthesizers) from SDV. The metatransformer is\n    responsible for transforming input data into a format that can be used by the model module, and transforming the module's output back\n    to the original format of the input data.\n    Args:\n        metadata: A dictionary mapping column names to their metadata.\n        sdv_workflow: A flag indicating whether or not to use the SDV workflow.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        synthesizer: The `BaseSingleTableSynthesizer` class to use within the SDV workflow.\n    Once instantiated via `mt = MetaTransformer(&lt;parameters&gt;)`, the following attributes will be available:\n    Attributes:\n        sdv_workflow: A flag indicating whether or not to use the SDV workflow.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers on some / all columns.\n        Synthesizer: The `BaseSingleTableSynthesizer` class to use within the SDV workflow.\n        dtypes: A dictionary mapping each column to its specified pandas dtype (will infer from pandas defaults if this is missing).\n        sdtypes: A dictionary mapping each column to the appropriate SDV-specific data type.\n        transformers: A dictionary mapping each column to their assigned (if any) transformer.\n    After preparing some data with the MetaTransformer, i.e. `prepared_data = mt.apply(data)`, the following attributes and methods will be available:\n    Attributes:\n        metatransformer (HyperTransformer | self.Synthesizer): An instanatiated `HyperTransformer` or `self.Synthesizer` object, ready to use on data.\n        assembled_metadata (dict[str, dict[str, Any]]): A dictionary containing the formatted and complete metadata for the MetaTransformer.\n        onehots (list[list[int]]): The groups of indices of one-hotted columns (i.e. each inner list contains all levels of one categorical).\n        singles (list[int]): The indices of non-one-hotted columns.\n    **Methods:**\n    - `get_assembled_metadata()`: Returns the assembled metadata.\n    - `get_onehots_and_singles()`: Returns the values of the MetaTransformer's `onehots` and `singles` attributes.\n    - `inverse_apply(synthetic_data)`: Apply the inverse of the MetaTransformer to the given data.\n    Note that `mt.apply` is a helper function that runs `mt.apply_dtypes`, `mt.instaniate`, `mt.assemble`, `mt.prepare` and finally\n    `mt.count_onehots_and_singles` in sequence on a given raw dataset. Along the way it assigns the attributes listed above.\n    This workflow is highly encouraged to ensure that the MetaTransformer is properly instantiated for use with the model module.\n    \"\"\"\ndef __init__(self, metadata, sdv_workflow, allow_null_transformers, synthesizer) -&gt; None:\nself.sdv_workflow: bool = sdv_workflow\nself.allow_null_transformers: bool = allow_null_transformers\nself.Synthesizer: BaseSingleTableSynthesizer = SDV_SYNTHESIZER_CHOICES[synthesizer]\n# TODO think about whether these belong here\nself.dtypes: dict[str, dict[str, Any]] = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\nself.sdtypes: dict[str, dict[str, Any]] = {\ncn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()\n}\nself.transformers: dict[str, BaseTransformer | None] = {cn: get_transformer(cd) for cn, cd in metadata.items()}\ndef apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n        Args:\n            data: The raw input DataFrame.\n        Returns:\n            The data with the dtypes applied.\n        \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\ndef _instantiate_ohe_component_transformers(\nself, transformers: dict[str, BaseTransformer | None]\n) -&gt; dict[str, BaseTransformer]:\n\"\"\"\n        Instantiates a OneHotEncoder for each resulting `*.component` column that arises from a ClusterBasedNormalizer.\n        Args:\n            transformers: A dictionary mapping column names to their assigned transformers.\n        Returns:\n            A dictionary mapping each `*.component` column to a OneHotEncoder.\n        \"\"\"\nreturn {\nf\"{cn}.component\": OneHotEncoder()\nfor cn, transformer in transformers.items()\nif transformer.get_name() == \"ClusterBasedNormalizer\"\n}\ndef _instantiate_synthesizer(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n        Instantiates a `self.Synthesizer` object from the given metadata and data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete and `self.allow_null_transformers` is `False`.\n        \"\"\"\nif all(self.sdtypes.values()):\nmetadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(data)\nfor column_name, values in self.sdtypes.items():\nif values:\nmetadata.update_column(column_name=column_name, **values)\nif not all(self.transformers.values()) and not self.allow_null_transformers:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\nUserWarning,\n)\nsynthesizer = self.Synthesizer(metadata)\nsynthesizer.auto_assign_transformers(data)\nsynthesizer.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(synthesizer.get_transformers())\nreturn synthesizer, component_transformer\ndef _instantiate_hypertransformer(self, data: pd.DataFrame) -&gt; HyperTransformer:\n\"\"\"\n        Instantiates a `HyperTransformer` object from the metadata and given data. Infers missing metadata (sdtypes and transformers).\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `HyperTransformer` object and a transformer for the `*.component` columns.\n        Raises:\n            UserWarning: If the metadata is incomplete.\n        \"\"\"\nht = HyperTransformer()\nif all(self.sdtypes.values()) and (all(self.transformers.values()) or self.allow_null_transformers):\nht.set_config(\nconfig={\n\"sdtypes\": {k: v[\"sdtype\"] for k, v in self.sdtypes.items()},\n\"transformers\": self.transformers,\n}\n)\nelse:\nwarnings.warn(\nf\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in self.sdtypes.items() if not v])) if not all(self.sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in self.transformers.items() if not v])) if not all(self.transformers.values()) and not self.allow_null_transformers else ''} automatically...\",\nUserWarning,\n)\nht.detect_initial_config(data)\nht.update_sdtypes({k: v[\"sdtype\"] for k, v in self.sdtypes.items() if v})\nht.update_transformers(\nself.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n)\n# TODO this is a hacky way to get the component columns we want to apply OneHotEncoder to\ncomponent_transformer = self._instantiate_ohe_component_transformers(ht.get_config()[\"transformers\"])\nreturn ht, component_transformer\ndef instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n        Calls the appropriate instantiation method based on the value of `self.sdv_workflow`.\n        Args:\n            data: The input DataFrame.\n        Returns:\n            A fully instantiated `self.Synthesizer` or `HyperTransformer` object.\n        \"\"\"\nif self.sdv_workflow:\nreturn self._instantiate_synthesizer(data)\nelse:\nreturn self._instantiate_hypertransformer(data)\ndef _get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\nreturn self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\ndef assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of `self,sdv_workflow`\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nsdmetadata = self.metatransformer.metadata\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in sdmetadata.columns.items()\n}\nelse:\nconfig = self.metatransformer.get_config()\nreturn {\ncn: {\n\"sdtype\": cd,\n\"transformer\": make_transformer_dict(config[\"transformers\"][cn])\nif config[\"transformers\"][cn]\nelse None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in config[\"sdtypes\"].items()\n}\ndef prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the data by processing it via the metatransformer.\n        Args:\n            data: The data to fit and apply the transformer to.\n        Returns:\n            The transformed data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nprepared_data = self.metatransformer.preprocess(data)\nelse:\nprepared_data = self.metatransformer.fit_transform(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\ndef count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n        Also records the indices of non-one-hotted columns in a separate list.\n        Args:\n            data: The data to extract column indices from.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn).columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelse:\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\ndef apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n        Args:\n            data: The DataFrame to transform.\n        Returns:\n            The transformed data.\n        \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn prepared_data\ndef get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Returns the assembled metadata for the transformer.\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - dtype: The pandas data type for the column\n                - sdtype: The SDV-specific data type for the column.\n                - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                    - name: The name of the transformer.\n                    - Any other properties of the transformer that we want to record in output.\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\nif not self.assembled_metadata:\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\ndef get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n        Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n        Returns:\n            A pair of lists:\n                - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n                - Non-one-hotted column indices\n        Raises:\n            ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n        \"\"\"\nif not self.onehots or not self.singles:\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\ndef inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n        Args:\n            data: The transformed data.\n        Returns:\n            The original data.\n        Raises:\n            ValueError: If the metatransformer has not yet been instantiated.\n        \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nif self.sdv_workflow:\nreturn self.metatransformer._data_processor.reverse_transform(data)\nelse:\nreturn self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply(self, data)</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n    Args:\n        data: The DataFrame to transform.\n    Returns:\n        The transformed data.\n    \"\"\"\ntyped_data = self.apply_dtypes(data)\nself.metatransformer, self.component_transformer = self.instantiate(typed_data)\nself.assembled_metadata = self.assemble()\nprepared_data = self.prepare(typed_data)\nself.onehots, self.singles = self.count_onehots_and_singles(prepared_data)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(self, data)</code>","text":"<p>Applies dtypes from the metadata to <code>data</code> and infers missing dtypes by reading pandas defaults.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The raw input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The data with the dtypes applied.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n    Args:\n        data: The raw input DataFrame.\n    Returns:\n        The data with the dtypes applied.\n    \"\"\"\nif not all(self.dtypes.values()):\nwarnings.warn(\nf\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\nUserWarning,\n)\nself.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\nreturn data.astype(self.dtypes)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.assemble","title":"<code>assemble(self)</code>","text":"<p>Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of <code>self,sdv_workflow</code></p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def assemble(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Rearranges the dtype, sdtype and transformer metadata into a consistent format regardless of the value of `self,sdv_workflow`\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nsdmetadata = self.metatransformer.metadata\ntransformers = self.metatransformer.get_transformers()\nreturn {\ncn: {\n**cd,\n\"transformer\": make_transformer_dict(transformers[cn]) if transformers[cn] else None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in sdmetadata.columns.items()\n}\nelse:\nconfig = self.metatransformer.get_config()\nreturn {\ncn: {\n\"sdtype\": cd,\n\"transformer\": make_transformer_dict(config[\"transformers\"][cn])\nif config[\"transformers\"][cn]\nelse None,\n\"dtype\": self._get_dtype(cn),\n}\nfor cn, cd in config[\"sdtypes\"].items()\n}\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.count_onehots_and_singles","title":"<code>count_onehots_and_singles(self, data)</code>","text":"<p>Uses the assembled metadata to identify and record the indices of one-hotted column groups. Also records the indices of non-one-hotted columns in a separate list.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to extract column indices from.</p> required <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def count_onehots_and_singles(self, data: pd.DataFrame) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Uses the assembled metadata to identify and record the indices of one-hotted column groups.\n    Also records the indices of non-one-hotted columns in a separate list.\n    Args:\n        data: The data to extract column indices from.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    \"\"\"\nif not self.assembled_metadata:\nself.assembled_metadata = self.assemble()\nonehot_idxs = []\nsingle_idxs = []\nfor cn, cd in self.assembled_metadata.items():\nif cd[\"transformer\"].get(\"name\") == \"OneHotEncoder\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn).columns).tolist())\nelif cd[\"transformer\"].get(\"name\") == \"ClusterBasedNormalizer\":\nonehot_idxs.append(data.columns.get_indexer(data.filter(like=cn + \".component\").columns).tolist())\nsingle_idxs.append(data.columns.get_loc(cn + \".normalized\"))\nelse:\nsingle_idxs.append(data.columns.get_loc(cn))\nreturn onehot_idxs, single_idxs\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_assembled_metadata","title":"<code>get_assembled_metadata(self)</code>","text":"<p>Returns the assembled metadata for the transformer.</p> <p>Returns:</p> Type Description <code>A dictionary mapping column names to column metadata.     The metadata for each column has the following keys</code> <ul> <li>dtype: The pandas data type for the column<ul> <li>sdtype: The SDV-specific data type for the column.</li> <li>transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:<ul> <li>name: The name of the transformer.</li> <li>Any other properties of the transformer that we want to record in output.</li> </ul> </li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Returns the assembled metadata for the transformer.\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - dtype: The pandas data type for the column\n            - sdtype: The SDV-specific data type for the column.\n            - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that we want to record in output.\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\nif not self.assembled_metadata:\nraise ValueError(\"Metadata has not yet been assembled. Call `my.apply(data)` (or `mt.assemble()`) first.\")\nreturn self.assembled_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_onehots_and_singles","title":"<code>get_onehots_and_singles(self)</code>","text":"<p>Get the values of the MetaTransformer's <code>onehots</code> and <code>singles</code> attributes.</p> <p>Returns:</p> Type Description <code>A pair of lists</code> <ul> <li>One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)<ul> <li>Non-one-hotted column indices</li> </ul> </li> </ul> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>self.onehots</code> and <code>self.singles</code> have yet to be counted.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_onehots_and_singles(self) -&gt; tuple[list[list[int]], list[int]]:\n\"\"\"\n    Get the values of the MetaTransformer's `onehots` and `singles` attributes.\n    Returns:\n        A pair of lists:\n            - One-hotted column index groups (i.e. one inner list with all corresponding indices per categorical variable)\n            - Non-one-hotted column indices\n    Raises:\n        ValueError: If `self.onehots` and `self.singles` have yet to be counted.\n    \"\"\"\nif not self.onehots or not self.singles:\nraise ValueError(\n\"Some metadata is missing. Call `mt.apply(data)` first (or `mt.count_onehots_and_singles(data)`).\"\n)\nreturn self.onehots, self.singles\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate","title":"<code>instantiate(self, data)</code>","text":"<p>Calls the appropriate instantiation method based on the value of <code>self.sdv_workflow</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The input DataFrame.</p> required <p>Returns:</p> Type Description <code>sdv.single_table.base.BaseSingleTableSynthesizer | rdt.hyper_transformer.HyperTransformer</code> <p>A fully instantiated <code>self.Synthesizer</code> or <code>HyperTransformer</code> object.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"\n    Calls the appropriate instantiation method based on the value of `self.sdv_workflow`.\n    Args:\n        data: The input DataFrame.\n    Returns:\n        A fully instantiated `self.Synthesizer` or `HyperTransformer` object.\n    \"\"\"\nif self.sdv_workflow:\nreturn self._instantiate_synthesizer(data)\nelse:\nreturn self._instantiate_hypertransformer(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(self, data)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The transformed data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The original data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n    Args:\n        data: The transformed data.\n    Returns:\n        The original data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nfor transformer in self.component_transformer.values():\ndata = transformer.reverse_transform(data)\nif self.sdv_workflow:\nreturn self.metatransformer._data_processor.reverse_transform(data)\nelse:\nreturn self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.prepare","title":"<code>prepare(self, data)</code>","text":"<p>Prepares the data by processing it via the metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to fit and apply the transformer to.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If the metatransformer has not yet been instantiated.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the data by processing it via the metatransformer.\n    Args:\n        data: The data to fit and apply the transformer to.\n    Returns:\n        The transformed data.\n    Raises:\n        ValueError: If the metatransformer has not yet been instantiated.\n    \"\"\"\nif not self.metatransformer:\nraise ValueError(\n\"The metatransformer has not yet been instantiated. Call `mt.apply(data)` first (or `mt.instantiate(data)`).\"\n)\nif self.sdv_workflow:\nprepared_data = self.metatransformer.preprocess(data)\nelse:\nprepared_data = self.metatransformer.fit_transform(data)\n# TODO this is kind of a hacky way to solve the component column problem\nfor cn, transformer in self.component_transformer.items():\nprepared_data = transformer.fit_transform(prepared_data, cn)\nreturn prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object constructed from data in the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>rdt.transformers.base.BaseTransformer | None</code> <p>An instantiated <code>BaseTransformer</code> if the dictionary contains valid transformer data, else None.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def get_transformer(d: dict) -&gt; BaseTransformer | None:\n\"\"\"\n    Return a callable transformer object constructed from data in the given dictionary.\n    Args:\n        d: A dictionary containing the transformer data.\n    Returns:\n        An instantiated `BaseTransformer` if the dictionary contains valid transformer data, else None.\n    \"\"\"\ntransformer_data = d.get(\"transformer\", None)\nif isinstance(transformer_data, dict) and \"name\" in transformer_data:\n# Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\ntransformer_data = transformer_data.copy()\ntransformer_name = transformer_data.pop(\"name\")\nreturn eval(transformer_name)(**transformer_data)\nelif isinstance(transformer_data, str):\nreturn eval(transformer_data)()\nelse:\nreturn None\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.make_transformer_dict","title":"<code>make_transformer_dict(transformer)</code>","text":"<p>Deconstruct a <code>transformer</code> into a dictionary of config.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>BaseTransformer</code> <p>A BaseTransformer object from RDT (SDV).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the transformer's name and arguments.</p> Source code in <code>modules/dataloader/metatransformer.py</code> <pre><code>def make_transformer_dict(transformer: BaseTransformer) -&gt; dict[str, Any]:\n\"\"\"\n    Deconstruct a `transformer` into a dictionary of config.\n    Args:\n        transformer: A BaseTransformer object from RDT (SDV).\n    Returns:\n        A dictionary containing the transformer's name and arguments.\n    \"\"\"\nreturn {\n\"name\": type(transformer).__name__,\n**filter_dict(\ntransformer.__dict__,\n{\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n),\n}\n</code></pre>"},{"location":"reference/modules/dataloader/run/","title":"run","text":""},{"location":"reference/modules/dataloader/run/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforming the input data and writing the output and transformer used to file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required Source code in <code>modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforming the input data and writing the output and transformer used to file.\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n    \"\"\"\nprint(\"Running dataloader module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\ndir_input, fn_input_data, fn_metadata = check_input_paths(args.input, args.metadata, args.data_dir)\n# Load the dataset and accompanying metadata\ninput = pd.read_csv(dir_input / fn_input_data, index_col=args.index_col)\nmetadata = load_metadata(dir_input / fn_metadata, input)\nmt = MetaTransformer(metadata, args.sdv_workflow, args.allow_null_transformers, args.synthesizer)\ntransformed_input = mt.apply(input)\n# Output the metadata corresponding to `transformed_input`, for reproducibility\nif not args.discard_metadata:\noutput_metadata(dir_experiment / fn_metadata, mt.get_assembled_metadata(), args.collapse_yaml)\n# Write the transformed input to the appropriate file\nif not args.modules_to_run or args.modules_to_run == [\"dataloader\"] or args.write_all:\nfn_output_data, fn_transformer = check_output_paths(\nfn_input_data, args.output, args.metatransformer, dir_experiment\n)\nwrite_data_outputs(transformed_input, mt, fn_output_data, fn_transformer, dir_experiment)\nif \"model\" in args.modules_to_run:\nargs.dataloader_output = {\n\"fn_base\": fn_input_data,\n\"data\": transformed_input,\n\"metatransformer\": mt,\n}\nreturn args\n</code></pre>"},{"location":"reference/modules/evaluation/","title":"evaluation","text":""},{"location":"reference/modules/evaluation/metrics/","title":"metrics","text":""},{"location":"reference/modules/evaluation/run/","title":"run","text":""},{"location":"reference/modules/model/","title":"model","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE","title":"<code>DPVAE</code>","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\ndef __init__(\nself,\nlatent_dim,\nonehots=[[]],\nsingles=[],\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = len(singles) + sum([len(x) for x in onehots])\nself.singles = singles\nself.onehots = onehots\nself.net = nn.Sequential(\nnn.Linear(latent_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nlatent_dim,\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = 2 * latent_dim\nself.latent_dim = latent_dim\nself.net = nn.Sequential(\nnn.Linear(input_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\ndef __init__(self, num_singles):\nsuper().__init__()\nself.output_logsigma_fn = nn.Linear(num_singles, num_singles, bias=True)\ntorch.nn.init.zeros_(self.output_logsigma_fn.weight)\ntorch.nn.init.zeros_(self.output_logsigma_fn.bias)\nself.output_logsigma_fn.weight.requires_grad = False\ndef forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder.to(encoder.device)\nself.decoder = decoder.to(decoder.device)\nself.device = encoder.device\nself.onehots = self.decoder.onehots\nself.singles = self.decoder.singles\nself.noiser = Noiser(len(self.singles)).to(decoder.device)\ndef reconstruct(self, X):\nmu_z, logsigma_z = self.encoder(X)\nx_recon = self.decoder(mu_z)\nreturn x_recon\ndef generate(self, N):\nz_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\nx_gen = self.decoder(z_samples)\nx_gen_ = torch.ones_like(x_gen, device=self.device)\nfor cat_idxs in self.onehots:\nx_gen_[:, cat_idxs] = torch.distributions.one_hot_categorical.OneHotCategorical(\nlogits=x_gen[:, cat_idxs]\n).sample()\nx_gen_[:, self.singles] = x_gen[:, self.singles] + torch.exp(\nself.noiser(x_gen[:, self.singles])\n) * torch.randn_like(x_gen[:, self.singles])\nif torch.cuda.is_available():\nx_gen_ = x_gen_.cpu()\nreturn x_gen_.detach()\ndef loss(self, X):\nmu_z, logsigma_z = self.encoder(X)\np = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\nq = Normal(mu_z, torch.exp(logsigma_z))\nkld = torch.sum(torch.distributions.kl_divergence(q, p))\ns = torch.randn_like(mu_z)\nz_samples = mu_z + s * torch.exp(logsigma_z)\nx_recon = self.decoder(z_samples)\ncategoric_loglik = 0\nif len(self.onehots):\nfor cat_idxs in self.onehots:\ncategoric_loglik += -torch.nn.functional.cross_entropy(\nx_recon[:, cat_idxs],\ntorch.max(X[:, cat_idxs], 1)[1],\n).sum()\ngauss_loglik = 0\nif len(self.singles):\ngauss_loglik = (\nNormal(\nloc=x_recon[:, self.singles],\nscale=torch.exp(self.noiser(x_recon[:, self.singles])),\n)\n.log_prob(X[:, self.singles])\n.sum()\n)\nreconstruction_loss = -(categoric_loglik + gauss_loglik)\nelbo = kld + reconstruction_loss\nreturn (elbo, reconstruction_loss, kld, categoric_loglik, gauss_loglik)\ndef train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\n# EARLY STOPPING #\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nlog_elbo = []\nlog_reconstruct = []\nlog_divergence = []\nlog_cat_loss = []\nlog_num_loss = []\nstats_bar_1 = tqdm(total=0, desc=\"\", position=0, bar_format=\"{desc}\", leave=True)\nstats_bar_2 = tqdm(total=0, desc=\"\", position=1, bar_format=\"{desc}\", leave=True)\nstats_bar_3 = tqdm(total=0, desc=\"\", position=2, bar_format=\"{desc}\", leave=True)\nstats_bar_4 = tqdm(total=0, desc=\"\", position=3, bar_format=\"{desc}\", leave=True)\nstats_bar_5 = tqdm(total=0, desc=\"\", position=4, bar_format=\"{desc}\", leave=True)\nposition = 5\nif self.privacy_engine is not None:\nepsilon = []\nstats_bar_6 = tqdm(total=0, desc=\"\", position=5, bar_format=\"{desc}\", leave=True)\nposition += 1\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=position, leave=False):\nelbo_e = 0.0\nkld_e = 0.0\nreconstruction_e = 0.0\ncategorical_e = 0.0\nnumerical_e = 0.0\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=position + 1, leave=False):\nself.optimizer.zero_grad()\n(\nelbo,\nreconstruction_loss,\nkld,\ncategorical_loss,\nnumerical_loss,\n) = self.loss(Y_subset.to(self.encoder.device))\nelbo.backward()\nself.optimizer.step()\nelbo_e += elbo.item()\nkld_e += kld.item()\nreconstruction_e += reconstruction_loss.item()\ncategorical_e += categorical_loss.item()\nnumerical_e += numerical_loss.item()\nstats_bar_1.set_description_str(f\"ELBO: \\t\\t\\t{elbo_e:.2f}\")\nstats_bar_2.set_description_str(f\"KLD: \\t\\t\\t{kld_e:.2f}\")\nstats_bar_3.set_description_str(f\"Reconstruction Loss: \\t{reconstruction_e:.2f}\")\nstats_bar_4.set_description_str(f\"Categorical Loss: \\t{categorical_e:.2f}\")\nstats_bar_5.set_description_str(f\"Numerical Loss: \\t{numerical_e:.2f}\")\nif self.privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar_6.set_description_str(f\"Epsilon and Best Alpha: \\t{epsilon_e[0]:.2f}\\t{epsilon_e[1]:.2f}\")\nepsilon.append(epsilon_e)\nlog_elbo.append(elbo_e)\nlog_reconstruct.append(reconstruction_e)\nlog_divergence.append(kld_e)\nlog_cat_loss.append(categorical_e)\nlog_num_loss.append(numerical_e)\nif epoch == 0:\nmin_elbo = elbo_e\nif elbo_e &lt; (min_elbo - delta):\nmin_elbo = elbo_e\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nstats_bar_1.close()\nstats_bar_2.close()\nstats_bar_3.close()\nstats_bar_4.close()\nstats_bar_5.close()\nif privacy_engine is not None:\nstats_bar_6.close()\nreturn (\nnum_epochs,\nlog_elbo,\nlog_reconstruct,\nlog_divergence,\nlog_cat_loss,\nlog_num_loss,\n)\ndef get_privacy_spent(self, delta):\nif hasattr(self, \"privacy_engine\"):\nreturn self.privacy_engine.get_privacy_spent(delta)\nelse:\nprint(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n)\ndef save(self, filename):\ntorch.save(self.state_dict(), filename)\ndef load(self, filename):\nself.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, num_epochs, privacy_engine=None, patience=5, delta=10)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\n# EARLY STOPPING #\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nlog_elbo = []\nlog_reconstruct = []\nlog_divergence = []\nlog_cat_loss = []\nlog_num_loss = []\nstats_bar_1 = tqdm(total=0, desc=\"\", position=0, bar_format=\"{desc}\", leave=True)\nstats_bar_2 = tqdm(total=0, desc=\"\", position=1, bar_format=\"{desc}\", leave=True)\nstats_bar_3 = tqdm(total=0, desc=\"\", position=2, bar_format=\"{desc}\", leave=True)\nstats_bar_4 = tqdm(total=0, desc=\"\", position=3, bar_format=\"{desc}\", leave=True)\nstats_bar_5 = tqdm(total=0, desc=\"\", position=4, bar_format=\"{desc}\", leave=True)\nposition = 5\nif self.privacy_engine is not None:\nepsilon = []\nstats_bar_6 = tqdm(total=0, desc=\"\", position=5, bar_format=\"{desc}\", leave=True)\nposition += 1\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=position, leave=False):\nelbo_e = 0.0\nkld_e = 0.0\nreconstruction_e = 0.0\ncategorical_e = 0.0\nnumerical_e = 0.0\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=position + 1, leave=False):\nself.optimizer.zero_grad()\n(\nelbo,\nreconstruction_loss,\nkld,\ncategorical_loss,\nnumerical_loss,\n) = self.loss(Y_subset.to(self.encoder.device))\nelbo.backward()\nself.optimizer.step()\nelbo_e += elbo.item()\nkld_e += kld.item()\nreconstruction_e += reconstruction_loss.item()\ncategorical_e += categorical_loss.item()\nnumerical_e += numerical_loss.item()\nstats_bar_1.set_description_str(f\"ELBO: \\t\\t\\t{elbo_e:.2f}\")\nstats_bar_2.set_description_str(f\"KLD: \\t\\t\\t{kld_e:.2f}\")\nstats_bar_3.set_description_str(f\"Reconstruction Loss: \\t{reconstruction_e:.2f}\")\nstats_bar_4.set_description_str(f\"Categorical Loss: \\t{categorical_e:.2f}\")\nstats_bar_5.set_description_str(f\"Numerical Loss: \\t{numerical_e:.2f}\")\nif self.privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar_6.set_description_str(f\"Epsilon and Best Alpha: \\t{epsilon_e[0]:.2f}\\t{epsilon_e[1]:.2f}\")\nepsilon.append(epsilon_e)\nlog_elbo.append(elbo_e)\nlog_reconstruct.append(reconstruction_e)\nlog_divergence.append(kld_e)\nlog_cat_loss.append(categorical_e)\nlog_num_loss.append(numerical_e)\nif epoch == 0:\nmin_elbo = elbo_e\nif elbo_e &lt; (min_elbo - delta):\nmin_elbo = elbo_e\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nstats_bar_1.close()\nstats_bar_2.close()\nstats_bar_3.close()\nstats_bar_4.close()\nstats_bar_5.close()\nif privacy_engine is not None:\nstats_bar_6.close()\nreturn (\nnum_epochs,\nlog_elbo,\nlog_reconstruct,\nlog_divergence,\nlog_cat_loss,\nlog_num_loss,\n)\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.io","title":"<code>io</code>","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_base, fn_prepared, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_data</code> <p>The name of the data file.</p> required <code>fn_metadata</code> <p>The name of the metadata file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_input_paths(fn_base: str, fn_prepared: str, fn_metatransformer: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_data: The name of the data file.\n        fn_metadata: The name of the metadata file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_base, fn_prepared, fn_metatransformer = (\nconsistent_ending(fn_base),\nconsistent_ending(fn_prepared),\nconsistent_ending(fn_metatransformer),\n)\nfn_prepared, fn_metatransformer = (\npotential_suffix(fn_prepared, fn_base),\npotential_suffix(fn_metatransformer, fn_base),\n)\nwarn_if_path_supplied([fn_base, fn_prepared, fn_metatransformer], dir_experiment)\ncheck_exists([fn_prepared, fn_metatransformer], dir_experiment)\nreturn fn_base, fn_prepared, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.io.check_output_paths","title":"<code>check_output_paths(fn_base, fn_out, fn_model, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_model</code> <code>str</code> <p>The name of the model file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The path to output the model.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_output_paths(fn_base: Path, fn_out: str, fn_model: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_model: The name of the model file.\n        dir_experiment: The path to the experiment output directory.\n    Returns:\n        The path to output the model.\n    \"\"\"\nfn_out, fn_model = consistent_ending(fn_out, \".csv\"), consistent_ending(fn_model, \".pt\")\nfn_out, fn_model = potential_suffix(fn_out, fn_base), potential_suffix(fn_model, fn_base)\nwarn_if_path_supplied([fn_out, fn_model], dir_experiment)\nreturn fn_out, fn_model\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/model/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif getattr(args, \"dataloader_output\", None):\nreturn (\nargs.dataloader_output[\"fn_base\"],\nargs.dataloader_output[\"data\"],\nargs.dataloader_output[\"metatransformer\"],\n)\nelse:\nif not args.real_data:\nraise ValueError(\n\"You must provide `--real-data` when running this module on its own, please provide this (a prepared version and corresponding MetaTransformer must also exist in {dir_experiment})\"\n)\nfn_base, fn_prepared_data, fn_metatransformer = check_input_paths(\nargs.real_data, args.prepared_data, args.real_metatransformer, dir_experiment\n)\nwith open(dir_experiment / fn_prepared_data, \"rb\") as f:\ndata = pickle.load(f)\nwith open(dir_experiment / fn_metatransformer, \"rb\") as f:\nmt = pickle.load(f)\nreturn fn_base, data, mt\n</code></pre>"},{"location":"reference/modules/model/#nhssynth.modules.model.run","title":"<code>run</code>","text":""},{"location":"reference/modules/model/#nhssynth.modules.model.run.run","title":"<code>run(args)</code>","text":"<p>Run the model architecture module.</p> Source code in <code>modules/model/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"Run the model architecture module.\"\"\"\nprint(\"Running model architecture module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\nfn_base, data, mt = load_required_data(args, dir_experiment)\nonehots, singles = mt.get_onehots_and_singles()\nprint(onehots)\nprint(singles)\nprint(data.shape)\nnrows, ncols = data.shape\n# Should the data also all be turned into floats?\ntorch_data = TensorDataset(torch.Tensor(data.to_numpy()))\nsample_rate = args.batch_size / nrows\nmodel = VAE(\nEncoder(input_dim=ncols, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim, use_gpu=args.use_gpu),\nDecoder(args.latent_dim, onehots=onehots, singles=singles, use_gpu=args.use_gpu),\n)\nmodel.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\ndata_loader = DataLoader(\ntorch_data,\nbatch_sampler=UniformWithReplacementSampler(num_samples=nrows, sample_rate=sample_rate),\npin_memory=True,\n# batch_size=args.batch_size,\n)\nif not args.non_private_training:\nprivacy_engine = PrivacyEngine(\n# secure_rng=args.secure_rng,\nmodule=model,\nsample_rate=sample_rate,\nalphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\ntarget_epsilon=args.target_epsilon,\ntarget_delta=args.target_delta,\nepochs=args.num_epochs,\nmax_grad_norm=args.max_grad_norm,\n)\n# model.privacy_engine = PrivacyEngine(secure_mode=args.secure_rng)\n# model, optimizer, data_loader = model.privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=data_loader,\n#     epochs=args.num_epochs,\n#     target_epsilon=args.target_epsilon,\n#     target_delta=args.target_delta,\n#     max_grad_norm=args.max_grad_norm,\n# )\n# print(model)\n# print(f\"Using sigma={optimizer.noise_multiplier} and C={args.max_grad_norm}\")\nresults = model.train(data_loader, args.num_epochs, privacy_engine=privacy_engine)\nelse:\nresults = model.train(data_loader, args.num_epochs)\nsynthetic_data = pd.DataFrame(model.generate(nrows), columns=data.columns)\nfn_output, fn_model = check_output_paths(fn_base, args.synthetic_data, args.model_file, dir_experiment)\nif not args.discard_synthetic:\nsynthetic_data = mt.inverse_apply(synthetic_data)\nsynthetic_data.to_csv(dir_experiment / fn_output, index=False)\nif not args.discard_model:\nmodel.save(dir_experiment / fn_model)\nif args.modules_to_run and \"evaluation\" in args.modules_to_run:\nargs.model_output = {\"results\": results}\nreturn args\n</code></pre>"},{"location":"reference/modules/model/DPVAE/","title":"DPVAE","text":""},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Decoder","title":"<code> Decoder            (Module)         </code>","text":"<p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\ndef __init__(\nself,\nlatent_dim,\nonehots=[[]],\nsingles=[],\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = len(singles) + sum([len(x) for x in onehots])\nself.singles = singles\nself.onehots = onehots\nself.net = nn.Sequential(\nnn.Linear(latent_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Decoder.forward","title":"<code>forward(self, z)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, z):\nreturn self.net(z)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Encoder","title":"<code> Encoder            (Module)         </code>","text":"<p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\ndef __init__(\nself,\ninput_dim,\nlatent_dim,\nhidden_dim=32,\nactivation=nn.Tanh,\nuse_gpu=False,\n):\nsuper().__init__()\nself.device = setup_device(use_gpu)\noutput_dim = 2 * latent_dim\nself.latent_dim = latent_dim\nself.net = nn.Sequential(\nnn.Linear(input_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, hidden_dim),\nactivation(),\nnn.Linear(hidden_dim, output_dim),\n)\ndef forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Encoder.forward","title":"<code>forward(self, x)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, x):\nouts = self.net(x)\nmu_z = outs[:, : self.latent_dim]\nlogsigma_z = outs[:, self.latent_dim :]\nreturn mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Noiser","title":"<code> Noiser            (Module)         </code>","text":"Source code in <code>modules/model/DPVAE.py</code> <pre><code>class Noiser(nn.Module):\ndef __init__(self, num_singles):\nsuper().__init__()\nself.output_logsigma_fn = nn.Linear(num_singles, num_singles, bias=True)\ntorch.nn.init.zeros_(self.output_logsigma_fn.weight)\ntorch.nn.init.zeros_(self.output_logsigma_fn.bias)\nself.output_logsigma_fn.weight.requires_grad = False\ndef forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Noiser.forward","title":"<code>forward(self, X)</code>","text":"<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <p>.. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def forward(self, X):\nreturn self.output_logsigma_fn(X)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.VAE","title":"<code> VAE            (Module)         </code>","text":"<p>Combines encoder and decoder into full VAE model</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\ndef __init__(self, encoder, decoder):\nsuper().__init__()\nself.encoder = encoder.to(encoder.device)\nself.decoder = decoder.to(decoder.device)\nself.device = encoder.device\nself.onehots = self.decoder.onehots\nself.singles = self.decoder.singles\nself.noiser = Noiser(len(self.singles)).to(decoder.device)\ndef reconstruct(self, X):\nmu_z, logsigma_z = self.encoder(X)\nx_recon = self.decoder(mu_z)\nreturn x_recon\ndef generate(self, N):\nz_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\nx_gen = self.decoder(z_samples)\nx_gen_ = torch.ones_like(x_gen, device=self.device)\nfor cat_idxs in self.onehots:\nx_gen_[:, cat_idxs] = torch.distributions.one_hot_categorical.OneHotCategorical(\nlogits=x_gen[:, cat_idxs]\n).sample()\nx_gen_[:, self.singles] = x_gen[:, self.singles] + torch.exp(\nself.noiser(x_gen[:, self.singles])\n) * torch.randn_like(x_gen[:, self.singles])\nif torch.cuda.is_available():\nx_gen_ = x_gen_.cpu()\nreturn x_gen_.detach()\ndef loss(self, X):\nmu_z, logsigma_z = self.encoder(X)\np = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\nq = Normal(mu_z, torch.exp(logsigma_z))\nkld = torch.sum(torch.distributions.kl_divergence(q, p))\ns = torch.randn_like(mu_z)\nz_samples = mu_z + s * torch.exp(logsigma_z)\nx_recon = self.decoder(z_samples)\ncategoric_loglik = 0\nif len(self.onehots):\nfor cat_idxs in self.onehots:\ncategoric_loglik += -torch.nn.functional.cross_entropy(\nx_recon[:, cat_idxs],\ntorch.max(X[:, cat_idxs], 1)[1],\n).sum()\ngauss_loglik = 0\nif len(self.singles):\ngauss_loglik = (\nNormal(\nloc=x_recon[:, self.singles],\nscale=torch.exp(self.noiser(x_recon[:, self.singles])),\n)\n.log_prob(X[:, self.singles])\n.sum()\n)\nreconstruction_loss = -(categoric_loglik + gauss_loglik)\nelbo = kld + reconstruction_loss\nreturn (elbo, reconstruction_loss, kld, categoric_loglik, gauss_loglik)\ndef train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\n# EARLY STOPPING #\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nlog_elbo = []\nlog_reconstruct = []\nlog_divergence = []\nlog_cat_loss = []\nlog_num_loss = []\nstats_bar_1 = tqdm(total=0, desc=\"\", position=0, bar_format=\"{desc}\", leave=True)\nstats_bar_2 = tqdm(total=0, desc=\"\", position=1, bar_format=\"{desc}\", leave=True)\nstats_bar_3 = tqdm(total=0, desc=\"\", position=2, bar_format=\"{desc}\", leave=True)\nstats_bar_4 = tqdm(total=0, desc=\"\", position=3, bar_format=\"{desc}\", leave=True)\nstats_bar_5 = tqdm(total=0, desc=\"\", position=4, bar_format=\"{desc}\", leave=True)\nposition = 5\nif self.privacy_engine is not None:\nepsilon = []\nstats_bar_6 = tqdm(total=0, desc=\"\", position=5, bar_format=\"{desc}\", leave=True)\nposition += 1\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=position, leave=False):\nelbo_e = 0.0\nkld_e = 0.0\nreconstruction_e = 0.0\ncategorical_e = 0.0\nnumerical_e = 0.0\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=position + 1, leave=False):\nself.optimizer.zero_grad()\n(\nelbo,\nreconstruction_loss,\nkld,\ncategorical_loss,\nnumerical_loss,\n) = self.loss(Y_subset.to(self.encoder.device))\nelbo.backward()\nself.optimizer.step()\nelbo_e += elbo.item()\nkld_e += kld.item()\nreconstruction_e += reconstruction_loss.item()\ncategorical_e += categorical_loss.item()\nnumerical_e += numerical_loss.item()\nstats_bar_1.set_description_str(f\"ELBO: \\t\\t\\t{elbo_e:.2f}\")\nstats_bar_2.set_description_str(f\"KLD: \\t\\t\\t{kld_e:.2f}\")\nstats_bar_3.set_description_str(f\"Reconstruction Loss: \\t{reconstruction_e:.2f}\")\nstats_bar_4.set_description_str(f\"Categorical Loss: \\t{categorical_e:.2f}\")\nstats_bar_5.set_description_str(f\"Numerical Loss: \\t{numerical_e:.2f}\")\nif self.privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar_6.set_description_str(f\"Epsilon and Best Alpha: \\t{epsilon_e[0]:.2f}\\t{epsilon_e[1]:.2f}\")\nepsilon.append(epsilon_e)\nlog_elbo.append(elbo_e)\nlog_reconstruct.append(reconstruction_e)\nlog_divergence.append(kld_e)\nlog_cat_loss.append(categorical_e)\nlog_num_loss.append(numerical_e)\nif epoch == 0:\nmin_elbo = elbo_e\nif elbo_e &lt; (min_elbo - delta):\nmin_elbo = elbo_e\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nstats_bar_1.close()\nstats_bar_2.close()\nstats_bar_3.close()\nstats_bar_4.close()\nstats_bar_5.close()\nif privacy_engine is not None:\nstats_bar_6.close()\nreturn (\nnum_epochs,\nlog_elbo,\nlog_reconstruct,\nlog_divergence,\nlog_cat_loss,\nlog_num_loss,\n)\ndef get_privacy_spent(self, delta):\nif hasattr(self, \"privacy_engine\"):\nreturn self.privacy_engine.get_privacy_spent(delta)\nelse:\nprint(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n)\ndef save(self, filename):\ntorch.save(self.state_dict(), filename)\ndef load(self, filename):\nself.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.VAE.train","title":"<code>train(self, x_dataloader, num_epochs, privacy_engine=None, patience=5, delta=10)</code>","text":"<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>whether to set training mode (<code>True</code>) or evaluation          mode (<code>False</code>). Default: <code>True</code>.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>self</p> Source code in <code>modules/model/DPVAE.py</code> <pre><code>def train(\nself,\nx_dataloader: torch.utils.data.DataLoader,\nnum_epochs: int,\nprivacy_engine: opacus.PrivacyEngine = None,\npatience: int = 5,\ndelta: int = 10,\n):\nif privacy_engine is not None:\nself.privacy_engine = privacy_engine\nself.privacy_engine.attach(self.optimizer)\n# EARLY STOPPING #\nmin_elbo = 0.0  # For early stopping workflow\nstop_counter = 0  # Counter for stops\nlog_elbo = []\nlog_reconstruct = []\nlog_divergence = []\nlog_cat_loss = []\nlog_num_loss = []\nstats_bar_1 = tqdm(total=0, desc=\"\", position=0, bar_format=\"{desc}\", leave=True)\nstats_bar_2 = tqdm(total=0, desc=\"\", position=1, bar_format=\"{desc}\", leave=True)\nstats_bar_3 = tqdm(total=0, desc=\"\", position=2, bar_format=\"{desc}\", leave=True)\nstats_bar_4 = tqdm(total=0, desc=\"\", position=3, bar_format=\"{desc}\", leave=True)\nstats_bar_5 = tqdm(total=0, desc=\"\", position=4, bar_format=\"{desc}\", leave=True)\nposition = 5\nif self.privacy_engine is not None:\nepsilon = []\nstats_bar_6 = tqdm(total=0, desc=\"\", position=5, bar_format=\"{desc}\", leave=True)\nposition += 1\nfor epoch in tqdm(range(num_epochs), desc=\"Epochs\", position=position, leave=False):\nelbo_e = 0.0\nkld_e = 0.0\nreconstruction_e = 0.0\ncategorical_e = 0.0\nnumerical_e = 0.0\nfor (Y_subset,) in tqdm(x_dataloader, desc=\"Batches\", position=position + 1, leave=False):\nself.optimizer.zero_grad()\n(\nelbo,\nreconstruction_loss,\nkld,\ncategorical_loss,\nnumerical_loss,\n) = self.loss(Y_subset.to(self.encoder.device))\nelbo.backward()\nself.optimizer.step()\nelbo_e += elbo.item()\nkld_e += kld.item()\nreconstruction_e += reconstruction_loss.item()\ncategorical_e += categorical_loss.item()\nnumerical_e += numerical_loss.item()\nstats_bar_1.set_description_str(f\"ELBO: \\t\\t\\t{elbo_e:.2f}\")\nstats_bar_2.set_description_str(f\"KLD: \\t\\t\\t{kld_e:.2f}\")\nstats_bar_3.set_description_str(f\"Reconstruction Loss: \\t{reconstruction_e:.2f}\")\nstats_bar_4.set_description_str(f\"Categorical Loss: \\t{categorical_e:.2f}\")\nstats_bar_5.set_description_str(f\"Numerical Loss: \\t{numerical_e:.2f}\")\nif self.privacy_engine is not None:\nepsilon_e = self.privacy_engine.get_privacy_spent()\n# epsilon_e = self.privacy_engine.accountant.get_epsilon()\nstats_bar_6.set_description_str(f\"Epsilon and Best Alpha: \\t{epsilon_e[0]:.2f}\\t{epsilon_e[1]:.2f}\")\nepsilon.append(epsilon_e)\nlog_elbo.append(elbo_e)\nlog_reconstruct.append(reconstruction_e)\nlog_divergence.append(kld_e)\nlog_cat_loss.append(categorical_e)\nlog_num_loss.append(numerical_e)\nif epoch == 0:\nmin_elbo = elbo_e\nif elbo_e &lt; (min_elbo - delta):\nmin_elbo = elbo_e\nstop_counter = 0  # Set counter to zero\nelse:  # elbo has not improved\nstop_counter += 1\nif stop_counter == patience:\nnum_epochs = epoch + 1\nbreak\nstats_bar_1.close()\nstats_bar_2.close()\nstats_bar_3.close()\nstats_bar_4.close()\nstats_bar_5.close()\nif privacy_engine is not None:\nstats_bar_6.close()\nreturn (\nnum_epochs,\nlog_elbo,\nlog_reconstruct,\nlog_divergence,\nlog_cat_loss,\nlog_num_loss,\n)\n</code></pre>"},{"location":"reference/modules/model/io/","title":"io","text":""},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_base, fn_prepared, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_data</code> <p>The name of the data file.</p> required <code>fn_metadata</code> <p>The name of the metadata file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_input_paths(fn_base: str, fn_prepared: str, fn_metatransformer: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_data: The name of the data file.\n        fn_metadata: The name of the metadata file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\nfn_base, fn_prepared, fn_metatransformer = (\nconsistent_ending(fn_base),\nconsistent_ending(fn_prepared),\nconsistent_ending(fn_metatransformer),\n)\nfn_prepared, fn_metatransformer = (\npotential_suffix(fn_prepared, fn_base),\npotential_suffix(fn_metatransformer, fn_base),\n)\nwarn_if_path_supplied([fn_base, fn_prepared, fn_metatransformer], dir_experiment)\ncheck_exists([fn_prepared, fn_metatransformer], dir_experiment)\nreturn fn_base, fn_prepared, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.check_output_paths","title":"<code>check_output_paths(fn_base, fn_out, fn_model, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_model</code> <code>str</code> <p>The name of the model file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The path to output the model.</p> Source code in <code>modules/model/io.py</code> <pre><code>def check_output_paths(fn_base: Path, fn_out: str, fn_model: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n    Args:\n        fn_model: The name of the model file.\n        dir_experiment: The path to the experiment output directory.\n    Returns:\n        The path to output the model.\n    \"\"\"\nfn_out, fn_model = consistent_ending(fn_out, \".csv\"), consistent_ending(fn_model, \".pt\")\nfn_out, fn_model = potential_suffix(fn_out, fn_base), potential_suffix(fn_model, fn_base)\nwarn_if_path_supplied([fn_out, fn_model], dir_experiment)\nreturn fn_out, fn_model\n</code></pre>"},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>The data, metadata and metatransformer.</p> Source code in <code>modules/model/io.py</code> <pre><code>def load_required_data(\nargs: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\nif getattr(args, \"dataloader_output\", None):\nreturn (\nargs.dataloader_output[\"fn_base\"],\nargs.dataloader_output[\"data\"],\nargs.dataloader_output[\"metatransformer\"],\n)\nelse:\nif not args.real_data:\nraise ValueError(\n\"You must provide `--real-data` when running this module on its own, please provide this (a prepared version and corresponding MetaTransformer must also exist in {dir_experiment})\"\n)\nfn_base, fn_prepared_data, fn_metatransformer = check_input_paths(\nargs.real_data, args.prepared_data, args.real_metatransformer, dir_experiment\n)\nwith open(dir_experiment / fn_prepared_data, \"rb\") as f:\ndata = pickle.load(f)\nwith open(dir_experiment / fn_metatransformer, \"rb\") as f:\nmt = pickle.load(f)\nreturn fn_base, data, mt\n</code></pre>"},{"location":"reference/modules/model/run/","title":"run","text":""},{"location":"reference/modules/model/run/#nhssynth.modules.model.run.run","title":"<code>run(args)</code>","text":"<p>Run the model architecture module.</p> Source code in <code>modules/model/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"Run the model architecture module.\"\"\"\nprint(\"Running model architecture module...\")\nset_seed(args.seed)\ndir_experiment = experiment_io(args.experiment_name)\nfn_base, data, mt = load_required_data(args, dir_experiment)\nonehots, singles = mt.get_onehots_and_singles()\nprint(onehots)\nprint(singles)\nprint(data.shape)\nnrows, ncols = data.shape\n# Should the data also all be turned into floats?\ntorch_data = TensorDataset(torch.Tensor(data.to_numpy()))\nsample_rate = args.batch_size / nrows\nmodel = VAE(\nEncoder(input_dim=ncols, latent_dim=args.latent_dim, hidden_dim=args.hidden_dim, use_gpu=args.use_gpu),\nDecoder(args.latent_dim, onehots=onehots, singles=singles, use_gpu=args.use_gpu),\n)\nmodel.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\ndata_loader = DataLoader(\ntorch_data,\nbatch_sampler=UniformWithReplacementSampler(num_samples=nrows, sample_rate=sample_rate),\npin_memory=True,\n# batch_size=args.batch_size,\n)\nif not args.non_private_training:\nprivacy_engine = PrivacyEngine(\n# secure_rng=args.secure_rng,\nmodule=model,\nsample_rate=sample_rate,\nalphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\ntarget_epsilon=args.target_epsilon,\ntarget_delta=args.target_delta,\nepochs=args.num_epochs,\nmax_grad_norm=args.max_grad_norm,\n)\n# model.privacy_engine = PrivacyEngine(secure_mode=args.secure_rng)\n# model, optimizer, data_loader = model.privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=data_loader,\n#     epochs=args.num_epochs,\n#     target_epsilon=args.target_epsilon,\n#     target_delta=args.target_delta,\n#     max_grad_norm=args.max_grad_norm,\n# )\n# print(model)\n# print(f\"Using sigma={optimizer.noise_multiplier} and C={args.max_grad_norm}\")\nresults = model.train(data_loader, args.num_epochs, privacy_engine=privacy_engine)\nelse:\nresults = model.train(data_loader, args.num_epochs)\nsynthetic_data = pd.DataFrame(model.generate(nrows), columns=data.columns)\nfn_output, fn_model = check_output_paths(fn_base, args.synthetic_data, args.model_file, dir_experiment)\nif not args.discard_synthetic:\nsynthetic_data = mt.inverse_apply(synthetic_data)\nsynthetic_data.to_csv(dir_experiment / fn_output, index=False)\nif not args.discard_model:\nmodel.save(dir_experiment / fn_model)\nif args.modules_to_run and \"evaluation\" in args.modules_to_run:\nargs.model_output = {\"results\": results}\nreturn args\n</code></pre>"},{"location":"reference/modules/plotting/","title":"plotting","text":""},{"location":"reference/modules/plotting/plot/","title":"plot","text":""},{"location":"reference/modules/plotting/run/","title":"run","text":""},{"location":"reference/modules/structure/","title":"structure","text":""},{"location":"reference/modules/structure/run/","title":"run","text":""}]}
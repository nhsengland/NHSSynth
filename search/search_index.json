{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NHS Synth","text":"<p>Under construction, see the Code Reference or Model Card.</p>"},{"location":"model_card/","title":"Model Card: Variational AutoEncoder with Differential Privacy","text":"<p>Following Model Cards for Model Reporting (Mitchell et al.) and Lessons from Archives (Jo &amp; Gebru), we're providing some information about about the Variational AutoEncoder (VAE) with Differential Privacy within this repository.</p>"},{"location":"model_card/#model-details","title":"Model Details","text":"<p>The implementation of the Variational AutoEncoder (VAE) with Differential Privacy within this repository was created as part of an NHSX Analytics Unit PhD internship project undertaken by Dominic Danks (last commit to the repository: commit 88a4bdf). This model card describes the updated version of the model, released in March 2022.  Further information about the previous version created by Dom and its model implementation can be found in Section 5.4 of the associated report.</p>"},{"location":"model_card/#model-use","title":"Model Use","text":""},{"location":"model_card/#intended-use","title":"Intended Use","text":"<p>This model is intended for use in experimenting with differential privacy and VAEs.</p>"},{"location":"model_card/#training-data","title":"Training Data","text":"<p>Experiments in this repository are run against the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) dataset accessed via the pycox python library. We also performed further analysis on a single table that we extracted from MIMIC-III.</p>"},{"location":"model_card/#performance-and-limitations","title":"Performance and Limitations","text":"<p>A from-scratch VAE implementation was compared against various models available within the SDV framework using a variety of quality and privacy metrics on the SUPPORT dataset. The VAE was found to be competitive with all of these models across the various metrics. Differential Privacy (DP) was introduced via DP-SGD and the performance of the VAE for different levels of privacy was evaluated. It was found that as the level of Differential Privacy introduced by DP-SGD was increased, it became easier to distinguish between synthetic and real data.</p> <p>Proper evaluation of quality and privacy of synthetic data is challenging. In this work, we utilised metrics from the SDV library due to their natural integration with the rest of the codebase. A valuable extension of this work would be to apply a variety of external metrics, including more advanced adversarial attacks to more thoroughly evaluate the privacy of the considered methods, including as the level of DP is varied. It would also be of interest to apply DP-SGD and/or PATE to all of the considered methods and evaluate whether the performance drop as a function of implemented privacy is similar or different across the models.</p> <p>Currently the SynthVAE model only works for data which is 'clean'. I.e data that has no missingness or NaNs within its input. It can handle continuous, categorical and datetime variables. Special types such as nominal data cannot be handled properly however the model may still run. Column names have to be specified in the code for the variable group they belong to.</p> <p>Hyperparameter tuning of the model can result in errors if certain parameter values are selected. Most commonly, changing learning rate in our example results in errors during training. An extensive test to evaluate plausible ranges has not been performed as of yet. If you get errors during tuning then consider your hyperparameter values and adjust accordingly.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cli<ul> <li>config</li> <li>module_arguments</li> <li>module_setup</li> <li>run</li> </ul> </li> <li>common<ul> <li>common</li> <li>constants</li> <li>dicts</li> <li>io</li> </ul> </li> <li>modules<ul> <li>dataloader<ul> <li>io</li> <li>metadata</li> <li>metatransformer</li> <li>run</li> </ul> </li> <li>evaluation<ul> <li>metrics</li> <li>run</li> </ul> </li> <li>model<ul> <li>DPVAE</li> <li>io</li> <li>run</li> </ul> </li> <li>plotting<ul> <li>plot</li> <li>run</li> </ul> </li> <li>structure<ul> <li>run</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/cli/","title":"cli","text":""},{"location":"reference/cli/config/","title":"config","text":""},{"location":"reference/cli/config/#nhssynth.cli.config.assemble_config","title":"<code>assemble_config(args, all_subparsers)</code>","text":"<p>Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>A namespace object containing all parsed command-line arguments.</p> required <code>all_subparsers</code> <code>dict[str, argparse.ArgumentParser]</code> <p>A dictionary mapping module names to subparser objects.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing configuration information extracted from <code>args</code> in a module-wise nested format that is YAML-friendly.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a module specified in <code>args.modules_to_run</code> is not in <code>all_subparsers</code>.</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def assemble_config(\n    args: argparse.Namespace,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; dict[str, Any]:\n\"\"\"\n    Assemble and arrange a module-wise nested configuration dictionary from parsed command-line arguments to be output as a YAML record.\n\n    Args:\n        args: A namespace object containing all parsed command-line arguments.\n        all_subparsers: A dictionary mapping module names to subparser objects.\n\n    Returns:\n        A dictionary containing configuration information extracted from `args` in a module-wise nested format that is YAML-friendly.\n\n    Raises:\n        ValueError: If a module specified in `args.modules_to_run` is not in `all_subparsers`.\n    \"\"\"\n    args_dict = vars(args)\n    modules_to_run = args_dict[\"modules_to_run\"]\n    if len(modules_to_run) == 1:\n        run_type = modules_to_run[0]\n    elif modules_to_run == PIPELINE:\n        run_type = \"pipeline\"\n    else:\n        raise ValueError(f\"Invalid value for `modules_to_run`: {modules_to_run}\")\n\n    # Generate a dictionary containing each module's name from the run, with all of its possible corresponding config args\n    module_args = {\n        module_name: [action.dest for action in all_subparsers[module_name]._actions if action.dest != \"help\"]\n        for module_name in modules_to_run\n    }\n\n    # Use the flat namespace to populate a nested (by module) dictionary of config args and values\n    out_dict = {}\n    for module_name in modules_to_run:\n        for k in args_dict.copy().keys():\n            if k in module_args[module_name]:\n                if out_dict.get(module_name):\n                    out_dict[module_name].update({k: args_dict.pop(k)})\n                else:\n                    out_dict[module_name] = {k: args_dict.pop(k)}\n\n    # Assemble the final dictionary in YAML-compliant form\n    return {\n        **({\"run_type\": run_type} if run_type else {}),\n        **{\n            k: v\n            for k, v in args_dict.items()\n            if k not in {\"func\", \"experiment_name\", \"save_config\", \"save_config_path\"}\n        },\n        **out_dict,\n    }\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_default_and_required_args","title":"<code>get_default_and_required_args(top_parser, module_parsers)</code>","text":"<p>Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.</p> <p>Parameters:</p> Name Type Description Default <code>top_parser</code> <code>argparse.ArgumentParser</code> <p>The top-level parser.</p> required <code>module_parsers</code> <code>dict[str, argparse.ArgumentParser]</code> <p>The dict of module-level parsers mapped to their names.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], list[str]]</code> <p>A tuple containing two elements: - A dictionary containing all arguments and their default values. - A list of kvps of the required arguments and their associated module.</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def get_default_and_required_args(\n    top_parser: argparse.ArgumentParser,\n    module_parsers: dict[str, argparse.ArgumentParser],\n) -&gt; tuple[dict[str, Any], list[str]]:\n\"\"\"\n    Get the default and required arguments for the top-level parser and the current run's corresponding list of module parsers.\n\n    Args:\n        top_parser: The top-level parser.\n        module_parsers: The dict of module-level parsers mapped to their names.\n\n    Returns:\n        A tuple containing two elements:\n            - A dictionary containing all arguments and their default values.\n            - A list of kvps of the required arguments and their associated module.\n    \"\"\"\n    all_actions = {\n        \"top-level\": top_parser._actions,\n        **{m: p._actions for m, p in module_parsers.items()},\n    }\n    defaults = {}\n    required_args = []\n    for module, actions in all_actions.items():\n        for action in actions:\n            if action.dest not in [\"help\", \"==SUPPRESS==\"]:\n                defaults[action.dest] = action.default\n                if action.required:\n                    required_args.append({\"arg\": action.dest, \"module\": module})\n    return defaults, required_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.get_modules_to_run","title":"<code>get_modules_to_run(executor)</code>","text":"<p>Get the list of modules to run from the passed executor function.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Callable</code> <p>The executor function to run.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of module names to run.</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def get_modules_to_run(executor: Callable) -&gt; list[str]:\n\"\"\"\n    Get the list of modules to run from the passed executor function.\n\n    Args:\n        executor: The executor function to run.\n\n    Returns:\n        A list of module names to run.\n    \"\"\"\n    if executor == run_pipeline:\n        return PIPELINE\n    else:\n        return [get_key_by_value({mn: mc.func for mn, mc in MODULE_MAP.items()}, executor)]\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.read_config","title":"<code>read_config(args, parser, all_subparsers)</code>","text":"<p>Hierarchically assembles a config Namespace object for the inferred modules to run and executes.</p> <ol> <li>Load the YAML file containing the config to read from</li> <li>Check a valid <code>run_type</code> is specified or infer it and determine the list of <code>modules_to_run</code></li> <li>Establish the appropriate default config from the parser and <code>all_subparsers</code> for the <code>modules_to_run</code></li> <li>Overwrite this config with the specified subset (or full set) of config in the YAML file</li> <li>Overwrite again with passed command-line <code>args</code> (these are considered 'overrides')</li> <li>Run the appropriate module(s) or pipeline with the resulting config</li> </ol> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>Namespace object containing arguments from the command line</p> required <code>parser</code> <code>argparse.ArgumentParser</code> <p>top-level ArgumentParser object</p> required <code>all_subparsers</code> <code>dict[str, argparse.ArgumentParser]</code> <p>dictionary of ArgumentParser objects, one for each module</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>Namespace object containing the assembled configuration settings</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if any required arguments are missing from the configuration file</p> Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def read_config(\n    args: argparse.Namespace,\n    parser: argparse.ArgumentParser,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; argparse.Namespace:\n\"\"\"\n    Hierarchically assembles a config Namespace object for the inferred modules to run and executes.\n\n    1. Load the YAML file containing the config to read from\n    2. Check a valid `run_type` is specified or infer it and determine the list of `modules_to_run`\n    3. Establish the appropriate default config from the parser and `all_subparsers` for the `modules_to_run`\n    4. Overwrite this config with the specified subset (or full set) of config in the YAML file\n    5. Overwrite again with passed command-line `args` (these are considered 'overrides')\n    6. Run the appropriate module(s) or pipeline with the resulting config\n\n    Args:\n        args: Namespace object containing arguments from the command line\n        parser: top-level ArgumentParser object\n        all_subparsers: dictionary of ArgumentParser objects, one for each module\n\n    Returns:\n        Namespace object containing the assembled configuration settings\n\n    Raises:\n        AssertionError: if any required arguments are missing from the configuration file\n    \"\"\"\n    # Open the passed yaml file and load into a dictionary\n    with open(f\"config/{args.input_config}.yaml\") as stream:\n        config_dict = yaml.safe_load(stream)\n\n    valid_run_types = [x for x in all_subparsers.keys() if x != \"config\"]\n\n    run_type = config_dict.pop(\"run_type\", None)\n\n    if run_type == \"pipeline\":\n        modules_to_run = PIPELINE\n    else:\n        modules_to_run = [x for x in config_dict.keys() | {run_type} if x in valid_run_types]\n        if not args.custom_pipeline:\n            modules_to_run = sorted(modules_to_run, key=lambda x: PIPELINE.index(x))\n\n    if not modules_to_run:\n        warnings.warn(\n            \"Missing or invalid `run_type` and / or module specification hierarchy in `config/{args.input_config}.yaml`, defaulting to a full run of the pipeline\"\n        )\n        modules_to_run = PIPELINE\n\n    # Get all possible default arguments by scraping the top level `parser` and the appropriate sub-parser for the `run_type`\n    args_dict, required_args = get_default_and_required_args(\n        parser, filter_dict(all_subparsers, modules_to_run, include=True)\n    )\n\n    # Find the non-default arguments amongst passed `args` by seeing which of them are different to the entries of `args_dict`\n    non_default_passed_args_dict = {\n        k: v\n        for k, v in vars(args).items()\n        if k in [\"input_config\", \"custom_pipeline\"] or (k in args_dict and k != \"func\" and v != args_dict[k])\n    }\n\n    # Overwrite the default arguments with the ones from the yaml file\n    args_dict.update(flatten_dict(config_dict))\n\n    # Overwrite the result of the above with any non-default CLI args\n    args_dict.update(non_default_passed_args_dict)\n\n    # Create a new Namespace using the assembled dictionary\n    new_args = argparse.Namespace(**args_dict)\n    assert all(\n        getattr(new_args, req_arg[\"arg\"]) for req_arg in required_args\n    ), f\"Required arguments are missing from the passed config file: {[ra['module'] + ':' + ra['arg'] for ra in required_args if not getattr(new_args, ra['arg'])]}\"\n\n    # Run the appropriate execution function(s)\n    new_args.modules_to_run = modules_to_run\n    for module in new_args.modules_to_run:\n        MODULE_MAP[module].func(new_args)\n\n    return new_args\n</code></pre>"},{"location":"reference/cli/config/#nhssynth.cli.config.write_config","title":"<code>write_config(args, all_subparsers)</code>","text":"<p>Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by <code>args.save_config_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>A namespace containing the run's configuration.</p> required <code>all_subparsers</code> <code>dict[str, argparse.ArgumentParser]</code> <p>A dictionary containing all subparsers for the config args.</p> required Source code in <code>src/nhssynth/cli/config.py</code> <pre><code>def write_config(\n    args: argparse.Namespace,\n    all_subparsers: dict[str, argparse.ArgumentParser],\n) -&gt; None:\n\"\"\"\n    Assembles a configuration dictionary from the run config and writes it to a YAML file at the location specified by `args.save_config_path`.\n\n    Args:\n        args: A namespace containing the run's configuration.\n        all_subparsers: A dictionary containing all subparsers for the config args.\n    \"\"\"\n    if args.sdv_workflow:\n        del args.synthesizer\n    args_dict = assemble_config(args, all_subparsers)\n    with open(f\"{args.save_config_path}\", \"w\") as yaml_file:\n        yaml.dump(args_dict, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/cli/module_arguments/","title":"module_arguments","text":""},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_dataloader_args","title":"<code>add_dataloader_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing dataloader module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_dataloader_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing dataloader module sub-parser instance.\"\"\"\n    parser.add_argument(\n        \"-i\",\n        \"--input\",\n        type=str,\n        required=(not override),\n        help=\"the name of the `.csv` file to prepare\",\n    )\n    parser.add_argument(\n        \"--output\",\n        type=str,\n        default=\"_prepared\",\n        help=\"where to write the prepared data, defaults to `experiments/&lt;args.experiment_name&gt;/&lt;args.input_file&gt;_prepared\\{.csv/.pkl\\}`, only used when `--write-all` is provided and/or this is a full pipeline run / one that involves the `model` module\",\n    )\n    parser.add_argument(\n        \"--metadata\",\n        type=str,\n        default=\"_metadata\",\n        help=\"metadata for the input data, defaults to `&lt;args.input_dir&gt;/&lt;args.input_file&gt;_metadata.yaml`\",\n    )\n    parser.add_argument(\n        \"--discard-metadata\",\n        action=\"store_true\",\n        help=\"discard the generated metadata file (not recommended, this is required for reproducibility)\",\n    )\n    parser.add_argument(\n        \"--metatransformer\",\n        type=str,\n        default=\"_metatransformer\",\n        help=\"name of the file to dump the `metatransformer` object used on the input data, defaults to `experiments/&lt;args.experiment_name&gt;/&lt;args.input_file&gt;_metatransformer.pkl`, only used when `--write-all` is provided and/or this is a full pipeline run / one that involves the `model` module\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--data-dir\",\n        type=str,\n        default=\"./data\",\n        help=\"the directory to read and write data from and to\",\n    )\n    parser.add_argument(\n        \"--index-col\",\n        default=None,\n        choices=[None, 0],\n        help=\"indicate whether the csv file's 0th column is an index column, such that pandas can ignore it\",\n    )\n    parser.add_argument(\n        \"--sdv-workflow\",\n        action=\"store_true\",\n        help=\"utilise the SDV synthesizer workflow for transformation and metadata, rather than a `HyperTransformer` from RDT\",\n    )\n    parser.add_argument(\n        \"--allow-null-transformers\",\n        action=\"store_true\",\n        help=\"allow null / None transformers, i.e. leave some columns as they are\",\n    )\n    parser.add_argument(\n        \"--collapse-yaml\",\n        action=\"store_true\",\n        help=\"use aliases and anchors in the output metadata yaml, this will make it much more compact\",\n    )\n    # TODO might be good to have something like this, needs some thought in how to only apply to appropriate transformers, without overriding metadata\n    # parser.add_argument(\n    #     \"--imputation-strategy\",\n    #     default=None,\n    #     help=\"imputation strategy for missing values, pick one of None, `mean`, `mode`, or a number\",\n    # )\n    parser.add_argument(\n        \"-s\",\n        \"--synthesizer\",\n        type=str,\n        default=\"TVAE\",\n        choices=list(SDV_SYNTHESIZER_CHOICES.keys()),\n        help=\"pick a synthesizer to use (note this can also be specified in the model module, these must match)\",\n    )\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_evaluation_args","title":"<code>add_evaluation_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing evaluation module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_evaluation_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing evaluation module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_model_args","title":"<code>add_model_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing model module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_model_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing model module sub-parser instance.\"\"\"\n    parser.add_argument(\n        \"-r\",\n        \"--real-data\",\n        type=str,\n        help=\"name of the dataset, only REQUIRED when this module is run on its own\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--prepared-data\",\n        type=str,\n        default=\"_prepared\",\n        help=\"name of the prepared dataset to load from `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_prepared.pkl`, only REQUIRED when this module is run on its own\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--real-metatransformer\",\n        default=\"_metatransformer\",\n        type=str,\n        help=\"name of the `.pkl` file of the MetaTransformer used on the prepared data to load from `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_metatransformer.pkl` only REQUIRED when this module is run on its own\",\n    )\n    parser.add_argument(\n        \"--model-file\",\n        type=str,\n        default=\"_model\",\n        help=\"specify the filename of the model to be saved in `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_model.pt`\",\n    )\n    parser.add_argument(\n        \"--discard-model\",\n        action=\"store_true\",\n        help=\"discard the model after training\",\n    )\n    parser.add_argument(\n        \"--synthetic-data\",\n        type=str,\n        default=\"_synthetic\",\n        help=\"specify the filename of the synthetic data to be written in `experiments/&lt;args.experiment_name&gt;/`, defaults to `&lt;args.real_data&gt;_synthetic.csv`\",\n    )\n    parser.add_argument(\n        \"--discard-synthetic\",\n        action=\"store_true\",\n        help=\"do not output the synthetic data generated during the run\",\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=32,\n        help=\"the batch size for the model\",\n    )\n    parser.add_argument(\n        \"--latent-dim\",\n        type=int,\n        default=256,\n        help=\"the latent dimension of the model\",\n    )\n    parser.add_argument(\n        \"--hidden-dim\",\n        type=int,\n        default=256,\n        help=\"the hidden dimension of the model\",\n    )\n    parser.add_argument(\n        \"--num-epochs\",\n        type=int,\n        default=100,\n        help=\"number of epochs to train for\",\n    )\n    parser.add_argument(\n        \"--patience\",\n        type=int,\n        default=5,\n        help=\"how many epochs the model is allowed to train for without improvement\",\n    )\n    parser.add_argument(\n        \"--delta\",\n        type=int,\n        default=10,\n        help=\"the difference in successive ELBO values that register as an 'improvement'\",\n    )\n    parser.add_argument(\n        \"--non-private-training\",\n        action=\"store_true\",\n        help=\"train the model in a non-private way\",\n    )\n    parser.add_argument(\n        \"--max-grad-norm\",\n        type=int,\n        default=10,\n        help=\"the clipping threshold for gradients (only relevant under differential privacy)\",\n    )\n    parser.add_argument(\n        \"--target-epsilon\",\n        type=float,\n        default=1.0,\n        help=\"the target epsilon for differential privacy\",\n    )\n    parser.add_argument(\n        \"--target-delta\",\n        type=float,\n        default=1e-5,\n        help=\"the target delta for differential privacy\",\n    )\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_plotting_args","title":"<code>add_plotting_args(parser, override=False)</code>","text":"<p>Adds arguments to an existing plotting module sub-parser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_plotting_args(parser: argparse.ArgumentParser, override=False) -&gt; None:\n\"\"\"Adds arguments to an existing plotting module sub-parser instance.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cli/module_arguments/#nhssynth.cli.module_arguments.add_top_level_args","title":"<code>add_top_level_args(parser)</code>","text":"<p>Adds top-level arguments to an existing ArgumentParser instance.</p> Source code in <code>src/nhssynth/cli/module_arguments.py</code> <pre><code>def add_top_level_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds top-level arguments to an existing ArgumentParser instance.\"\"\"\n    parser.add_argument(\n        \"-e\",\n        \"--experiment-name\",\n        type=str,\n        default=TIME,\n        help=f\"name the experiment run to affect logging, config, and default-behaviour io, defaults to current time, i.e. `{TIME}`\",\n    )\n    parser.add_argument(\n        \"-sc\",\n        \"--save-config\",\n        action=\"store_true\",\n        help=\"save the config provided via the cli\",\n    )\n    parser.add_argument(\n        \"--save-config-path\",\n        type=str,\n        help=\"where to save the config when `-sc` is provided, defaults to `experiments/&lt;experiment_name&gt;/config_&lt;experiment_name&gt;.yaml`\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--seed\",\n        type=int,\n        help=\"specify a seed for reproducibility\",\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--write-all\",\n        action=\"store_true\",\n        help=\"write all outputs to disk, including those that are not strictly necessary i.e. intermediary outputs in a full pipeline run\",\n    )\n</code></pre>"},{"location":"reference/cli/module_setup/","title":"module_setup","text":""},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig","title":"<code>ModuleConfig</code>","text":"Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>class ModuleConfig:\n    def __init__(\n        self,\n        func: Callable[..., Any],\n        add_args_func: Callable[..., Any],\n        description: str,\n        help: str,\n    ) -&gt; None:\n\"\"\"\n        Represents a module's configuration, containing the following attributes:\n\n        Args:\n            func: A callable that executes the module's functionality.\n            add_args_func: A callable that populates the module's sub-parser arguments.\n            description: A description of the module's functionality.\n            help: A help message for the module's command-line interface.\n        \"\"\"\n        self.func = func\n        self.add_args_func = add_args_func\n        self.description = description\n        self.help = help\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.ModuleConfig.__init__","title":"<code>__init__(func, add_args_func, description, help)</code>","text":"<p>Represents a module's configuration, containing the following attributes:</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>A callable that executes the module's functionality.</p> required <code>add_args_func</code> <code>Callable[..., Any]</code> <p>A callable that populates the module's sub-parser arguments.</p> required <code>description</code> <code>str</code> <p>A description of the module's functionality.</p> required <code>help</code> <code>str</code> <p>A help message for the module's command-line interface.</p> required Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def __init__(\n    self,\n    func: Callable[..., Any],\n    add_args_func: Callable[..., Any],\n    description: str,\n    help: str,\n) -&gt; None:\n\"\"\"\n    Represents a module's configuration, containing the following attributes:\n\n    Args:\n        func: A callable that executes the module's functionality.\n        add_args_func: A callable that populates the module's sub-parser arguments.\n        description: A description of the module's functionality.\n        help: A help message for the module's command-line interface.\n    \"\"\"\n    self.func = func\n    self.add_args_func = add_args_func\n    self.description = description\n    self.help = help\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_config_args","title":"<code>add_config_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> relating to configuration file handling and module-specific config overrides.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def add_config_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` relating to configuration file handling and module-specific config overrides.\"\"\"\n    parser.add_argument(\n        \"-c\",\n        \"--input-config\",\n        required=True,\n        help=\"specify the config file name\",\n    )\n    parser.add_argument(\n        \"-cp\",\n        \"--custom-pipeline\",\n        action=\"store_true\",\n        help=\"infer a custom pipeline running order of modules from the config\",\n    )\n    for module_name in VALID_MODULES:\n        group = parser.add_argument_group(title=f\"{module_name} overrides\")\n        MODULE_MAP[module_name].add_args_func(group, override=True)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_pipeline_args","title":"<code>add_pipeline_args(parser)</code>","text":"<p>Adds arguments to a <code>parser</code> for each module in the pipeline.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def add_pipeline_args(parser: argparse.ArgumentParser) -&gt; None:\n\"\"\"Adds arguments to a `parser` for each module in the pipeline.\"\"\"\n    for module_name in PIPELINE:\n        group = parser.add_argument_group(title=module_name)\n        MODULE_MAP[module_name].add_args_func(group)\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.add_subparser","title":"<code>add_subparser(subparsers, name, config)</code>","text":"<p>Add a subparser to an argparse argument parser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The subparsers action to which the subparser will be added.</p> required <code>name</code> <code>str</code> <p>The name of the subparser.</p> required <code>config</code> <code>ModuleConfig</code> <p>A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.</p> required <p>Returns:</p> Type Description <code>argparse.ArgumentParser</code> <p>The newly created subparser.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def add_subparser(\n    subparsers: argparse._SubParsersAction,\n    name: str,\n    config: ModuleConfig,\n) -&gt; argparse.ArgumentParser:\n\"\"\"\n    Add a subparser to an argparse argument parser.\n\n    Args:\n        subparsers: The subparsers action to which the subparser will be added.\n        name: The name of the subparser.\n        config: A ModuleConfig object containing information about the subparser, including a function to execute and a function to add arguments.\n\n    Returns:\n        The newly created subparser.\n    \"\"\"\n    parser = subparsers.add_parser(\n        name=name,\n        description=config.description,\n        help=config.help,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    config.add_args_func(parser)\n    parser.set_defaults(func=config.func)\n    return parser\n</code></pre>"},{"location":"reference/cli/module_setup/#nhssynth.cli.module_setup.run_pipeline","title":"<code>run_pipeline(args)</code>","text":"<p>Runs the specified pipeline of modules with the passed configuration <code>args</code>.</p> Source code in <code>src/nhssynth/cli/module_setup.py</code> <pre><code>def run_pipeline(args: argparse.Namespace) -&gt; None:\n\"\"\"Runs the specified pipeline of modules with the passed configuration `args`.\"\"\"\n    print(\"Running full pipeline...\")\n    args.modules_to_run = PIPELINE\n    for module_name in PIPELINE:\n        args = MODULE_MAP[module_name].func(args)\n</code></pre>"},{"location":"reference/cli/run/","title":"run","text":""},{"location":"reference/cli/run/#nhssynth.cli.run.run","title":"<code>run()</code>","text":"<p>CLI for preparing, training and evaluating a synthetic data generator.</p> Source code in <code>src/nhssynth/cli/run.py</code> <pre><code>def run() -&gt; None:\n\"\"\"CLI for preparing, training and evaluating a synthetic data generator.\"\"\"\n\n    parser = argparse.ArgumentParser(\n        prog=\"nhssynth\", description=\"CLI for preparing, training and evaluating a synthetic data generator.\"\n    )\n    add_top_level_args(parser)\n\n    # Below we instantiate one subparser for each module + one for running with a config file and one for\n    # doing a full pipeline run with CLI-specified config\n    subparsers = parser.add_subparsers()\n\n    # TODO can probably do this better as we dont actually need the `pipeline` or `config` subparsers in this dict\n    all_subparsers = {\n        name: add_subparser(subparsers, name, option_config) for name, option_config in MODULE_MAP.items()\n    }\n\n    args = parser.parse_args()\n\n    # Use get to return None when no function has been set, i.e. user made no running choice\n    executor = vars(args).get(\"func\")\n\n    # If `config` is the specified running choice, we mutate `args` in `read_config`\n    # else we execute according to the user's choice\n    # else we return `--help` if no choice has been passed, i.e. executor is None\n    if not executor:\n        args = read_config(args, parser, all_subparsers)\n    elif executor:\n        args.modules_to_run = get_modules_to_run(executor)\n        executor(args)\n    else:\n        parser.parse_args([\"--help\"])\n\n    # Whenever either are specified, we want to dump the configuration to allow for this run to be replicated\n    if args.save_config or args.save_config_path:\n        if not args.save_config_path:\n            args.save_config_path = f\"experiments/{args.experiment_name}/config_{args.experiment_name}.yaml\"\n        write_config(args, all_subparsers)\n\n    print(\"Complete!\")\n</code></pre>"},{"location":"reference/common/","title":"common","text":""},{"location":"reference/common/common/","title":"common","text":""},{"location":"reference/common/common/#nhssynth.common.common.set_seed","title":"<code>set_seed(seed=None)</code>","text":"<p>Set the seed for numpy and torch.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>None | int</code> <p>The seed to set.</p> <code>None</code> Source code in <code>src/nhssynth/common/common.py</code> <pre><code>def set_seed(seed: None | int = None) -&gt; None:\n\"\"\"\n    Set the seed for numpy and torch.\n\n    Args:\n        seed: The seed to set.\n    \"\"\"\n    if seed:\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n</code></pre>"},{"location":"reference/common/constants/","title":"constants","text":""},{"location":"reference/common/dicts/","title":"dicts","text":""},{"location":"reference/common/dicts/#nhssynth.common.dicts.filter_dict","title":"<code>filter_dict(d, filter_keys, include=False)</code>","text":"<p>Given a dictionary, return a new dictionary either including or excluding keys in a given <code>filter</code> set.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to filter.</p> required <code>filter_keys</code> <code>set | list</code> <p>A list or set of keys to either include or exclude.</p> required <code>include</code> <code>bool</code> <p>Determine whether to return a dictionary including or excluding keys in <code>filter</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A filtered dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n{'c': 3}\n&gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n{'a': 1, 'b': 2}\n</code></pre> Source code in <code>src/nhssynth/common/dicts.py</code> <pre><code>def filter_dict(d: dict, filter_keys: set | list, include: bool = False) -&gt; dict:\n\"\"\"\n    Given a dictionary, return a new dictionary either including or excluding keys in a given `filter` set.\n\n    Args:\n        d: A dictionary to filter.\n        filter_keys: A list or set of keys to either include or exclude.\n        include: Determine whether to return a dictionary including or excluding keys in `filter`.\n\n    Returns:\n        A filtered dictionary.\n\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'})\n        {'c': 3}\n        &gt;&gt;&gt; filter_dict(d, {'a', 'b'}, include=True)\n        {'a': 1, 'b': 2}\n    \"\"\"\n    if include:\n        filtered_keys = set(filter_keys) &amp; set(d.keys())\n    else:\n        filtered_keys = set(d.keys()) - set(filter_keys)\n    return {k: v for k, v in d.items() if k in filtered_keys}\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.flatten_dict","title":"<code>flatten_dict(d)</code>","text":"<p>Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict[str, Any]</code> <p>A dictionary with possibly nested keys.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n&gt;&gt;&gt; flatten_dict(d)\n{'a': 1, 'c': 2, 'e': 3}\n</code></pre> Source code in <code>src/nhssynth/common/dicts.py</code> <pre><code>def flatten_dict(d: dict[str, Any]) -&gt; dict[str, Any]:\n\"\"\"\n    Flatten a dictionary by recursively combining nested keys into a single dictionary until no nested keys remain.\n\n    Args:\n        d: A dictionary with possibly nested keys.\n\n    Returns:\n        A flattened dictionary.\n\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': {'c': 2, 'd': {'e': 3}}}\n        &gt;&gt;&gt; flatten_dict(d)\n        {'a': 1, 'c': 2, 'e': 3}\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v).items())\n        else:\n            items.append((k, v))\n    return dict(items)\n</code></pre>"},{"location":"reference/common/dicts/#nhssynth.common.dicts.get_key_by_value","title":"<code>get_key_by_value(d, value)</code>","text":"<p>Find the first key in a dictionary with a given value.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary to search through.</p> required <code>value</code> <p>The value to search for.</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>The first key in <code>d</code> with the value <code>value</code>, or <code>None</code> if no such key exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n&gt;&gt;&gt; get_key_by_value(d, 2)\n'b'\n&gt;&gt;&gt; get_key_by_value(d, 3)\nNone\n</code></pre> Source code in <code>src/nhssynth/common/dicts.py</code> <pre><code>def get_key_by_value(d: dict, value) -&gt; Any | None:\n\"\"\"\n    Find the first key in a dictionary with a given value.\n\n    Args:\n        d: A dictionary to search through.\n        value: The value to search for.\n\n    Returns:\n        The first key in `d` with the value `value`, or `None` if no such key exists.\n\n    Examples:\n        &gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 1}\n        &gt;&gt;&gt; get_key_by_value(d, 2)\n        'b'\n        &gt;&gt;&gt; get_key_by_value(d, 3)\n        None\n\n    \"\"\"\n    for key, val in d.items():\n        if val == value:\n            return key\n    return None\n</code></pre>"},{"location":"reference/common/io/","title":"io","text":""},{"location":"reference/common/io/#nhssynth.common.io.check_exists","title":"<code>check_exists(fns, dir)</code>","text":"<p>Checks if the files in <code>fns</code> exist in <code>dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list[str]</code> <p>The list of files to check.</p> required <code>dir_experiment</code> <p>The directory the files should exist in.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If any of the files in <code>fns</code> do not exist in <code>dir_experiment</code>.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def check_exists(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Checks if the files in `fns` exist in `dir`.\n\n    Args:\n        fns: The list of files to check.\n        dir_experiment: The directory the files should exist in.\n\n    Raises:\n        FileNotFoundError: If any of the files in `fns` do not exist in `dir_experiment`.\n    \"\"\"\n    for fn in fns:\n        if not (dir / fn).exists():\n            raise FileNotFoundError(f\"File {fn} does not exist at {dir}.\")\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.consistent_ending","title":"<code>consistent_ending(fn, ending='.pkl')</code>","text":"<p>Ensures that the filename <code>fn</code> ends with <code>ending</code>. If not, removes any existing ending and appends <code>ending</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename to check.</p> required <code>ending</code> <code>str</code> <p>The desired ending to check for. Default is \".pkl\".</p> <code>'.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The filename with the correct ending.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def consistent_ending(fn: str, ending: str = \".pkl\") -&gt; str:\n\"\"\"\n    Ensures that the filename `fn` ends with `ending`. If not, removes any existing ending and appends `ending`.\n\n    Args:\n        fn: The filename to check.\n        ending: The desired ending to check for. Default is \".pkl\".\n\n    Returns:\n        The filename with the correct ending.\n    \"\"\"\n    path_fn = Path(fn)\n    if path_fn.suffix == ending:\n        return fn\n    else:\n        return str(path_fn.parent / path_fn.stem) + ending\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.experiment_io","title":"<code>experiment_io(experiment_name, dir_experiments='experiments')</code>","text":"<p>Create an experiment's directory and return the path.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <code>dir_experiments</code> <code>str</code> <p>The name of the directory containing all experiments.</p> <code>'experiments'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the experiment directory.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def experiment_io(experiment_name: str, dir_experiments: str = \"experiments\") -&gt; str:\n\"\"\"\n    Create an experiment's directory and return the path.\n\n    Args:\n        experiment_name: The name of the experiment.\n        dir_experiments: The name of the directory containing all experiments.\n\n    Returns:\n        The path to the experiment directory.\n    \"\"\"\n    dir_experiment = Path(dir_experiments) / experiment_name\n    dir_experiment.mkdir(parents=True, exist_ok=True)\n    return dir_experiment\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.potential_suffix","title":"<code>potential_suffix(fn, fn_base)</code>","text":"<p>Checks if <code>fn</code> is a suffix (starts with an underscore) to append to <code>fn_base</code>, or a filename in its own right.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>The filename / potential suffix to append to <code>fn_base</code>.</p> required <code>fn_base</code> <code>str</code> <p>The name of the file the suffix would attach to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The appropriately processed <code>fn</code></p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def potential_suffix(fn: str, fn_base: str) -&gt; str:\n\"\"\"\n    Checks if `fn` is a suffix (starts with an underscore) to append to `fn_base`, or a filename in its own right.\n\n    Args:\n        fn: The filename / potential suffix to append to `fn_base`.\n        fn_base: The name of the file the suffix would attach to.\n\n    Returns:\n        The appropriately processed `fn`\n    \"\"\"\n    fn_base = Path(fn_base).stem\n    if fn[0] == \"_\":\n        return fn_base + fn\n    else:\n        return fn\n</code></pre>"},{"location":"reference/common/io/#nhssynth.common.io.warn_if_path_supplied","title":"<code>warn_if_path_supplied(fns, dir)</code>","text":"<p>Warns if the files in <code>fns</code> include directory separators.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list[str]</code> <p>The list of files to check.</p> required <code>dir</code> <code>Path</code> <p>The directory the files should exist in.</p> required Warnings <p>Raises a UserWarning when the path to any of the files in <code>fns</code> includes directory separators, as this may not work as intended.</p> Source code in <code>src/nhssynth/common/io.py</code> <pre><code>def warn_if_path_supplied(fns: list[str], dir: Path) -&gt; None:\n\"\"\"\n    Warns if the files in `fns` include directory separators.\n\n    Args:\n        fns: The list of files to check.\n        dir: The directory the files should exist in.\n\n    Warnings:\n        Raises a UserWarning when the path to any of the files in `fns` includes directory separators, as this may not work as intended.\n    \"\"\"\n    for fn in fns:\n        if \"/\" in fn:\n            warnings.warn(\n                f\"Using the path supplied appended to {dir}, i.e. attempting to read data from {dir / fn}\",\n                UserWarning,\n            )\n</code></pre>"},{"location":"reference/modules/","title":"modules","text":""},{"location":"reference/modules/dataloader/","title":"dataloader","text":""},{"location":"reference/modules/dataloader/io/","title":"io","text":""},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_input_paths","title":"<code>check_input_paths(fn_data, fn_metadata, dir_data)</code>","text":"<p>Formats the input and output filenames and directories for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_in</code> <p>The input data filename.</p> required <code>fn_metadata</code> <code>str</code> <p>The metadata filename / suffix to append to <code>fn_in</code>.</p> required <code>dir_data</code> <code>str</code> <p>The directory that should contain both of the above.</p> required <p>Returns:</p> Type Description <code>tuple[Path, str, str]</code> <p>A tuple containing the formatted input, output, and metadata (in and out) paths.</p> Warnings <p>Raises a UserWarning when the path to <code>fn_in</code> includes directory separators, as this is not supported and may not work as intended. Raises a UserWarning when the path to <code>fn_metadata</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>src/nhssynth/modules/dataloader/io.py</code> <pre><code>def check_input_paths(\n    fn_data: str,\n    fn_metadata: str,\n    dir_data: str,\n) -&gt; tuple[Path, str, str]:\n\"\"\"\n    Formats the input and output filenames and directories for an experiment.\n\n    Args:\n        fn_in: The input data filename.\n        fn_metadata: The metadata filename / suffix to append to `fn_in`.\n        dir_data: The directory that should contain both of the above.\n\n    Returns:\n        A tuple containing the formatted input, output, and metadata (in and out) paths.\n\n    Warnings:\n        Raises a UserWarning when the path to `fn_in` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_metadata` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\n    fn_data, fn_metadata = consistent_ending(fn_data, \".csv\"), consistent_ending(fn_metadata, \".yaml\")\n    dir_data = Path(dir_data)\n    fn_metadata = potential_suffix(fn_metadata, fn_data)\n    warn_if_path_supplied([fn_data, fn_metadata], dir_data)\n    check_exists([fn_data], dir_data)\n    return dir_data, fn_data, fn_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.check_output_paths","title":"<code>check_output_paths(fn_in, fn_out, fn_transformer, dir_experiment)</code>","text":"<p>Formats the output filenames and directories for an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>fn_in</code> <code>str</code> <p>The input data filename.</p> required <code>fn_out</code> <code>str</code> <p>The output data filename / suffix to append to <code>fn_in</code>.</p> required <code>fn_transformer</code> <code>str</code> <p>The transformer filename / suffix to append to <code>fn_in</code>.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>A tuple containing the formatted output and transformer (in and out) paths.</p> Warnings <p>Raises a UserWarning when the path to <code>fn_out</code> includes directory separators, as this is not supported and may not work as intended. Raises a UserWarning when the path to <code>fn_transformer</code> includes directory separators, as this is not supported and may not work as intended.</p> Source code in <code>src/nhssynth/modules/dataloader/io.py</code> <pre><code>def check_output_paths(\n    fn_in: str,\n    fn_out: str,\n    fn_transformer: str,\n    dir_experiment: Path,\n) -&gt; tuple[str, str]:\n\"\"\"\n    Formats the output filenames and directories for an experiment.\n\n    Args:\n        fn_in: The input data filename.\n        fn_out: The output data filename / suffix to append to `fn_in`.\n        fn_transformer: The transformer filename / suffix to append to `fn_in`.\n\n    Returns:\n        A tuple containing the formatted output and transformer (in and out) paths.\n\n    Warnings:\n        Raises a UserWarning when the path to `fn_out` includes directory separators, as this is not supported and may not work as intended.\n        Raises a UserWarning when the path to `fn_transformer` includes directory separators, as this is not supported and may not work as intended.\n    \"\"\"\n    fn_out, fn_transformer = consistent_ending(fn_out), consistent_ending(fn_transformer)\n    fn_out, fn_transformer = potential_suffix(fn_out, fn_in), potential_suffix(fn_transformer, fn_in)\n    warn_if_path_supplied([fn_out, fn_transformer], dir_experiment)\n    return fn_out, fn_transformer\n</code></pre>"},{"location":"reference/modules/dataloader/io/#nhssynth.modules.dataloader.io.write_data_outputs","title":"<code>write_data_outputs(transformed_input, metatransformer, fn_out, fn_transformer, dir_experiment)</code>","text":"<p>Writes the transformed data and metatransformer to disk.</p> <p>Parameters:</p> Name Type Description Default <code>transformed_input</code> <code>pd.DataFrame</code> <p>The prepared version of the input data.</p> required <code>metatransformer</code> <code>MetaTransformer</code> <p>The metatransformer used to transform the data into its prepared state.</p> required <code>fn_out</code> <code>str</code> <p>The filename to dump the prepared data to.</p> required <code>fn_transformer</code> <code>str</code> <p>The filename to dump the metatransformer to.</p> required <code>dir_experiment</code> <code>Path</code> <p>The experiment directory to write the outputs to.</p> required Source code in <code>src/nhssynth/modules/dataloader/io.py</code> <pre><code>def write_data_outputs(\n    transformed_input: pd.DataFrame,\n    metatransformer: MetaTransformer,\n    fn_out: str,\n    fn_transformer: str,\n    dir_experiment: Path,\n) -&gt; None:\n\"\"\"\n    Writes the transformed data and metatransformer to disk.\n\n    Args:\n        transformed_input: The prepared version of the input data.\n        metatransformer: The metatransformer used to transform the data into its prepared state.\n        fn_out: The filename to dump the prepared data to.\n        fn_transformer: The filename to dump the metatransformer to.\n        dir_experiment: The experiment directory to write the outputs to.\n    \"\"\"\n    transformed_input.to_pickle(dir_experiment / fn_out)\n    transformed_input.to_csv(dir_experiment / (fn_out[:-3] + \"csv\"), index=False)\n    with open(dir_experiment / fn_transformer, \"wb\") as f:\n        pickle.dump(metatransformer, f)\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/","title":"metadata","text":""},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.check_metadata_columns","title":"<code>check_metadata_columns(metadata, data)</code>","text":"<p>Check if all column representations in the metadata correspond to valid columns in the DataFrame. If any columns are not present, add them to the metadata and instantiate an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing metadata for the columns in the DataFrame.</p> required <code>data</code> <code>pd.DataFrame</code> <p>The DataFrame to check against the metadata.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any columns in metadata are not present in the DataFrame.</p> Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def check_metadata_columns(metadata: dict[str, dict[str, Any]], data: pd.DataFrame) -&gt; None:\n\"\"\"\n    Check if all column representations in the metadata correspond to valid columns in the DataFrame.\n    If any columns are not present, add them to the metadata and instantiate an empty dictionary.\n\n    Args:\n        metadata: A dictionary containing metadata for the columns in the DataFrame.\n        data: The DataFrame to check against the metadata.\n\n    Raises:\n        AssertionError: If any columns in metadata are not present in the DataFrame.\n    \"\"\"\n    assert all([k in data.columns for k in metadata.keys()])\n    metadata.update({cn: {} for cn in data.columns if cn not in metadata})\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.collapse","title":"<code>collapse(metadata)</code>","text":"<p>Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>The metadata dictionary to be rewritten.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A rewritten metadata dictionary with collapsed column types and transformers. The returned dictionary has the following structure: {     \"transformers\": dict,     \"column_types\": dict,     metadata  # columns that now reference the dicts above } - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations. - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations. - \"metadata\" contains the original metadata dictionary, with column types and transformers   rewritten to use the indices in \"transformers\" and \"column_types\".</p> Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def collapse(metadata: dict) -&gt; dict:\n\"\"\"\n    Given a metadata dictionary, rewrites it to collapse duplicate column types and transformers.\n\n    Args:\n        metadata: The metadata dictionary to be rewritten.\n\n    Returns:\n        dict: A rewritten metadata dictionary with collapsed column types and transformers.\n            The returned dictionary has the following structure:\n            {\n                \"transformers\": dict,\n                \"column_types\": dict,\n                **metadata  # columns that now reference the dicts above\n            }\n            - \"transformers\" is a dictionary mapping transformer indices (integers) to transformer configurations.\n            - \"column_types\" is a dictionary mapping column type indices (integers) to column type configurations.\n            - \"**metadata\" contains the original metadata dictionary, with column types and transformers\n              rewritten to use the indices in \"transformers\" and \"column_types\".\n    \"\"\"\n    c_index = 1\n    column_types = {}\n    t_index = 1\n    transformers = {}\n    for cn, cd in metadata.items():\n\n        if cd not in column_types.values():\n            column_types[c_index] = cd.copy()\n            metadata[cn] = column_types[c_index]\n            c_index += 1\n        else:\n            cix = get_key_by_value(column_types, cd)\n            metadata[cn] = column_types[cix]\n\n        if cd[\"transformer\"] not in transformers.values() and cd[\"transformer\"]:\n            transformers[t_index] = cd[\"transformer\"].copy()\n            metadata[cn][\"transformer\"] = transformers[t_index]\n            t_index += 1\n        elif cd[\"transformer\"]:\n            tix = get_key_by_value(transformers, cd[\"transformer\"])\n            metadata[cn][\"transformer\"] = transformers[tix]\n\n    return {\"transformers\": transformers, \"column_types\": column_types, **metadata}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.create_empty_metadata","title":"<code>create_empty_metadata(data)</code>","text":"<p>Creates an empty metadata dictionary for a given pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The DataFrame for which an empty metadata dictionary is created.</p> required <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.</p> Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def create_empty_metadata(data: pd.DataFrame) -&gt; dict[str, dict]:\n\"\"\"\n    Creates an empty metadata dictionary for a given pandas DataFrame.\n\n    Args:\n        data: The DataFrame for which an empty metadata dictionary is created.\n\n    Returns:\n        A dictionary where each key corresponds to a column name in the DataFrame, and each value is an empty dictionary.\n    \"\"\"\n    return {cn: {} for cn in data.columns}\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.load_metadata","title":"<code>load_metadata(in_path, data)</code>","text":"<p>Load metadata from a YAML file located at <code>in_path</code>. If the file does not exist, create an empty metadata dictionary with column names from the <code>data</code> DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>in_path</code> <code>pathlib.Path</code> <p>The path to the YAML file containing the metadata.</p> required <code>data</code> <code>pd.DataFrame</code> <p>The DataFrame containing the data for which metadata is being loaded.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>A metadata dictionary containing information about the columns in the <code>data</code> DataFrame.</p> Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def load_metadata(in_path: pathlib.Path, data: pd.DataFrame) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Load metadata from a YAML file located at `in_path`. If the file does not exist, create an empty metadata\n    dictionary with column names from the `data` DataFrame.\n\n    Args:\n        in_path: The path to the YAML file containing the metadata.\n        data: The DataFrame containing the data for which metadata is being loaded.\n\n    Returns:\n        A metadata dictionary containing information about the columns in the `data` DataFrame.\n    \"\"\"\n    if in_path.exists():\n        with open(in_path) as stream:\n            metadata = yaml.safe_load(stream)\n        # Filter out expanded common groups\n        metadata = filter_dict(metadata, {\"transformers\", \"column_types\"})\n        check_metadata_columns(metadata, data)\n    else:\n        metadata = create_empty_metadata(data)\n    return metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metadata/#nhssynth.modules.dataloader.metadata.output_metadata","title":"<code>output_metadata(out_path, metadata, collapse_yaml)</code>","text":"<p>Writes metadata to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>pathlib.Path</code> <p>The path at which to write the metadata YAML file.</p> required <code>metadata</code> <code>dict[str, dict[str, Any]]</code> <p>The metadata dictionary to be written.</p> required <code>collapse_yaml</code> <code>bool</code> <p>A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.</p> required Source code in <code>src/nhssynth/modules/dataloader/metadata.py</code> <pre><code>def output_metadata(\n    out_path: pathlib.Path,\n    metadata: dict[str, dict[str, Any]],\n    collapse_yaml: bool,\n) -&gt; None:\n\"\"\"\n    Writes metadata to a YAML file.\n\n    Args:\n        out_path: The path at which to write the metadata YAML file.\n        metadata: The metadata dictionary to be written.\n        collapse_yaml: A boolean indicating whether to collapse the YAML representation of the metadata, reducing duplication.\n    \"\"\"\n    if collapse_yaml:\n        metadata = collapse(metadata)\n    with open(out_path, \"w\") as yaml_file:\n        yaml.safe_dump(metadata, yaml_file, default_flow_style=False, sort_keys=False)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/","title":"metatransformer","text":""},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer","title":"<code>MetaTransformer</code>","text":"<p>A metatransformer object that can be either a <code>HyperTransformer</code> from RDT or a <code>BaseSingleTableSynthesizer</code> from SDV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The input data as a pandas DataFrame.</p> required <code>metadata</code> <p>A dictionary containing the metadata for the input data. Each key corresponds to a column name in the input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the name of the transformer to use for the column and its configuration (to be instantiated below).</p> required <code>dtypes</code> <p>A dictionary mapping column names to their data types.</p> required <code>sdv_workflow</code> <p>A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from RDT will be used instead.</p> required <code>allow_null_transformers</code> <p>A boolean flag indicating whether to allow transformers to be None. If True, a None value for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated for that column.</p> required <code>Synthesizer</code> <p>The synthesizer class to use for the SDV workflow.</p> required Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>class MetaTransformer:\n\"\"\"\n    A metatransformer object that can be either a `HyperTransformer` from RDT or a `BaseSingleTableSynthesizer` from SDV.\n\n    Args:\n        data: The input data as a pandas DataFrame.\n        metadata: A dictionary containing the metadata for the input data. Each key corresponds to a column name in the\n            input data, and its value is a dictionary containing the metadata for the corresponding column. The metadata\n            should contain \"dtype\" and \"sdtype\" fields specifying the column's data type, and a \"transformer\" field specifying the\n            name of the transformer to use for the column and its configuration (to be instantiated below).\n        dtypes: A dictionary mapping column names to their data types.\n        sdv_workflow: A boolean flag indicating whether to use the SDV workflow or the RDT workflow. If True, the\n            TVAESynthesizer from SDV will be used as the metatransformer. If False, the HyperTransformer from\n            RDT will be used instead.\n        allow_null_transformers: A boolean flag indicating whether to allow transformers to be None. If True, a None value\n            for a transformer in the metadata will be treated as a valid value, and no transformer will be instantiated\n            for that column.\n        Synthesizer: The synthesizer class to use for the SDV workflow.\n    \"\"\"\n\n    def __init__(self, metadata, sdv_workflow, allow_null_transformers, synthesizer) -&gt; None:\n\n        self.sdv_workflow = sdv_workflow\n        self.allow_null_transformers = allow_null_transformers\n        self.Synthesizer = SDV_SYNTHESIZER_CHOICES[synthesizer]\n        self.dtypes = {cn: cd.get(\"dtype\", {}) for cn, cd in metadata.items()}\n        self.sdtypes = {cn: filter_dict(cd, {\"dtype\", \"transformer\"}) for cn, cd in metadata.items()}\n        self.transformers = {cn: get_transformer(cd) for cn, cd in metadata.items()}\n\n    def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n\n        Returns:\n            The data with the data types applied.\n        \"\"\"\n        if not all(self.dtypes.values()):\n            warnings.warn(\n                f\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\n                UserWarning,\n            )\n            self.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\n        return data.astype(self.dtypes)\n\n    def instantiate_synthesizer(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n        Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.\n\n        Args:\n            sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n                specifies the semantic SDV data type of the column.\n            transformers: A dictionary of column names to their transformers.\n            data: The input DataFrame.\n            allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n        Returns:\n            A BaseSingleTableSynthesizer object instantiated from the given metadata and data.\n        \"\"\"\n        if all(self.sdtypes.values()):\n            metadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\n        else:\n            warnings.warn(\n                f\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\n                UserWarning,\n            )\n            metadata = SingleTableMetadata()\n            metadata.detect_from_dataframe(data)\n            for column_name, values in self.sdtypes.items():\n                if values:\n                    metadata.update_column(column_name=column_name, **values)\n        if not all(self.transformers.values()) and not self.allow_null_transformers:\n            warnings.warn(\n                f\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\n                UserWarning,\n            )\n        synthesizer = self.Synthesizer(metadata)\n        synthesizer.auto_assign_transformers(data)\n        synthesizer.update_transformers(\n            self.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n        )\n        return synthesizer\n\n    def instantiate_hypertransformer(self, data: pd.DataFrame) -&gt; HyperTransformer:\n\"\"\"\n        Instantiates a HyperTransformer object from the given metadata and data.\n\n        Args:\n            sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n                specifies the semantic SDV data type of the column.\n            transformers: A dictionary of column names to their transformers.\n            data: The input DataFrame.\n            allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n        Returns:\n            A HyperTransformer object instantiated from the given metadata and data.\n        \"\"\"\n        ht = HyperTransformer()\n        if all(self.sdtypes.values()) and (all(self.transformers.values()) or self.allow_null_transformers):\n            ht.set_config(\n                config={\n                    \"sdtypes\": {k: v[\"sdtype\"] for k, v in self.sdtypes.items()},\n                    \"transformers\": self.transformers,\n                }\n            )\n        else:\n            warnings.warn(\n                f\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in self.sdtypes.items() if not v])) if not all(self.sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in self.transformers.items() if not v])) if not all(self.transformers.values()) and not self.allow_null_transformers else ''} automatically...\",\n                UserWarning,\n            )\n            ht.detect_initial_config(data)\n            ht.update_sdtypes({k: v[\"sdtype\"] for k, v in self.sdtypes.items() if v})\n            ht.update_transformers(\n                self.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n            )\n        return ht\n\n    def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"Calls the appropriate instantiation method based on the value of `sdv_workflow`.\"\"\"\n        if self.sdv_workflow:\n            return self.instantiate_synthesizer(data)\n        else:\n            return self.instantiate_hypertransformer(data)\n\n    def get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\n        return self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\n\n    def assemble(self) -&gt; None:\n\"\"\"\n        Extracts the metadata for the transformers and sdtypes used to transform the data.\n\n        Returns:\n            dict[str, Any]: A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - sdtype: The data type for the column (only present if `sdv_workflow` is False).\n                - transformer: A dictionary containing information about the transformer\n                used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that are not private or set by a random seed.\n                - dtype: The data type for the column\n        \"\"\"\n        if self.sdv_workflow:\n            sdmetadata = self.metatransformer.metadata\n            transformers = self.metatransformer.get_transformers()\n            return {\n                cn: {\n                    **cd,\n                    \"transformer\": make_transformer_dict(transformers[cn]),\n                    \"dtype\": self.get_dtype(cn),\n                }\n                for cn, cd in sdmetadata.columns.items()\n            }\n        else:\n            config = self.metatransformer.get_config()\n            return {\n                cn: {\n                    \"sdtype\": cd,\n                    \"transformer\": make_transformer_dict(config[\"transformers\"][cn]),\n                    \"dtype\": self.get_dtype(cn),\n                }\n                for cn, cd in config[\"sdtypes\"].items()\n            }\n\n    def infer_categorical_and_continuous(self, data: pd.DataFrame) -&gt; tuple[dict[str, int], int]:\n\"\"\"\n        Infers the categorical columns from the data.\n\n        Args:\n            data: The data to infer the categorical columns from.\n\n        Returns:\n            A dictionary mapping column names to the number of unique values in the column (if the column is categorical).\n        \"\"\"\n        categoricals = {\n            cn: data[cn].nunique() for cn, cd in self.assembled_metadata.items() if cd[\"sdtype\"] == \"categorical\"\n        }\n        num_continuous = len(data.columns) - len(categoricals)\n        return categoricals, num_continuous\n\n    def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Prepares the data by processing via the metatransformer.\n\n        Args:\n            data: The data to fit and apply the transformer to.\n        \"\"\"\n        if self.sdv_workflow:\n            return self.metatransformer.preprocess(data)\n        else:\n            return self.metatransformer.fit_transform(data)\n\n    def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Applies the various steps of the MetaTransformer to a passed DataFrame.\n\n        Args:\n            data: The DataFrame to transform.\n\n        Returns:\n            The transformed data.\n        \"\"\"\n        typed_data = self.apply_dtypes(data)\n        self.metatransformer = self.instantiate(typed_data)\n        self.assembled_metadata = self.assemble()\n        self.categoricals, self.num_continuous = self.infer_categorical_and_continuous(typed_data)\n        prepared_data = self.prepare(typed_data)\n        return prepared_data\n\n    def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n        Returns the assembled metadata for the transformer.\n\n        Returns:\n            A dictionary mapping column names to column metadata.\n                The metadata for each column has the following keys:\n                - sdtype: The data type for the column (only present if `sdv_workflow` is False).\n                - transformer: A dictionary containing information about the transformer\n                used for the column (if any). The dictionary has the following keys:\n                - name: The name of the transformer.\n                - Any other properties of the transformer that are not private or set by a random seed.\n                - dtype: The data type for the column\n\n        Raises:\n            ValueError: If the metadata has not yet been assembled.\n        \"\"\"\n        if not self.assembled_metadata:\n            raise ValueError(\n                \"Metadata has not yet been assembled. Call `MetaTransformer.apply` (or `MetaTransformer.assemble`) first.\"\n            )\n        return self.assembled_metadata\n\n    def order(self, data: pd.DataFrame) -&gt; tuple[pd.DataFrame, list[int], int]:\n\"\"\"\n        Orders the data based on the inferred categorical and continuous columns.\n\n        Args:\n            data: The data to order.\n\n        Returns:\n            A tuple containing the ordered data, a list of the number of unique values for each categorical column,\n            and the number of continuous columns.\n\n        Raises:\n            ValueError: If the categorical and continuous metadata has not yet been inferred.\n            ValueError: If a column is not found in the passed data.\n        \"\"\"\n        if not self.categoricals or not self.num_continuous:\n            raise ValueError(\n                \"Categorical and continuous metadata has not yet been inferred. Call `MetaTransformer.apply` (or `MetaTransformer.infer_categorical_and_continuous`) first.\"\n            )\n        categorical_ordering = []\n        continuous_ordering = []\n        for cn, cd in self.assembled_metadata.items():\n            if cd[\"transformer\"] and cd[\"transformer\"][\"name\"] == \"OneHotEncoder\":\n                idx = data.columns.get_loc(cn + \".value0\")\n                categorical_ordering += [*range(idx, idx + self.categoricals[cn])]\n            elif cn not in data.columns:\n                raise ValueError(f\"The {cn} column was not found in the passed data.\")\n            else:\n                continuous_ordering.append(data.columns.get_loc(cn))\n        ordering = categorical_ordering + continuous_ordering\n        return data.iloc[:, ordering], list(self.categoricals.values()), self.num_continuous\n\n    def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Reverses the transformation applied by the MetaTransformer.\n\n        Args:\n            data: The transformed data.\n\n        Returns:\n            The original data.\n        \"\"\"\n        if self.sdv_workflow:\n            return self.metatransformer.reverse_transform(data)\n        else:\n            return self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply","title":"<code>apply(data)</code>","text":"<p>Applies the various steps of the MetaTransformer to a passed DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The DataFrame to transform.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>The transformed data.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies the various steps of the MetaTransformer to a passed DataFrame.\n\n    Args:\n        data: The DataFrame to transform.\n\n    Returns:\n        The transformed data.\n    \"\"\"\n    typed_data = self.apply_dtypes(data)\n    self.metatransformer = self.instantiate(typed_data)\n    self.assembled_metadata = self.assemble()\n    self.categoricals, self.num_continuous = self.infer_categorical_and_continuous(typed_data)\n    prepared_data = self.prepare(typed_data)\n    return prepared_data\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.apply_dtypes","title":"<code>apply_dtypes(data)</code>","text":"<p>Applies dtypes from the metadata to <code>data</code> and infers missing dtypes by reading pandas defaults.</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>The data with the data types applied.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def apply_dtypes(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Applies dtypes from the metadata to `data` and infers missing dtypes by reading pandas defaults.\n\n    Returns:\n        The data with the data types applied.\n    \"\"\"\n    if not all(self.dtypes.values()):\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `dtype`s for column(s): {[k for k, v in self.dtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        self.dtypes.update({cn: data[cn].dtype for cn, cv in self.dtypes.items() if not cv})\n    return data.astype(self.dtypes)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.assemble","title":"<code>assemble()</code>","text":"<p>Extracts the metadata for the transformers and sdtypes used to transform the data.</p> <p>Returns:</p> Type Description <code>None</code> <p>dict[str, Any]: A dictionary mapping column names to column metadata. The metadata for each column has the following keys: - sdtype: The data type for the column (only present if <code>sdv_workflow</code> is False). - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys: - name: The name of the transformer. - Any other properties of the transformer that are not private or set by a random seed. - dtype: The data type for the column</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def assemble(self) -&gt; None:\n\"\"\"\n    Extracts the metadata for the transformers and sdtypes used to transform the data.\n\n    Returns:\n        dict[str, Any]: A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - sdtype: The data type for the column (only present if `sdv_workflow` is False).\n            - transformer: A dictionary containing information about the transformer\n            used for the column (if any). The dictionary has the following keys:\n            - name: The name of the transformer.\n            - Any other properties of the transformer that are not private or set by a random seed.\n            - dtype: The data type for the column\n    \"\"\"\n    if self.sdv_workflow:\n        sdmetadata = self.metatransformer.metadata\n        transformers = self.metatransformer.get_transformers()\n        return {\n            cn: {\n                **cd,\n                \"transformer\": make_transformer_dict(transformers[cn]),\n                \"dtype\": self.get_dtype(cn),\n            }\n            for cn, cd in sdmetadata.columns.items()\n        }\n    else:\n        config = self.metatransformer.get_config()\n        return {\n            cn: {\n                \"sdtype\": cd,\n                \"transformer\": make_transformer_dict(config[\"transformers\"][cn]),\n                \"dtype\": self.get_dtype(cn),\n            }\n            for cn, cd in config[\"sdtypes\"].items()\n        }\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_assembled_metadata","title":"<code>get_assembled_metadata()</code>","text":"<p>Returns the assembled metadata for the transformer.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>A dictionary mapping column names to column metadata. The metadata for each column has the following keys: - sdtype: The data type for the column (only present if <code>sdv_workflow</code> is False). - transformer: A dictionary containing information about the transformer used for the column (if any). The dictionary has the following keys: - name: The name of the transformer. - Any other properties of the transformer that are not private or set by a random seed. - dtype: The data type for the column</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the metadata has not yet been assembled.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def get_assembled_metadata(self) -&gt; dict[str, dict[str, Any]]:\n\"\"\"\n    Returns the assembled metadata for the transformer.\n\n    Returns:\n        A dictionary mapping column names to column metadata.\n            The metadata for each column has the following keys:\n            - sdtype: The data type for the column (only present if `sdv_workflow` is False).\n            - transformer: A dictionary containing information about the transformer\n            used for the column (if any). The dictionary has the following keys:\n            - name: The name of the transformer.\n            - Any other properties of the transformer that are not private or set by a random seed.\n            - dtype: The data type for the column\n\n    Raises:\n        ValueError: If the metadata has not yet been assembled.\n    \"\"\"\n    if not self.assembled_metadata:\n        raise ValueError(\n            \"Metadata has not yet been assembled. Call `MetaTransformer.apply` (or `MetaTransformer.assemble`) first.\"\n        )\n    return self.assembled_metadata\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.get_dtype","title":"<code>get_dtype(cn)</code>","text":"<p>Returns the dtype for the given column name <code>cn</code>.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def get_dtype(self, cn: str) -&gt; str | np.dtype:\n\"\"\"Returns the dtype for the given column name `cn`.\"\"\"\n    return self.dtypes[cn].name if not isinstance(self.dtypes[cn], str) else self.dtypes[cn]\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.infer_categorical_and_continuous","title":"<code>infer_categorical_and_continuous(data)</code>","text":"<p>Infers the categorical columns from the data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The data to infer the categorical columns from.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, int], int]</code> <p>A dictionary mapping column names to the number of unique values in the column (if the column is categorical).</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def infer_categorical_and_continuous(self, data: pd.DataFrame) -&gt; tuple[dict[str, int], int]:\n\"\"\"\n    Infers the categorical columns from the data.\n\n    Args:\n        data: The data to infer the categorical columns from.\n\n    Returns:\n        A dictionary mapping column names to the number of unique values in the column (if the column is categorical).\n    \"\"\"\n    categoricals = {\n        cn: data[cn].nunique() for cn, cd in self.assembled_metadata.items() if cd[\"sdtype\"] == \"categorical\"\n    }\n    num_continuous = len(data.columns) - len(categoricals)\n    return categoricals, num_continuous\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate","title":"<code>instantiate(data)</code>","text":"<p>Calls the appropriate instantiation method based on the value of <code>sdv_workflow</code>.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def instantiate(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer | HyperTransformer:\n\"\"\"Calls the appropriate instantiation method based on the value of `sdv_workflow`.\"\"\"\n    if self.sdv_workflow:\n        return self.instantiate_synthesizer(data)\n    else:\n        return self.instantiate_hypertransformer(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate_hypertransformer","title":"<code>instantiate_hypertransformer(data)</code>","text":"<p>Instantiates a HyperTransformer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>pd.DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>HyperTransformer</code> <p>A HyperTransformer object instantiated from the given metadata and data.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def instantiate_hypertransformer(self, data: pd.DataFrame) -&gt; HyperTransformer:\n\"\"\"\n    Instantiates a HyperTransformer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A HyperTransformer object instantiated from the given metadata and data.\n    \"\"\"\n    ht = HyperTransformer()\n    if all(self.sdtypes.values()) and (all(self.transformers.values()) or self.allow_null_transformers):\n        ht.set_config(\n            config={\n                \"sdtypes\": {k: v[\"sdtype\"] for k, v in self.sdtypes.items()},\n                \"transformers\": self.transformers,\n            }\n        )\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing{(' `sdtype`s for column(s): ' + str([k for k, v in self.sdtypes.items() if not v])) if not all(self.sdtypes.values()) else ''}{(' `transformer`s for column(s): ' + str([k for k, v in self.transformers.items() if not v])) if not all(self.transformers.values()) and not self.allow_null_transformers else ''} automatically...\",\n            UserWarning,\n        )\n        ht.detect_initial_config(data)\n        ht.update_sdtypes({k: v[\"sdtype\"] for k, v in self.sdtypes.items() if v})\n        ht.update_transformers(\n            self.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n        )\n    return ht\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.instantiate_synthesizer","title":"<code>instantiate_synthesizer(data)</code>","text":"<p>Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.</p> <p>Parameters:</p> Name Type Description Default <code>sdtypes</code> <p>A dictionary of column names to their metadata, containing the key \"sdtype\" which specifies the semantic SDV data type of the column.</p> required <code>transformers</code> <p>A dictionary of column names to their transformers.</p> required <code>data</code> <code>pd.DataFrame</code> <p>The input DataFrame.</p> required <code>allow_null_transformers</code> <p>A flag indicating whether or not to allow null transformers.</p> required <p>Returns:</p> Type Description <code>BaseSingleTableSynthesizer</code> <p>A BaseSingleTableSynthesizer object instantiated from the given metadata and data.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def instantiate_synthesizer(self, data: pd.DataFrame) -&gt; BaseSingleTableSynthesizer:\n\"\"\"\n    Instantiates a BaseSingleTableSynthesizer object from the given metadata and data.\n\n    Args:\n        sdtypes: A dictionary of column names to their metadata, containing the key \"sdtype\" which\n            specifies the semantic SDV data type of the column.\n        transformers: A dictionary of column names to their transformers.\n        data: The input DataFrame.\n        allow_null_transformers: A flag indicating whether or not to allow null transformers.\n\n    Returns:\n        A BaseSingleTableSynthesizer object instantiated from the given metadata and data.\n    \"\"\"\n    if all(self.sdtypes.values()):\n        metadata = SingleTableMetadata.load_from_dict({\"columns\": self.sdtypes})\n    else:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `sdtype`s for column(s): {[k for k, v in self.sdtypes.items() if not v]} automatically...\",\n            UserWarning,\n        )\n        metadata = SingleTableMetadata()\n        metadata.detect_from_dataframe(data)\n        for column_name, values in self.sdtypes.items():\n            if values:\n                metadata.update_column(column_name=column_name, **values)\n    if not all(self.transformers.values()) and not self.allow_null_transformers:\n        warnings.warn(\n            f\"Incomplete metadata, detecting missing `transformers`s for column(s): {[k for k, v in self.transformers.items() if not v]} automatically...\",\n            UserWarning,\n        )\n    synthesizer = self.Synthesizer(metadata)\n    synthesizer.auto_assign_transformers(data)\n    synthesizer.update_transformers(\n        self.transformers if self.allow_null_transformers else {k: v for k, v in self.transformers.items() if v}\n    )\n    return synthesizer\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.inverse_apply","title":"<code>inverse_apply(data)</code>","text":"<p>Reverses the transformation applied by the MetaTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The transformed data.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>The original data.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def inverse_apply(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Reverses the transformation applied by the MetaTransformer.\n\n    Args:\n        data: The transformed data.\n\n    Returns:\n        The original data.\n    \"\"\"\n    if self.sdv_workflow:\n        return self.metatransformer.reverse_transform(data)\n    else:\n        return self.metatransformer.reverse_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.order","title":"<code>order(data)</code>","text":"<p>Orders the data based on the inferred categorical and continuous columns.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The data to order.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A tuple containing the ordered data, a list of the number of unique values for each categorical column,</p> <code>list[int]</code> <p>and the number of continuous columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the categorical and continuous metadata has not yet been inferred.</p> <code>ValueError</code> <p>If a column is not found in the passed data.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def order(self, data: pd.DataFrame) -&gt; tuple[pd.DataFrame, list[int], int]:\n\"\"\"\n    Orders the data based on the inferred categorical and continuous columns.\n\n    Args:\n        data: The data to order.\n\n    Returns:\n        A tuple containing the ordered data, a list of the number of unique values for each categorical column,\n        and the number of continuous columns.\n\n    Raises:\n        ValueError: If the categorical and continuous metadata has not yet been inferred.\n        ValueError: If a column is not found in the passed data.\n    \"\"\"\n    if not self.categoricals or not self.num_continuous:\n        raise ValueError(\n            \"Categorical and continuous metadata has not yet been inferred. Call `MetaTransformer.apply` (or `MetaTransformer.infer_categorical_and_continuous`) first.\"\n        )\n    categorical_ordering = []\n    continuous_ordering = []\n    for cn, cd in self.assembled_metadata.items():\n        if cd[\"transformer\"] and cd[\"transformer\"][\"name\"] == \"OneHotEncoder\":\n            idx = data.columns.get_loc(cn + \".value0\")\n            categorical_ordering += [*range(idx, idx + self.categoricals[cn])]\n        elif cn not in data.columns:\n            raise ValueError(f\"The {cn} column was not found in the passed data.\")\n        else:\n            continuous_ordering.append(data.columns.get_loc(cn))\n    ordering = categorical_ordering + continuous_ordering\n    return data.iloc[:, ordering], list(self.categoricals.values()), self.num_continuous\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.MetaTransformer.prepare","title":"<code>prepare(data)</code>","text":"<p>Prepares the data by processing via the metatransformer.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The data to fit and apply the transformer to.</p> required Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def prepare(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n    Prepares the data by processing via the metatransformer.\n\n    Args:\n        data: The data to fit and apply the transformer to.\n    \"\"\"\n    if self.sdv_workflow:\n        return self.metatransformer.preprocess(data)\n    else:\n        return self.metatransformer.fit_transform(data)\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.get_transformer","title":"<code>get_transformer(d)</code>","text":"<p>Return a callable transformer object extracted from the given dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>A dictionary containing the transformer data.</p> required <p>Returns:</p> Type Description <code>BaseTransformer | None</code> <p>A callable object (transformer) if the dictionary contains transformer data, else None.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def get_transformer(d: dict) -&gt; BaseTransformer | None:\n\"\"\"\n    Return a callable transformer object extracted from the given dictionary.\n\n    Args:\n        d: A dictionary containing the transformer data.\n\n    Returns:\n        A callable object (transformer) if the dictionary contains transformer data, else None.\n    \"\"\"\n    transformer_data = d.get(\"transformer\", None)\n    if isinstance(transformer_data, dict) and \"name\" in transformer_data:\n        # Need to copy in case dicts are shared across columns, this can happen when reading a yaml with anchors\n        transformer_data = transformer_data.copy()\n        transformer_name = transformer_data.pop(\"name\")\n        return eval(transformer_name)(**transformer_data)\n    elif isinstance(transformer_data, str):\n        return eval(transformer_data)()\n    else:\n        return None\n</code></pre>"},{"location":"reference/modules/dataloader/metatransformer/#nhssynth.modules.dataloader.metatransformer.make_transformer_dict","title":"<code>make_transformer_dict(transformer)</code>","text":"<p>Deconstruct an instance of a transformer (if one is present) into a dictionary of config.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>BaseTransformer | None</code> <p>A BaseTransformer object from RDT (SDV).</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the transformer name and arguments.</p> Source code in <code>src/nhssynth/modules/dataloader/metatransformer.py</code> <pre><code>def make_transformer_dict(transformer: BaseTransformer | None) -&gt; dict:\n\"\"\"\n    Deconstruct an instance of a transformer (if one is present) into a dictionary of config.\n\n    Args:\n        transformer: A BaseTransformer object from RDT (SDV).\n\n    Returns:\n        A dictionary containing the transformer name and arguments.\n    \"\"\"\n    if transformer:\n        return {\n            \"name\": type(transformer).__name__,\n            **filter_dict(\n                transformer.__dict__,\n                {\"output_properties\", \"random_states\", \"transform\", \"reverse_transform\", \"_dtype\"},\n            ),\n        }\n    else:\n        return None\n</code></pre>"},{"location":"reference/modules/dataloader/run/","title":"run","text":""},{"location":"reference/modules/dataloader/run/#nhssynth.modules.dataloader.run.run","title":"<code>run(args)</code>","text":"<p>Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>An argparse Namespace containing the command line arguments.</p> required <p>Returns:</p> Type Description <code>argparse.Namespace</code> <p>None</p> Source code in <code>src/nhssynth/modules/dataloader/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"\n    Runs the main workflow of the dataloader module, transforming the input data and writing the output to file.\n\n    Args:\n        args: An argparse Namespace containing the command line arguments.\n\n    Returns:\n        None\n    \"\"\"\n    print(\"Running dataloader module...\")\n\n    set_seed(args.seed)\n    dir_experiment = experiment_io(args.experiment_name)\n\n    dir_input, fn_input_data, fn_metadata = check_input_paths(args.input, args.metadata, args.data_dir)\n\n    # Load the dataset and accompanying metadata\n    input = pd.read_csv(dir_input / fn_input_data, index_col=args.index_col)\n    metadata = load_metadata(dir_input / fn_metadata, input)\n\n    mt = MetaTransformer(metadata, args.sdv_workflow, args.allow_null_transformers, args.synthesizer)\n    transformed_input = mt.apply(input)\n\n    # Output the metadata corresponding to `transformed_input`, for reproducibility\n    if not args.discard_metadata:\n        output_metadata(dir_experiment / fn_metadata, mt.get_assembled_metadata(), args.collapse_yaml)\n\n    # Write the transformed input to the appropriate file\n    if not args.modules_to_run or args.modules_to_run == [\"dataloader\"] or args.write_all:\n        fn_output_data, fn_transformer = check_output_paths(\n            fn_input_data, args.output, args.metatransformer, dir_experiment\n        )\n        write_data_outputs(transformed_input, mt, fn_output_data, fn_transformer, dir_experiment)\n\n    # TODO Probably some way to ensure modules_to_run exists in args\n    if \"model\" in args.modules_to_run:\n        args.dataloader_output = {\n            \"fn_data\": fn_input_data,\n            \"data\": transformed_input,\n            \"categorical_metadata\": mt.get_categoricals(),\n            \"metatransformer\": mt,\n        }\n\n    return args\n</code></pre>"},{"location":"reference/modules/evaluation/","title":"evaluation","text":""},{"location":"reference/modules/evaluation/metrics/","title":"metrics","text":""},{"location":"reference/modules/evaluation/run/","title":"run","text":""},{"location":"reference/modules/model/","title":"model","text":""},{"location":"reference/modules/model/DPVAE/","title":"DPVAE","text":""},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Decoder","title":"<code>Decoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Decoder, takes in z and outputs reconstruction</p> Source code in <code>src/nhssynth/modules/model/DPVAE.py</code> <pre><code>class Decoder(nn.Module):\n\"\"\"Decoder, takes in z and outputs reconstruction\"\"\"\n\n    def __init__(\n        self,\n        latent_dim,\n        num_continuous,\n        num_categories=[0],\n        hidden_dim=32,\n        activation=nn.Tanh,\n        device=\"gpu\",\n    ):\n        super().__init__()\n\n        output_dim = num_continuous + sum(num_categories)\n        self.num_continuous = num_continuous\n        self.num_categories = num_categories\n\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Decoder: {device} specified, {self.device} used\")\n\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, z):\n        return self.net(z)\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.Encoder","title":"<code>Encoder</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Encoder, takes in x and outputs mu_z, sigma_z (diagonal Gaussian variational posterior assumed)</p> Source code in <code>src/nhssynth/modules/model/DPVAE.py</code> <pre><code>class Encoder(nn.Module):\n\"\"\"Encoder, takes in x\n    and outputs mu_z, sigma_z\n    (diagonal Gaussian variational posterior assumed)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim,\n        latent_dim,\n        hidden_dim=32,\n        activation=nn.Tanh,\n        device=\"gpu\",\n    ):\n        super().__init__()\n        if device == \"gpu\":\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        else:\n            self.device = torch.device(\"cpu\")\n            print(f\"Encoder: {device} specified, {self.device} used\")\n        output_dim = 2 * latent_dim\n        self.latent_dim = latent_dim\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, hidden_dim),\n            activation(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, x):\n        outs = self.net(x)\n        mu_z = outs[:, : self.latent_dim]\n        logsigma_z = outs[:, self.latent_dim :]\n        return mu_z, logsigma_z\n</code></pre>"},{"location":"reference/modules/model/DPVAE/#nhssynth.modules.model.DPVAE.VAE","title":"<code>VAE</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Combines encoder and decoder into full VAE model</p> Source code in <code>src/nhssynth/modules/model/DPVAE.py</code> <pre><code>class VAE(nn.Module):\n\"\"\"Combines encoder and decoder into full VAE model\"\"\"\n\n    def __init__(self, encoder, decoder, lr=1e-3):\n        super().__init__()\n        self.encoder = encoder.to(encoder.device)\n        self.decoder = decoder.to(decoder.device)\n        self.device = encoder.device\n        self.num_categories = self.decoder.num_categories\n        self.num_continuous = self.decoder.num_continuous\n        self.noiser = Noiser(self.num_continuous).to(decoder.device)\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n        self.lr = lr\n\n    def reconstruct(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        x_recon = self.decoder(mu_z)\n        return x_recon\n\n    def generate(self, N):\n        z_samples = torch.randn_like(torch.ones((N, self.encoder.latent_dim)), device=self.device)\n        x_gen = self.decoder(z_samples)\n        x_gen_ = torch.ones_like(x_gen, device=self.device)\n        i = 0\n\n        for v in range(len(self.num_categories)):\n            x_gen_[:, i : (i + self.num_categories[v])] = torch.distributions.one_hot_categorical.OneHotCategorical(\n                logits=x_gen[:, i : (i + self.num_categories[v])]\n            ).sample()\n            i = i + self.num_categories[v]\n\n        x_gen_[:, -self.num_continuous :] = x_gen[:, -self.num_continuous :] + torch.exp(\n            self.noiser(x_gen[:, -self.num_continuous :])\n        ) * torch.randn_like(x_gen[:, -self.num_continuous :])\n        return x_gen_\n\n    def loss(self, X):\n        mu_z, logsigma_z = self.encoder(X)\n\n        p = Normal(torch.zeros_like(mu_z), torch.ones_like(mu_z))\n        q = Normal(mu_z, torch.exp(logsigma_z))\n\n        divergence_loss = torch.sum(torch.distributions.kl_divergence(q, p))\n\n        s = torch.randn_like(mu_z)\n        z_samples = mu_z + s * torch.exp(logsigma_z)\n\n        x_recon = self.decoder(z_samples)\n\n        categoric_loglik = 0\n        if sum(self.num_categories) != 0:\n            i = 0\n\n            for v in range(len(self.num_categories)):\n\n                categoric_loglik += -torch.nn.functional.cross_entropy(\n                    x_recon[:, i : (i + self.num_categories[v])],\n                    torch.max(X[:, i : (i + self.num_categories[v])], 1)[1],\n                ).sum()\n                i = i + self.decoder.num_categories[v]\n\n        gauss_loglik = 0\n        if self.decoder.num_continuous != 0:\n            gauss_loglik = (\n                Normal(\n                    loc=x_recon[:, -self.num_continuous :],\n                    scale=torch.exp(self.noiser(x_recon[:, -self.num_continuous :])),\n                )\n                .log_prob(X[:, -self.num_continuous :])\n                .sum()\n            )\n\n        reconstruct_loss = -(categoric_loglik + gauss_loglik)\n\n        elbo = divergence_loss + reconstruct_loss\n\n        return (elbo, reconstruct_loss, divergence_loss, categoric_loglik, gauss_loglik)\n\n    def train(\n        self,\n        x_dataloader,\n        num_epochs,\n        logging_freq=1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        # mean_norm = 0\n        # counter = 0\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(num_epochs):\n\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruc,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruc.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # counter += 1\n                # l2_norm = 0\n                # for p in self.parameters():\n                #     if p.requires_grad:\n                #         p_norm = p.grad.detach().data.norm(2)\n                #         l2_norm += p_norm.item() ** 2\n                # l2_norm = l2_norm ** 0.5  # / Y_subset.shape[0]\n                # mean_norm = (mean_norm * (counter - 1) + l2_norm) / counter\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n            # self.mean_norm = mean_norm\n\n            if stop_counter == patience:\n\n                num_epochs = epoch + 1\n\n                break\n\n        return (\n            num_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def diff_priv_train(\n        self,\n        x_dataloader,\n        num_epochs,\n        C=1e16,\n        noise_scale=None,\n        target_epsilon=1,\n        target_delta=1e-5,\n        logging_freq=1,\n        sample_rate=0.1,\n        patience=5,\n        delta=10,\n        filepath=None,\n    ):\n        if noise_scale is not None:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                noise_multiplier=noise_scale,\n                max_grad_norm=C,\n            )\n        else:\n            self.privacy_engine = PrivacyEngine(\n                self,\n                sample_rate=sample_rate,\n                alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n                target_epsilonilon=target_epsilon,\n                target_delta=target_delta,\n                epochs=num_epochs,\n                max_grad_norm=C,\n            )\n        self.privacy_engine.attach(self.optimizer)\n\n        log_elbo = []\n        log_reconstruct = []\n        log_divergence = []\n        log_cat_loss = []\n        log_num_loss = []\n\n        # EARLY STOPPING #\n        min_elbo = 0.0  # For early stopping workflow\n        patience = patience  # How many epochs patience we give for early stopping\n        stop_counter = 0  # Counter for stops\n        delta = delta  # Difference in elbo value\n\n        for epoch in range(num_epochs):\n            train_loss = 0.0\n            divergence_epoch_loss = 0.0\n            reconstruction_epoch_loss = 0.0\n            categorical_epoch_reconstruct = 0.0\n            numerical_epoch_reconstruct = 0.0\n            # print(self.get_privacy_spent(target_delta))\n\n            for batch_idx, (Y_subset,) in enumerate(tqdm(x_dataloader)):\n\n                self.optimizer.zero_grad()\n                (\n                    elbo,\n                    reconstruct_loss,\n                    divergence_loss,\n                    categorical_reconstruct,\n                    numerical_reconstruct,\n                ) = self.loss(Y_subset.to(self.encoder.device))\n                elbo.backward()\n                self.optimizer.step()\n\n                train_loss += elbo.item()\n                divergence_epoch_loss += divergence_loss.item()\n                reconstruction_epoch_loss += reconstruct_loss.item()\n                categorical_epoch_reconstruct += categorical_reconstruct.item()\n                numerical_epoch_reconstruct += numerical_reconstruct.item()\n\n                # print(self.get_privacy_spent(target_delta))\n                # print(loss.item())\n\n            log_elbo.append(train_loss)\n            log_reconstruct.append(reconstruction_epoch_loss)\n            log_divergence.append(divergence_epoch_loss)\n            log_cat_loss.append(categorical_epoch_reconstruct)\n            log_num_loss.append(numerical_epoch_reconstruct)\n\n            if epoch == 0:\n\n                min_elbo = train_loss\n\n            if train_loss &lt; (min_elbo - delta):\n\n                min_elbo = train_loss\n                stop_counter = 0  # Set counter to zero\n                if filepath != None:\n                    self.save(filepath)  # Save best model if we want to\n\n            else:  # elbo has not improved\n\n                stop_counter += 1\n\n            if epoch % logging_freq == 0:\n                print(\n                    f\"\\tEpoch: {epoch:2}. Elbo: {train_loss:11.2f}. Reconstruction Loss: {reconstruction_epoch_loss:11.2f}. KL Divergence: {divergence_epoch_loss:11.2f}. Categorical Loss: {categorical_epoch_reconstruct:11.2f}. Numerical Loss: {numerical_epoch_reconstruct:11.2f}\"\n                )\n                # print(f\"\\tMean norm: {mean_norm}\")\n\n            if stop_counter == patience:\n\n                num_epochs = epoch + 1\n                break\n\n        return (\n            num_epochs,\n            log_elbo,\n            log_reconstruct,\n            log_divergence,\n            log_cat_loss,\n            log_num_loss,\n        )\n\n    def get_privacy_spent(self, delta):\n        if hasattr(self, \"privacy_engine\"):\n            return self.privacy_engine.get_privacy_spent(delta)\n        else:\n            print(\n\"\"\"This VAE object does not a privacy_engine attribute.\n                Run diff_priv_train to create one.\"\"\"\n            )\n\n    def save(self, filename):\n        torch.save(self.state_dict(), filename)\n\n    def load(self, filename):\n        self.load_state_dict(torch.load(filename))\n</code></pre>"},{"location":"reference/modules/model/io/","title":"io","text":""},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.check_input_paths","title":"<code>check_input_paths(fn_base, fn_prepared, fn_metatransformer, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_data</code> <p>The name of the data file.</p> required <code>fn_metadata</code> <p>The name of the metadata file.</p> required <code>fn_metatransformer</code> <code>str</code> <p>The name of the metatransformer file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>The paths to the data, metadata and metatransformer files.</p> Source code in <code>src/nhssynth/modules/model/io.py</code> <pre><code>def check_input_paths(fn_base: str, fn_prepared: str, fn_metatransformer: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n\n    Args:\n        fn_data: The name of the data file.\n        fn_metadata: The name of the metadata file.\n        fn_metatransformer: The name of the metatransformer file.\n        dir_experiment: The path to the experiment directory.\n\n    Returns:\n        The paths to the data, metadata and metatransformer files.\n    \"\"\"\n    fn_base, fn_prepared, fn_metatransformer = (\n        consistent_ending(fn_base),\n        consistent_ending(fn_prepared),\n        consistent_ending(fn_metatransformer),\n    )\n    fn_prepared, fn_metatransformer = (\n        potential_suffix(fn_prepared, fn_base),\n        potential_suffix(fn_metatransformer, fn_base),\n    )\n    warn_if_path_supplied([fn_base, fn_prepared, fn_metatransformer], dir_experiment)\n    check_exists([fn_prepared, fn_metatransformer], dir_experiment)\n    return fn_base, fn_prepared, fn_metatransformer\n</code></pre>"},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.check_output_paths","title":"<code>check_output_paths(fn_base, fn_out, fn_model, dir_experiment)</code>","text":"<p>Sets up the input and output paths for the model files.</p> <p>Parameters:</p> Name Type Description Default <code>fn_model</code> <code>str</code> <p>The name of the model file.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment output directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>The path to output the model.</p> Source code in <code>src/nhssynth/modules/model/io.py</code> <pre><code>def check_output_paths(fn_base: Path, fn_out: str, fn_model: str, dir_experiment: Path) -&gt; tuple[str, str]:\n\"\"\"\n    Sets up the input and output paths for the model files.\n\n    Args:\n        fn_model: The name of the model file.\n        dir_experiment: The path to the experiment output directory.\n\n    Returns:\n        The path to output the model.\n    \"\"\"\n    fn_out, fn_model = consistent_ending(fn_out, \".csv\"), consistent_ending(fn_model, \".pt\")\n    fn_out, fn_model = potential_suffix(fn_out, fn_base), potential_suffix(fn_model, fn_base)\n    warn_if_path_supplied([fn_out, fn_model], dir_experiment)\n    return fn_out, fn_model\n</code></pre>"},{"location":"reference/modules/model/io/#nhssynth.modules.model.io.load_required_data","title":"<code>load_required_data(args, dir_experiment)</code>","text":"<p>Loads the data from <code>args</code> or from disk when the dataloader has not be run previously.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>argparse.Namespace</code> <p>The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.</p> required <code>dir_experiment</code> <code>Path</code> <p>The path to the experiment directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]</code> <p>The data, metadata and metatransformer.</p> Source code in <code>src/nhssynth/modules/model/io.py</code> <pre><code>def load_required_data(\n    args: argparse.Namespace, dir_experiment: Path\n) -&gt; tuple[str, pd.DataFrame, dict[str, int], MetaTransformer]:\n\"\"\"\n    Loads the data from `args` or from disk when the dataloader has not be run previously.\n\n    Args:\n        args: The arguments passed to the module, in this case potentially carrying the outputs of the dataloader module.\n        dir_experiment: The path to the experiment directory.\n\n    Returns:\n        The data, metadata and metatransformer.\n    \"\"\"\n    if getattr(args, \"dataloader_output\", None):\n        return (\n            args.dataloader_output[\"fn_real_data\"],\n            args.dataloader_output[\"data\"],\n            args.dataloader_output[\"metatransformer\"],\n        )\n    else:\n        if not args.real_data:\n            raise ValueError(\n                \"You must provide `--real-data` when running this module on its own, please provide this (a prepared version and corresponding MetaTransformer must also exist in {dir_experiment})\"\n            )\n        fn_real_data, fn_prepared_data, fn_metatransformer = check_input_paths(\n            args.real_data, args.prepared_data, args.real_metatransformer, dir_experiment\n        )\n\n        with open(dir_experiment / fn_prepared_data, \"rb\") as f:\n            data = pickle.load(f)\n        with open(dir_experiment / fn_metatransformer, \"rb\") as f:\n            metatransformer = pickle.load(f)\n\n        return fn_real_data, data, metatransformer\n</code></pre>"},{"location":"reference/modules/model/run/","title":"run","text":""},{"location":"reference/modules/model/run/#nhssynth.modules.model.run.run","title":"<code>run(args)</code>","text":"<p>Run the model architecture module.</p> Source code in <code>src/nhssynth/modules/model/run.py</code> <pre><code>def run(args: argparse.Namespace) -&gt; argparse.Namespace:\n\"\"\"Run the model architecture module.\"\"\"\n    print(\"Running model architecture module...\")\n\n    set_seed(args.seed)\n\n    dir_experiment = experiment_io(args.experiment_name)\n\n    print(args)\n\n    fn_real_data, data, metatransformer = load_required_data(args, dir_experiment)\n    data, categoricals, num_continuous = metatransformer.order(data)\n\n    nrows, ncols = data.shape\n    torch_data = TensorDataset(torch.Tensor(data.to_numpy().astype(\"float32\")))\n\n    sample_rate = args.batch_size / nrows\n    data_loader = DataLoader(\n        torch_data,\n        batch_sampler=UniformWithReplacementSampler(num_samples=nrows, sample_rate=sample_rate),\n        pin_memory=True,\n    )\n\n    print(f\"Train, generate and evaluate {'' if args.non_private_training else 'DP'}VAE...\")\n\n    encoder = Encoder(ncols, args.latent_dim, hidden_dim=args.hidden_dim)\n    decoder = Decoder(args.latent_dim, num_continuous, num_categories=categoricals, hidden_dim=args.hidden_dim)\n    vae = VAE(encoder, decoder)\n    if not args.non_private_training:\n        results = vae.diff_priv_train(\n            data_loader,\n            num_epochs=args.num_epochs,\n            C=args.max_grad_norm,\n            target_epsilon=args.target_epsilon,\n            target_delta=args.target_delta,\n            sample_rate=sample_rate,\n        )\n        print(f\"(epsilon, delta): {vae.get_privacy_spent(args.target_delta)}\")\n    else:\n        results = vae.train(data_loader, num_epochs=args.num_epochs)\n\n    synthetic_data = vae.generate(nrows)\n\n    if torch.cuda.is_available():\n        synthetic_data = synthetic_data.cpu()\n\n    synthetic_data = pd.DataFrame(synthetic_data.detach(), columns=data.columns)\n    fn_output, fn_model = check_output_paths(fn_real_data, args.synthetic_data, args.model_file, dir_experiment)\n    if not args.discard_data:\n        # synthetic_data = metatransformer.inverse_apply(synthetic_data)\n        synthetic_data.to_csv(dir_experiment / fn_output, index=False)\n    if not args.discard_model:\n        vae.save(dir_experiment / fn_model)\n\n    if args.modules_to_run and \"evaluation\" in args.modules_to_run:\n        args.model_output = {\"results\": results}\n\n    return args\n</code></pre>"},{"location":"reference/modules/plotting/","title":"plotting","text":""},{"location":"reference/modules/plotting/plot/","title":"plot","text":""},{"location":"reference/modules/plotting/run/","title":"run","text":""},{"location":"reference/modules/structure/","title":"structure","text":""},{"location":"reference/modules/structure/run/","title":"run","text":""}]}